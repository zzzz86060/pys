{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-07T07:35:50.515284Z",
     "start_time": "2024-06-07T07:35:50.501191Z"
    }
   },
   "source": [
    "import paddle \n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "class ConvBNLayer(paddle.nn.Layer):\n",
    "    def __init__(self,numchannals,num_fileters,filter_size,stride=1,groups=1,act=None):\n",
    "        super(ConvBNLayer,self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2D(\n",
    "        in_channels=numchannals,\n",
    "        out_channels=num_fileters,\n",
    "        kernel_size=filter_size,\n",
    "        stride=stride,\n",
    "        padding=(filter_size-1)//2,\n",
    "        groups=groups,\n",
    "        bias_attr = False)\n",
    "        self.batch_norm = nn.BatchNorm2D(num_fileters)\n",
    "        self.act = act\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        y = self.conv(inputs)\n",
    "        y = self.batch_norm(y)\n",
    "        if self.act =='leaky':\n",
    "            y=F.leaky_relu(x=y,negative_slope=0.1)\n",
    "        elif self.act =='relu':\n",
    "            y=F.relu(y)\n",
    "        return y"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T07:35:50.715260Z",
     "start_time": "2024-06-07T07:35:50.703776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "class BottleneckBlock(paddle.nn.Layer):\n",
    "    def __init__(self,num_channels,num_filters,stride,shortcut=True):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        # 创建第一个卷积层 1x1\n",
    "        self.conv0 = ConvBNLayer(\n",
    "            numchannals=num_channels,\n",
    "            num_fileters=num_filters,\n",
    "            filter_size=1,\n",
    "            act='relu')\n",
    "\n",
    "        # 创建第二个卷积层 3x3\n",
    "\n",
    "        self.conv1 = ConvBNLayer(\n",
    "            numchannals=num_filters,\n",
    "            num_fileters=num_filters,\n",
    "            filter_size=3,\n",
    "            stride=stride,\n",
    "            act='relu')\n",
    "        # 创建第三个卷积 1x1，但输出通道数乘以4\n",
    "        self.conv2 = ConvBNLayer(\n",
    "            numchannals=num_filters,\n",
    "            num_fileters=num_filters * 4,\n",
    "            filter_size=1,\n",
    "            act=None)\n",
    "        # 如果conv2的输出跟此残差块的输入数据形状一致，则shortcut=True\n",
    "        # 否则shortcut = False，添加1个1x1的卷积作用在输入数据上，使其形状变成跟conv2一致\n",
    "        if not shortcut:\n",
    "            self.short = ConvBNLayer(\n",
    "                numchannals=num_channels,\n",
    "                num_fileters=num_filters * 4,\n",
    "                filter_size=1,\n",
    "                stride=stride)\n",
    "        self.shortcut = shortcut\n",
    "        self._num_channels_out = num_filters * 4\n",
    "    def forward(self, inputs):\n",
    "        y = self.conv0(inputs)\n",
    "        conv1 = self.conv1(y)\n",
    "        conv2 = self.conv2(conv1)\n",
    "        # 如果shortcut=True，直接将inputs跟conv2的输出相加\n",
    "        # 否则需要对inputs进行一次卷积，将形状调整成跟conv2输出一致\n",
    "        if self.shortcut:\n",
    "            short = inputs\n",
    "        else:\n",
    "            short = self.short(inputs)\n",
    "        y = paddle.add(x=short, y=conv2)\n",
    "        y = F.relu(y)\n",
    "        return y"
   ],
   "id": "921b50e3ef08a7f2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# resnet",
   "id": "1424a776ad7aa680"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T07:35:50.824968Z",
     "start_time": "2024-06-07T07:35:50.805931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义ResNet模型\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddle.nn as nn\n",
    "import paddle.nn.functional as F\n",
    "class ResNet(paddle.nn.Layer):\n",
    "    def __init__(self, layers=50, class_dim=1):\n",
    "        \"\"\"\n",
    "        layers, 网络层数，可以是50, 101或者152\n",
    "        class_dim，分类标签的类别数\n",
    "        \"\"\"\n",
    "        super(ResNet, self).__init__()\n",
    "        self.layers = layers\n",
    "        supported_layers = [50, 101, 152]\n",
    "        assert layers in supported_layers, \\\n",
    "\"supported layers are {} but input layer is {}\".format(supported_layers, layers)\n",
    "        if layers == 50:\n",
    "            #ResNet50包含多个模块，其中第2到第5个模块分别包含3、4、6、3个残差块\n",
    "            depth = [3, 4, 6, 3]\n",
    "        elif layers == 101:\n",
    "            #ResNet101包含多个模块，其中第2到第5个模块分别包含3、4、23、3个残差块\n",
    "            depth = [3, 4, 23, 3]\n",
    "        elif layers == 152:\n",
    "            #ResNet152包含多个模块，其中第2到第5个模块分别包含3、8、36、3个残差块\n",
    "            depth = [3, 8, 36, 3]\n",
    "        # 残差块中使用到的卷积的输出通道数\n",
    "        num_filters = [64, 128, 256, 512]\n",
    "        # ResNet的第一个模块，包含1个7x7卷积，后面跟着1个最大池化层\n",
    "        self.conv = ConvBNLayer(\n",
    "            numchannals=3,\n",
    "            num_fileters=64,\n",
    "            filter_size=7,\n",
    "            stride=2,\n",
    "            act='relu')\n",
    "        self.pool2d_max = nn.MaxPool2D(\n",
    "            kernel_size=3,\n",
    "            stride=2,\n",
    "            padding=1)\n",
    "        # ResNet的第二到第五个模块c2、c3、c4、c5\n",
    "        self.bottleneck_block_list = []\n",
    "        num_channels = 64\n",
    "        for block in range(len(depth)):\n",
    "            shortcut = False\n",
    "            for i in range(depth[block]):\n",
    "                # c3、c4、c5将会在第一个残差块使用stride=2；其余所有残差块stride=1\n",
    "                bottleneck_block = self.add_sublayer(\n",
    "                    'bb_%d_%d' % (block, i),\n",
    "                    BottleneckBlock(\n",
    "                        num_channels=num_channels,\n",
    "                        num_filters=num_filters[block],\n",
    "                        stride=2 if i == 0 and block != 0 else 1, \n",
    "                        shortcut=shortcut))\n",
    "                num_channels = bottleneck_block._num_channels_out\n",
    "                self.bottleneck_block_list.append(bottleneck_block)\n",
    "                shortcut = True\n",
    "        # 在c5的输出特征图上使用全局池化\n",
    "        self.pool2d_avg = paddle.nn.AdaptiveAvgPool2D(output_size=1)\n",
    "        # stdv用来作为全连接层随机初始化参数的方差\n",
    "        import math\n",
    "        stdv = 1.0 / math.sqrt(2048 * 1.0)\n",
    "        # 创建全连接层，输出大小为类别数目，经过残差网络的卷积和全局池化后，\n",
    "        # 卷积特征的维度是[B,2048,1,1]，故最后一层全连接的输入维度是2048\n",
    "        self.out = nn.Linear(in_features=2048, out_features=class_dim,\n",
    "                      weight_attr=paddle.ParamAttr(\n",
    "                          initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))\n",
    "    def forward(self, inputs):\n",
    "        y = self.conv(inputs)\n",
    "        y = self.pool2d_max(y)\n",
    "        for bottleneck_block in self.bottleneck_block_list:\n",
    "            y = bottleneck_block(y)\n",
    "        y = self.pool2d_avg(y)\n",
    "        y = paddle.reshape(y, [y.shape[0], -1])\n",
    "        y = self.out(y)\n",
    "        return y"
   ],
   "id": "a3768a1517c2b4e5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T07:35:51.197530Z",
     "start_time": "2024-06-07T07:35:50.853255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_1 = ResNet()\n",
    "print(model_1)"
   ],
   "id": "5a6f229fd2fb439a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv): ConvBNLayer(\n",
      "    (conv): Conv2D(3, 64, kernel_size=[7, 7], stride=[2, 2], padding=3, data_format=NCHW)\n",
      "    (batch_norm): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "  )\n",
      "  (pool2d_max): MaxPool2D(kernel_size=3, stride=2, padding=1)\n",
      "  (bb_0_0): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(64, 64, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (short): ConvBNLayer(\n",
      "      (conv): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_0_1): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 64, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_0_2): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 64, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(64, 64, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=64, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(64, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_1_0): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 128, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(128, 128, kernel_size=[3, 3], stride=[2, 2], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(128, 512, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (short): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 512, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_1_1): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(512, 128, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(128, 512, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_1_2): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(512, 128, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(128, 512, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_1_3): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(512, 128, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(128, 128, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=128, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(128, 512, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_2_0): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(512, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 256, kernel_size=[3, 3], stride=[2, 2], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (short): ConvBNLayer(\n",
      "      (conv): Conv2D(512, 1024, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_2_1): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_2_2): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_2_3): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_2_4): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_2_5): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(1024, 256, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 256, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=256, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(256, 1024, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=1024, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_3_0): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(1024, 512, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(512, 512, kernel_size=[3, 3], stride=[2, 2], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(512, 2048, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (short): ConvBNLayer(\n",
      "      (conv): Conv2D(1024, 2048, kernel_size=[1, 1], stride=[2, 2], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_3_1): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(2048, 512, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(512, 512, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(512, 2048, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (bb_3_2): BottleneckBlock(\n",
      "    (conv0): ConvBNLayer(\n",
      "      (conv): Conv2D(2048, 512, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv1): ConvBNLayer(\n",
      "      (conv): Conv2D(512, 512, kernel_size=[3, 3], padding=1, data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=512, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "    (conv2): ConvBNLayer(\n",
      "      (conv): Conv2D(512, 2048, kernel_size=[1, 1], data_format=NCHW)\n",
      "      (batch_norm): BatchNorm2D(num_features=2048, momentum=0.9, epsilon=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (pool2d_avg): AdaptiveAvgPool2D(output_size=1)\n",
      "  (out): Linear(in_features=2048, out_features=1, dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T07:35:52.629680Z",
     "start_time": "2024-06-07T07:35:51.199524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from paddle.vision import resnet50\n",
    "\n",
    "model = resnet50()\n",
    "x = paddle.rand([1,3,224,224])\n",
    "out = model(x)\n",
    "print(out.shape)"
   ],
   "id": "26f0872cbd373d9a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1000]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T08:11:55.192926Z",
     "start_time": "2024-06-07T07:35:52.630682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from paddle.vision.models import resnet50\n",
    "from paddle.vision.datasets import Cifar10\n",
    "from paddle.optimizer import Momentum\n",
    "from paddle.regularizer import L2Decay\n",
    "from paddle.nn import CrossEntropyLoss\n",
    "from paddle.metric import Accuracy\n",
    "from paddle.vision.transforms import Transpose\n",
    "paddle.vision.set_image_backend('cv2')\n",
    "model = paddle.Model(resnet50(pretrained=False, num_classes=10))\n",
    "train_dataset = Cifar10(mode = 'train',transform=Transpose())\n",
    "val_dataset = Cifar10(mode='test',transform=Transpose())\n",
    "\n",
    "optimizer = Momentum(learning_rate=0.01, momentum=0.9,weight_decay=L2Decay(1e-4),parameters=model.parameters())\n",
    "\n",
    "model.prepare(optimizer,CrossEntropyLoss(),Accuracy(topk=(1,5)))\n",
    "\n",
    "model.fit(train_dataset,val_dataset,epochs=50,batch_size=64,save_dir='./output',num_workers=8)"
   ],
   "id": "496d404641a18f0f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\io\\reader.py:429: UserWarning: DataLoader with multi-process mode is not supported on MacOs and Windows currently. Please use signle-process mode with num_workers = 0 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/50\n",
      "step  10/782 - loss: 7.9059 - acc_top1: 0.1094 - acc_top5: 0.4891 - 121ms/step\n",
      "step  20/782 - loss: 4.3738 - acc_top1: 0.1297 - acc_top5: 0.5461 - 86ms/step\n",
      "step  30/782 - loss: 6.7132 - acc_top1: 0.1318 - acc_top5: 0.5672 - 74ms/step\n",
      "step  40/782 - loss: 4.7281 - acc_top1: 0.1277 - acc_top5: 0.5797 - 68ms/step\n",
      "step  50/782 - loss: 9.5035 - acc_top1: 0.1313 - acc_top5: 0.5813 - 65ms/step\n",
      "step  60/782 - loss: 5.3554 - acc_top1: 0.1354 - acc_top5: 0.5813 - 62ms/step\n",
      "step  70/782 - loss: 4.5363 - acc_top1: 0.1366 - acc_top5: 0.5900 - 61ms/step\n",
      "step  80/782 - loss: 4.6638 - acc_top1: 0.1473 - acc_top5: 0.6057 - 59ms/step\n",
      "step  90/782 - loss: 3.8496 - acc_top1: 0.1550 - acc_top5: 0.6160 - 58ms/step\n",
      "step 100/782 - loss: 7.2787 - acc_top1: 0.1581 - acc_top5: 0.6272 - 57ms/step\n",
      "step 110/782 - loss: 4.0366 - acc_top1: 0.1631 - acc_top5: 0.6357 - 57ms/step\n",
      "step 120/782 - loss: 4.3811 - acc_top1: 0.1673 - acc_top5: 0.6439 - 56ms/step\n",
      "step 130/782 - loss: 3.4015 - acc_top1: 0.1694 - acc_top5: 0.6481 - 56ms/step\n",
      "step 140/782 - loss: 2.2091 - acc_top1: 0.1740 - acc_top5: 0.6530 - 56ms/step\n",
      "step 150/782 - loss: 3.0536 - acc_top1: 0.1778 - acc_top5: 0.6564 - 55ms/step\n",
      "step 160/782 - loss: 6.6203 - acc_top1: 0.1826 - acc_top5: 0.6634 - 55ms/step\n",
      "step 170/782 - loss: 2.2011 - acc_top1: 0.1853 - acc_top5: 0.6666 - 55ms/step\n",
      "step 180/782 - loss: 4.1953 - acc_top1: 0.1872 - acc_top5: 0.6707 - 55ms/step\n",
      "step 190/782 - loss: 5.8271 - acc_top1: 0.1885 - acc_top5: 0.6713 - 54ms/step\n",
      "step 200/782 - loss: 2.5269 - acc_top1: 0.1909 - acc_top5: 0.6757 - 54ms/step\n",
      "step 210/782 - loss: 3.2812 - acc_top1: 0.1939 - acc_top5: 0.6807 - 54ms/step\n",
      "step 220/782 - loss: 3.4503 - acc_top1: 0.1986 - acc_top5: 0.6839 - 54ms/step\n",
      "step 230/782 - loss: 3.4687 - acc_top1: 0.2016 - acc_top5: 0.6887 - 54ms/step\n",
      "step 240/782 - loss: 3.8904 - acc_top1: 0.2049 - acc_top5: 0.6924 - 53ms/step\n",
      "step 250/782 - loss: 2.8718 - acc_top1: 0.2076 - acc_top5: 0.6957 - 53ms/step\n",
      "step 260/782 - loss: 2.6105 - acc_top1: 0.2102 - acc_top5: 0.6989 - 53ms/step\n",
      "step 270/782 - loss: 2.8834 - acc_top1: 0.2105 - acc_top5: 0.7010 - 53ms/step\n",
      "step 280/782 - loss: 2.0989 - acc_top1: 0.2123 - acc_top5: 0.7042 - 53ms/step\n",
      "step 290/782 - loss: 5.3365 - acc_top1: 0.2158 - acc_top5: 0.7069 - 53ms/step\n",
      "step 300/782 - loss: 6.8371 - acc_top1: 0.2174 - acc_top5: 0.7099 - 53ms/step\n",
      "step 310/782 - loss: 2.5251 - acc_top1: 0.2185 - acc_top5: 0.7127 - 53ms/step\n",
      "step 320/782 - loss: 2.5552 - acc_top1: 0.2200 - acc_top5: 0.7153 - 53ms/step\n",
      "step 330/782 - loss: 3.5591 - acc_top1: 0.2227 - acc_top5: 0.7180 - 53ms/step\n",
      "step 340/782 - loss: 2.0095 - acc_top1: 0.2244 - acc_top5: 0.7203 - 53ms/step\n",
      "step 350/782 - loss: 3.8204 - acc_top1: 0.2264 - acc_top5: 0.7212 - 53ms/step\n",
      "step 360/782 - loss: 3.4347 - acc_top1: 0.2282 - acc_top5: 0.7223 - 52ms/step\n",
      "step 370/782 - loss: 5.6627 - acc_top1: 0.2306 - acc_top5: 0.7243 - 52ms/step\n",
      "step 380/782 - loss: 3.3251 - acc_top1: 0.2320 - acc_top5: 0.7256 - 52ms/step\n",
      "step 390/782 - loss: 4.5216 - acc_top1: 0.2324 - acc_top5: 0.7260 - 52ms/step\n",
      "step 400/782 - loss: 3.6669 - acc_top1: 0.2327 - acc_top5: 0.7272 - 52ms/step\n",
      "step 410/782 - loss: 2.1694 - acc_top1: 0.2329 - acc_top5: 0.7285 - 52ms/step\n",
      "step 420/782 - loss: 4.8760 - acc_top1: 0.2335 - acc_top5: 0.7298 - 52ms/step\n",
      "step 430/782 - loss: 3.7216 - acc_top1: 0.2336 - acc_top5: 0.7296 - 52ms/step\n",
      "step 440/782 - loss: 3.4792 - acc_top1: 0.2327 - acc_top5: 0.7281 - 52ms/step\n",
      "step 450/782 - loss: 2.3018 - acc_top1: 0.2318 - acc_top5: 0.7275 - 52ms/step\n",
      "step 460/782 - loss: 5.4689 - acc_top1: 0.2324 - acc_top5: 0.7272 - 52ms/step\n",
      "step 470/782 - loss: 4.3261 - acc_top1: 0.2312 - acc_top5: 0.7263 - 52ms/step\n",
      "step 480/782 - loss: 2.0191 - acc_top1: 0.2308 - acc_top5: 0.7257 - 52ms/step\n",
      "step 490/782 - loss: 2.0002 - acc_top1: 0.2306 - acc_top5: 0.7258 - 52ms/step\n",
      "step 500/782 - loss: 2.9023 - acc_top1: 0.2302 - acc_top5: 0.7253 - 52ms/step\n",
      "step 510/782 - loss: 1.9522 - acc_top1: 0.2308 - acc_top5: 0.7264 - 52ms/step\n",
      "step 520/782 - loss: 2.2139 - acc_top1: 0.2310 - acc_top5: 0.7267 - 52ms/step\n",
      "step 530/782 - loss: 2.8624 - acc_top1: 0.2313 - acc_top5: 0.7267 - 52ms/step\n",
      "step 540/782 - loss: 7.6796 - acc_top1: 0.2309 - acc_top5: 0.7271 - 52ms/step\n",
      "step 550/782 - loss: 3.1166 - acc_top1: 0.2312 - acc_top5: 0.7275 - 52ms/step\n",
      "step 560/782 - loss: 1.9891 - acc_top1: 0.2317 - acc_top5: 0.7283 - 52ms/step\n",
      "step 570/782 - loss: 3.3919 - acc_top1: 0.2319 - acc_top5: 0.7294 - 52ms/step\n",
      "step 580/782 - loss: 2.5378 - acc_top1: 0.2323 - acc_top5: 0.7305 - 52ms/step\n",
      "step 590/782 - loss: 1.7963 - acc_top1: 0.2333 - acc_top5: 0.7316 - 52ms/step\n",
      "step 600/782 - loss: 3.6578 - acc_top1: 0.2343 - acc_top5: 0.7326 - 52ms/step\n",
      "step 610/782 - loss: 2.2953 - acc_top1: 0.2339 - acc_top5: 0.7326 - 52ms/step\n",
      "step 620/782 - loss: 3.2467 - acc_top1: 0.2337 - acc_top5: 0.7327 - 52ms/step\n",
      "step 630/782 - loss: 5.6680 - acc_top1: 0.2335 - acc_top5: 0.7325 - 52ms/step\n",
      "step 640/782 - loss: 5.2625 - acc_top1: 0.2331 - acc_top5: 0.7322 - 52ms/step\n",
      "step 650/782 - loss: 2.2057 - acc_top1: 0.2327 - acc_top5: 0.7320 - 52ms/step\n",
      "step 660/782 - loss: 4.7464 - acc_top1: 0.2324 - acc_top5: 0.7320 - 52ms/step\n",
      "step 670/782 - loss: 2.1121 - acc_top1: 0.2324 - acc_top5: 0.7321 - 52ms/step\n",
      "step 680/782 - loss: 2.2907 - acc_top1: 0.2327 - acc_top5: 0.7318 - 52ms/step\n",
      "step 690/782 - loss: 3.7068 - acc_top1: 0.2319 - acc_top5: 0.7302 - 52ms/step\n",
      "step 700/782 - loss: 2.6919 - acc_top1: 0.2309 - acc_top5: 0.7289 - 52ms/step\n",
      "step 710/782 - loss: 2.3299 - acc_top1: 0.2303 - acc_top5: 0.7283 - 52ms/step\n",
      "step 720/782 - loss: 4.0211 - acc_top1: 0.2299 - acc_top5: 0.7275 - 52ms/step\n",
      "step 730/782 - loss: 3.4418 - acc_top1: 0.2295 - acc_top5: 0.7269 - 52ms/step\n",
      "step 740/782 - loss: 2.4059 - acc_top1: 0.2289 - acc_top5: 0.7263 - 52ms/step\n",
      "step 750/782 - loss: 3.6201 - acc_top1: 0.2284 - acc_top5: 0.7257 - 51ms/step\n",
      "step 760/782 - loss: 3.8451 - acc_top1: 0.2278 - acc_top5: 0.7253 - 51ms/step\n",
      "step 770/782 - loss: 2.8563 - acc_top1: 0.2277 - acc_top5: 0.7255 - 51ms/step\n",
      "step 780/782 - loss: 2.8029 - acc_top1: 0.2274 - acc_top5: 0.7258 - 51ms/step\n",
      "step 782/782 - loss: 3.5576 - acc_top1: 0.2273 - acc_top5: 0.7257 - 52ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\0\n",
      "Eval begin...\n",
      "step  10/157 - loss: 2.3568 - acc_top1: 0.1469 - acc_top5: 0.6359 - 19ms/step\n",
      "step  20/157 - loss: 2.2317 - acc_top1: 0.1508 - acc_top5: 0.6391 - 20ms/step\n",
      "step  30/157 - loss: 2.2907 - acc_top1: 0.1469 - acc_top5: 0.6260 - 20ms/step\n",
      "step  40/157 - loss: 2.3569 - acc_top1: 0.1527 - acc_top5: 0.6289 - 20ms/step\n",
      "step  50/157 - loss: 2.3040 - acc_top1: 0.1575 - acc_top5: 0.6278 - 20ms/step\n",
      "step  60/157 - loss: 2.2028 - acc_top1: 0.1539 - acc_top5: 0.6234 - 20ms/step\n",
      "step  70/157 - loss: 2.3545 - acc_top1: 0.1522 - acc_top5: 0.6275 - 20ms/step\n",
      "step  80/157 - loss: 2.2717 - acc_top1: 0.1514 - acc_top5: 0.6297 - 20ms/step\n",
      "step  90/157 - loss: 8.8233 - acc_top1: 0.1524 - acc_top5: 0.6280 - 20ms/step\n",
      "step 100/157 - loss: 2.1711 - acc_top1: 0.1553 - acc_top5: 0.6303 - 20ms/step\n",
      "step 110/157 - loss: 3.8263 - acc_top1: 0.1567 - acc_top5: 0.6312 - 20ms/step\n",
      "step 120/157 - loss: 2.5004 - acc_top1: 0.1560 - acc_top5: 0.6315 - 20ms/step\n",
      "step 130/157 - loss: 2.4095 - acc_top1: 0.1556 - acc_top5: 0.6305 - 20ms/step\n",
      "step 140/157 - loss: 2.3571 - acc_top1: 0.1565 - acc_top5: 0.6279 - 20ms/step\n",
      "step 150/157 - loss: 5.7369 - acc_top1: 0.1568 - acc_top5: 0.6284 - 20ms/step\n",
      "step 157/157 - loss: 2.0079 - acc_top1: 0.1575 - acc_top5: 0.6284 - 20ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 2.1821 - acc_top1: 0.1781 - acc_top5: 0.6094 - 53ms/step\n",
      "step  20/782 - loss: 3.8519 - acc_top1: 0.1625 - acc_top5: 0.6227 - 52ms/step\n",
      "step  30/782 - loss: 3.7383 - acc_top1: 0.1667 - acc_top5: 0.6286 - 51ms/step\n",
      "step  40/782 - loss: 2.4963 - acc_top1: 0.1797 - acc_top5: 0.6484 - 51ms/step\n",
      "step  50/782 - loss: 2.9330 - acc_top1: 0.1888 - acc_top5: 0.6516 - 51ms/step\n",
      "step  60/782 - loss: 2.8296 - acc_top1: 0.1906 - acc_top5: 0.6667 - 51ms/step\n",
      "step  70/782 - loss: 2.5877 - acc_top1: 0.1897 - acc_top5: 0.6708 - 50ms/step\n",
      "step  80/782 - loss: 3.0268 - acc_top1: 0.1936 - acc_top5: 0.6762 - 50ms/step\n",
      "step  90/782 - loss: 2.1350 - acc_top1: 0.1970 - acc_top5: 0.6819 - 50ms/step\n",
      "step 100/782 - loss: 3.9354 - acc_top1: 0.1988 - acc_top5: 0.6881 - 50ms/step\n",
      "step 110/782 - loss: 2.9908 - acc_top1: 0.2024 - acc_top5: 0.6928 - 51ms/step\n",
      "step 120/782 - loss: 4.1277 - acc_top1: 0.2044 - acc_top5: 0.6951 - 51ms/step\n",
      "step 130/782 - loss: 2.7165 - acc_top1: 0.2061 - acc_top5: 0.6957 - 50ms/step\n",
      "step 140/782 - loss: 2.6916 - acc_top1: 0.2089 - acc_top5: 0.6987 - 50ms/step\n",
      "step 150/782 - loss: 2.2878 - acc_top1: 0.2096 - acc_top5: 0.7011 - 50ms/step\n",
      "step 160/782 - loss: 2.7647 - acc_top1: 0.2104 - acc_top5: 0.7048 - 50ms/step\n",
      "step 170/782 - loss: 4.1141 - acc_top1: 0.2132 - acc_top5: 0.7068 - 50ms/step\n",
      "step 180/782 - loss: 3.0053 - acc_top1: 0.2141 - acc_top5: 0.7069 - 50ms/step\n",
      "step 190/782 - loss: 4.9396 - acc_top1: 0.2124 - acc_top5: 0.7094 - 50ms/step\n",
      "step 200/782 - loss: 2.3722 - acc_top1: 0.2124 - acc_top5: 0.7107 - 50ms/step\n",
      "step 210/782 - loss: 3.7504 - acc_top1: 0.2134 - acc_top5: 0.7115 - 50ms/step\n",
      "step 220/782 - loss: 2.6708 - acc_top1: 0.2149 - acc_top5: 0.7114 - 50ms/step\n",
      "step 230/782 - loss: 2.5522 - acc_top1: 0.2164 - acc_top5: 0.7139 - 50ms/step\n",
      "step 240/782 - loss: 2.6284 - acc_top1: 0.2174 - acc_top5: 0.7162 - 50ms/step\n",
      "step 250/782 - loss: 3.3592 - acc_top1: 0.2176 - acc_top5: 0.7188 - 50ms/step\n",
      "step 260/782 - loss: 2.3346 - acc_top1: 0.2175 - acc_top5: 0.7200 - 50ms/step\n",
      "step 270/782 - loss: 2.0967 - acc_top1: 0.2171 - acc_top5: 0.7214 - 50ms/step\n",
      "step 280/782 - loss: 2.1677 - acc_top1: 0.2179 - acc_top5: 0.7240 - 50ms/step\n",
      "step 290/782 - loss: 2.6087 - acc_top1: 0.2191 - acc_top5: 0.7268 - 50ms/step\n",
      "step 300/782 - loss: 2.5600 - acc_top1: 0.2202 - acc_top5: 0.7290 - 50ms/step\n",
      "step 310/782 - loss: 2.7368 - acc_top1: 0.2214 - acc_top5: 0.7313 - 50ms/step\n",
      "step 320/782 - loss: 5.3089 - acc_top1: 0.2226 - acc_top5: 0.7329 - 50ms/step\n",
      "step 330/782 - loss: 2.0085 - acc_top1: 0.2223 - acc_top5: 0.7334 - 50ms/step\n",
      "step 340/782 - loss: 2.5603 - acc_top1: 0.2231 - acc_top5: 0.7340 - 50ms/step\n",
      "step 350/782 - loss: 2.5414 - acc_top1: 0.2239 - acc_top5: 0.7354 - 50ms/step\n",
      "step 360/782 - loss: 1.9798 - acc_top1: 0.2245 - acc_top5: 0.7367 - 50ms/step\n",
      "step 370/782 - loss: 4.1577 - acc_top1: 0.2245 - acc_top5: 0.7378 - 50ms/step\n",
      "step 380/782 - loss: 2.4225 - acc_top1: 0.2261 - acc_top5: 0.7397 - 50ms/step\n",
      "step 390/782 - loss: 2.1072 - acc_top1: 0.2275 - acc_top5: 0.7409 - 50ms/step\n",
      "step 400/782 - loss: 3.9991 - acc_top1: 0.2288 - acc_top5: 0.7428 - 50ms/step\n",
      "step 410/782 - loss: 1.8739 - acc_top1: 0.2298 - acc_top5: 0.7438 - 50ms/step\n",
      "step 420/782 - loss: 2.2837 - acc_top1: 0.2304 - acc_top5: 0.7452 - 50ms/step\n",
      "step 430/782 - loss: 5.0491 - acc_top1: 0.2311 - acc_top5: 0.7456 - 50ms/step\n",
      "step 440/782 - loss: 3.3986 - acc_top1: 0.2316 - acc_top5: 0.7467 - 50ms/step\n",
      "step 450/782 - loss: 1.8413 - acc_top1: 0.2319 - acc_top5: 0.7476 - 50ms/step\n",
      "step 460/782 - loss: 2.5307 - acc_top1: 0.2320 - acc_top5: 0.7482 - 50ms/step\n",
      "step 470/782 - loss: 1.9124 - acc_top1: 0.2318 - acc_top5: 0.7490 - 50ms/step\n",
      "step 480/782 - loss: 2.5543 - acc_top1: 0.2313 - acc_top5: 0.7497 - 50ms/step\n",
      "step 490/782 - loss: 2.3065 - acc_top1: 0.2313 - acc_top5: 0.7501 - 50ms/step\n",
      "step 500/782 - loss: 3.0365 - acc_top1: 0.2320 - acc_top5: 0.7507 - 50ms/step\n",
      "step 510/782 - loss: 4.4471 - acc_top1: 0.2321 - acc_top5: 0.7510 - 50ms/step\n",
      "step 520/782 - loss: 2.2698 - acc_top1: 0.2322 - acc_top5: 0.7515 - 50ms/step\n",
      "step 530/782 - loss: 4.4492 - acc_top1: 0.2330 - acc_top5: 0.7516 - 50ms/step\n",
      "step 540/782 - loss: 1.8837 - acc_top1: 0.2333 - acc_top5: 0.7520 - 50ms/step\n",
      "step 550/782 - loss: 2.6585 - acc_top1: 0.2330 - acc_top5: 0.7527 - 50ms/step\n",
      "step 560/782 - loss: 3.1808 - acc_top1: 0.2335 - acc_top5: 0.7528 - 50ms/step\n",
      "step 570/782 - loss: 3.9276 - acc_top1: 0.2338 - acc_top5: 0.7537 - 50ms/step\n",
      "step 580/782 - loss: 2.4343 - acc_top1: 0.2340 - acc_top5: 0.7541 - 50ms/step\n",
      "step 590/782 - loss: 2.0958 - acc_top1: 0.2342 - acc_top5: 0.7549 - 50ms/step\n",
      "step 600/782 - loss: 2.7005 - acc_top1: 0.2347 - acc_top5: 0.7551 - 50ms/step\n",
      "step 610/782 - loss: 2.7883 - acc_top1: 0.2351 - acc_top5: 0.7554 - 50ms/step\n",
      "step 620/782 - loss: 2.6796 - acc_top1: 0.2351 - acc_top5: 0.7555 - 50ms/step\n",
      "step 630/782 - loss: 1.9491 - acc_top1: 0.2356 - acc_top5: 0.7561 - 50ms/step\n",
      "step 640/782 - loss: 2.6126 - acc_top1: 0.2357 - acc_top5: 0.7562 - 50ms/step\n",
      "step 650/782 - loss: 2.2633 - acc_top1: 0.2357 - acc_top5: 0.7561 - 50ms/step\n",
      "step 660/782 - loss: 1.9170 - acc_top1: 0.2356 - acc_top5: 0.7562 - 50ms/step\n",
      "step 670/782 - loss: 3.0669 - acc_top1: 0.2364 - acc_top5: 0.7566 - 50ms/step\n",
      "step 680/782 - loss: 3.1909 - acc_top1: 0.2372 - acc_top5: 0.7576 - 50ms/step\n",
      "step 690/782 - loss: 2.8487 - acc_top1: 0.2381 - acc_top5: 0.7581 - 50ms/step\n",
      "step 700/782 - loss: 1.8161 - acc_top1: 0.2390 - acc_top5: 0.7589 - 50ms/step\n",
      "step 710/782 - loss: 2.0544 - acc_top1: 0.2393 - acc_top5: 0.7596 - 50ms/step\n",
      "step 720/782 - loss: 2.0770 - acc_top1: 0.2393 - acc_top5: 0.7597 - 50ms/step\n",
      "step 730/782 - loss: 2.3649 - acc_top1: 0.2399 - acc_top5: 0.7601 - 50ms/step\n",
      "step 740/782 - loss: 2.9580 - acc_top1: 0.2397 - acc_top5: 0.7599 - 50ms/step\n",
      "step 750/782 - loss: 5.7464 - acc_top1: 0.2399 - acc_top5: 0.7596 - 50ms/step\n",
      "step 760/782 - loss: 2.8790 - acc_top1: 0.2402 - acc_top5: 0.7601 - 50ms/step\n",
      "step 770/782 - loss: 3.1015 - acc_top1: 0.2405 - acc_top5: 0.7607 - 50ms/step\n",
      "step 780/782 - loss: 2.8342 - acc_top1: 0.2408 - acc_top5: 0.7609 - 50ms/step\n",
      "step 782/782 - loss: 2.5570 - acc_top1: 0.2407 - acc_top5: 0.7609 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\1\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.9787 - acc_top1: 0.2469 - acc_top5: 0.7672 - 20ms/step\n",
      "step  20/157 - loss: 4.2599 - acc_top1: 0.2687 - acc_top5: 0.7758 - 20ms/step\n",
      "step  30/157 - loss: 4.0107 - acc_top1: 0.2661 - acc_top5: 0.7823 - 20ms/step\n",
      "step  40/157 - loss: 2.6661 - acc_top1: 0.2723 - acc_top5: 0.7871 - 20ms/step\n",
      "step  50/157 - loss: 8.5068 - acc_top1: 0.2700 - acc_top5: 0.7831 - 20ms/step\n",
      "step  60/157 - loss: 24.5338 - acc_top1: 0.2667 - acc_top5: 0.7812 - 20ms/step\n",
      "step  70/157 - loss: 40.3763 - acc_top1: 0.2638 - acc_top5: 0.7783 - 20ms/step\n",
      "step  80/157 - loss: 2.0793 - acc_top1: 0.2621 - acc_top5: 0.7799 - 20ms/step\n",
      "step  90/157 - loss: 1.9935 - acc_top1: 0.2594 - acc_top5: 0.7788 - 20ms/step\n",
      "step 100/157 - loss: 3.8683 - acc_top1: 0.2595 - acc_top5: 0.7789 - 20ms/step\n",
      "step 110/157 - loss: 2.1119 - acc_top1: 0.2592 - acc_top5: 0.7786 - 20ms/step\n",
      "step 120/157 - loss: 2.0036 - acc_top1: 0.2589 - acc_top5: 0.7810 - 20ms/step\n",
      "step 130/157 - loss: 2.1147 - acc_top1: 0.2579 - acc_top5: 0.7812 - 20ms/step\n",
      "step 140/157 - loss: 2.0723 - acc_top1: 0.2561 - acc_top5: 0.7812 - 20ms/step\n",
      "step 150/157 - loss: 3.4536 - acc_top1: 0.2573 - acc_top5: 0.7808 - 20ms/step\n",
      "step 157/157 - loss: 1.7522 - acc_top1: 0.2565 - acc_top5: 0.7805 - 20ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 2.5417 - acc_top1: 0.2234 - acc_top5: 0.7625 - 53ms/step\n",
      "step  20/782 - loss: 3.4266 - acc_top1: 0.2484 - acc_top5: 0.7680 - 51ms/step\n",
      "step  30/782 - loss: 1.8960 - acc_top1: 0.2516 - acc_top5: 0.7724 - 52ms/step\n",
      "step  40/782 - loss: 2.9618 - acc_top1: 0.2598 - acc_top5: 0.7754 - 51ms/step\n",
      "step  50/782 - loss: 2.4197 - acc_top1: 0.2622 - acc_top5: 0.7791 - 51ms/step\n",
      "step  60/782 - loss: 2.4001 - acc_top1: 0.2682 - acc_top5: 0.7836 - 51ms/step\n",
      "step  70/782 - loss: 2.0754 - acc_top1: 0.2737 - acc_top5: 0.7915 - 51ms/step\n",
      "step  80/782 - loss: 2.3776 - acc_top1: 0.2787 - acc_top5: 0.7930 - 51ms/step\n",
      "step  90/782 - loss: 2.3784 - acc_top1: 0.2778 - acc_top5: 0.7943 - 50ms/step\n",
      "step 100/782 - loss: 2.9507 - acc_top1: 0.2773 - acc_top5: 0.7973 - 50ms/step\n",
      "step 110/782 - loss: 2.0999 - acc_top1: 0.2803 - acc_top5: 0.8007 - 50ms/step\n",
      "step 120/782 - loss: 3.5310 - acc_top1: 0.2822 - acc_top5: 0.8022 - 50ms/step\n",
      "step 130/782 - loss: 2.0690 - acc_top1: 0.2841 - acc_top5: 0.8031 - 50ms/step\n",
      "step 140/782 - loss: 3.4730 - acc_top1: 0.2847 - acc_top5: 0.8027 - 50ms/step\n",
      "step 150/782 - loss: 1.9966 - acc_top1: 0.2831 - acc_top5: 0.8014 - 50ms/step\n",
      "step 160/782 - loss: 3.1601 - acc_top1: 0.2809 - acc_top5: 0.7983 - 50ms/step\n",
      "step 170/782 - loss: 2.8189 - acc_top1: 0.2801 - acc_top5: 0.7960 - 51ms/step\n",
      "step 180/782 - loss: 2.7353 - acc_top1: 0.2786 - acc_top5: 0.7948 - 51ms/step\n",
      "step 190/782 - loss: 2.6254 - acc_top1: 0.2783 - acc_top5: 0.7956 - 51ms/step\n",
      "step 200/782 - loss: 2.5871 - acc_top1: 0.2778 - acc_top5: 0.7957 - 51ms/step\n",
      "step 210/782 - loss: 2.0501 - acc_top1: 0.2783 - acc_top5: 0.7958 - 52ms/step\n",
      "step 220/782 - loss: 2.3163 - acc_top1: 0.2793 - acc_top5: 0.7963 - 52ms/step\n",
      "step 230/782 - loss: 2.5618 - acc_top1: 0.2801 - acc_top5: 0.7966 - 52ms/step\n",
      "step 240/782 - loss: 2.0734 - acc_top1: 0.2809 - acc_top5: 0.7971 - 52ms/step\n",
      "step 250/782 - loss: 1.9297 - acc_top1: 0.2802 - acc_top5: 0.7961 - 52ms/step\n",
      "step 260/782 - loss: 2.4000 - acc_top1: 0.2824 - acc_top5: 0.7970 - 52ms/step\n",
      "step 270/782 - loss: 2.6947 - acc_top1: 0.2832 - acc_top5: 0.7975 - 52ms/step\n",
      "step 280/782 - loss: 2.6371 - acc_top1: 0.2840 - acc_top5: 0.7978 - 52ms/step\n",
      "step 290/782 - loss: 4.4887 - acc_top1: 0.2835 - acc_top5: 0.7972 - 52ms/step\n",
      "step 300/782 - loss: 1.8202 - acc_top1: 0.2835 - acc_top5: 0.7977 - 52ms/step\n",
      "step 310/782 - loss: 2.1423 - acc_top1: 0.2837 - acc_top5: 0.7977 - 51ms/step\n",
      "step 320/782 - loss: 2.3447 - acc_top1: 0.2841 - acc_top5: 0.7980 - 51ms/step\n",
      "step 330/782 - loss: 2.9615 - acc_top1: 0.2842 - acc_top5: 0.7985 - 51ms/step\n",
      "step 340/782 - loss: 1.8712 - acc_top1: 0.2849 - acc_top5: 0.7983 - 51ms/step\n",
      "step 350/782 - loss: 2.2703 - acc_top1: 0.2861 - acc_top5: 0.7993 - 51ms/step\n",
      "step 360/782 - loss: 2.0876 - acc_top1: 0.2867 - acc_top5: 0.7988 - 51ms/step\n",
      "step 370/782 - loss: 2.3742 - acc_top1: 0.2868 - acc_top5: 0.7985 - 51ms/step\n",
      "step 380/782 - loss: 2.4468 - acc_top1: 0.2875 - acc_top5: 0.7992 - 51ms/step\n",
      "step 390/782 - loss: 2.6877 - acc_top1: 0.2883 - acc_top5: 0.7991 - 51ms/step\n",
      "step 400/782 - loss: 1.9434 - acc_top1: 0.2884 - acc_top5: 0.7987 - 51ms/step\n",
      "step 410/782 - loss: 2.0981 - acc_top1: 0.2885 - acc_top5: 0.7994 - 51ms/step\n",
      "step 420/782 - loss: 2.8119 - acc_top1: 0.2888 - acc_top5: 0.7994 - 51ms/step\n",
      "step 430/782 - loss: 3.3583 - acc_top1: 0.2892 - acc_top5: 0.7993 - 51ms/step\n",
      "step 440/782 - loss: 2.0684 - acc_top1: 0.2893 - acc_top5: 0.7998 - 51ms/step\n",
      "step 450/782 - loss: 3.4003 - acc_top1: 0.2893 - acc_top5: 0.7995 - 51ms/step\n",
      "step 460/782 - loss: 3.0624 - acc_top1: 0.2897 - acc_top5: 0.7998 - 51ms/step\n",
      "step 470/782 - loss: 1.9150 - acc_top1: 0.2901 - acc_top5: 0.8003 - 51ms/step\n",
      "step 480/782 - loss: 2.4856 - acc_top1: 0.2904 - acc_top5: 0.8005 - 51ms/step\n",
      "step 490/782 - loss: 1.9676 - acc_top1: 0.2909 - acc_top5: 0.8008 - 51ms/step\n",
      "step 500/782 - loss: 1.8033 - acc_top1: 0.2921 - acc_top5: 0.8014 - 51ms/step\n",
      "step 510/782 - loss: 1.8978 - acc_top1: 0.2924 - acc_top5: 0.8017 - 51ms/step\n",
      "step 520/782 - loss: 2.3265 - acc_top1: 0.2928 - acc_top5: 0.8020 - 51ms/step\n",
      "step 530/782 - loss: 2.5870 - acc_top1: 0.2939 - acc_top5: 0.8031 - 51ms/step\n",
      "step 540/782 - loss: 3.4757 - acc_top1: 0.2948 - acc_top5: 0.8031 - 51ms/step\n",
      "step 550/782 - loss: 2.7103 - acc_top1: 0.2953 - acc_top5: 0.8035 - 51ms/step\n",
      "step 560/782 - loss: 2.3514 - acc_top1: 0.2953 - acc_top5: 0.8036 - 51ms/step\n",
      "step 570/782 - loss: 2.3860 - acc_top1: 0.2959 - acc_top5: 0.8041 - 51ms/step\n",
      "step 580/782 - loss: 1.8397 - acc_top1: 0.2966 - acc_top5: 0.8046 - 51ms/step\n",
      "step 590/782 - loss: 3.0844 - acc_top1: 0.2973 - acc_top5: 0.8051 - 51ms/step\n",
      "step 600/782 - loss: 2.3686 - acc_top1: 0.2973 - acc_top5: 0.8052 - 51ms/step\n",
      "step 610/782 - loss: 3.5493 - acc_top1: 0.2977 - acc_top5: 0.8054 - 51ms/step\n",
      "step 620/782 - loss: 2.0179 - acc_top1: 0.2979 - acc_top5: 0.8055 - 51ms/step\n",
      "step 630/782 - loss: 3.3223 - acc_top1: 0.2980 - acc_top5: 0.8058 - 51ms/step\n",
      "step 640/782 - loss: 1.8001 - acc_top1: 0.2974 - acc_top5: 0.8055 - 51ms/step\n",
      "step 650/782 - loss: 2.8216 - acc_top1: 0.2973 - acc_top5: 0.8059 - 51ms/step\n",
      "step 660/782 - loss: 1.9136 - acc_top1: 0.2980 - acc_top5: 0.8065 - 51ms/step\n",
      "step 670/782 - loss: 1.8950 - acc_top1: 0.2985 - acc_top5: 0.8066 - 51ms/step\n",
      "step 680/782 - loss: 1.9923 - acc_top1: 0.2990 - acc_top5: 0.8070 - 51ms/step\n",
      "step 690/782 - loss: 2.7385 - acc_top1: 0.2995 - acc_top5: 0.8075 - 51ms/step\n",
      "step 700/782 - loss: 2.0831 - acc_top1: 0.3000 - acc_top5: 0.8079 - 51ms/step\n",
      "step 710/782 - loss: 2.8729 - acc_top1: 0.3004 - acc_top5: 0.8083 - 51ms/step\n",
      "step 720/782 - loss: 2.1460 - acc_top1: 0.3009 - acc_top5: 0.8087 - 51ms/step\n",
      "step 730/782 - loss: 2.1080 - acc_top1: 0.3013 - acc_top5: 0.8087 - 51ms/step\n",
      "step 740/782 - loss: 2.6657 - acc_top1: 0.3016 - acc_top5: 0.8091 - 51ms/step\n",
      "step 750/782 - loss: 2.0736 - acc_top1: 0.3023 - acc_top5: 0.8097 - 51ms/step\n",
      "step 760/782 - loss: 1.6795 - acc_top1: 0.3029 - acc_top5: 0.8102 - 51ms/step\n",
      "step 770/782 - loss: 1.9193 - acc_top1: 0.3038 - acc_top5: 0.8107 - 51ms/step\n",
      "step 780/782 - loss: 2.0531 - acc_top1: 0.3046 - acc_top5: 0.8113 - 51ms/step\n",
      "step 782/782 - loss: 5.5062 - acc_top1: 0.3046 - acc_top5: 0.8113 - 51ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\2\n",
      "Eval begin...\n",
      "step  10/157 - loss: 10.6083 - acc_top1: 0.3266 - acc_top5: 0.8438 - 20ms/step\n",
      "step  20/157 - loss: 2.0077 - acc_top1: 0.3352 - acc_top5: 0.8344 - 20ms/step\n",
      "step  30/157 - loss: 1.7535 - acc_top1: 0.3385 - acc_top5: 0.8313 - 20ms/step\n",
      "step  40/157 - loss: 48.8057 - acc_top1: 0.3438 - acc_top5: 0.8348 - 20ms/step\n",
      "step  50/157 - loss: 2.7195 - acc_top1: 0.3394 - acc_top5: 0.8334 - 20ms/step\n",
      "step  60/157 - loss: 7.4378 - acc_top1: 0.3411 - acc_top5: 0.8299 - 20ms/step\n",
      "step  70/157 - loss: 6.1377 - acc_top1: 0.3373 - acc_top5: 0.8328 - 20ms/step\n",
      "step  80/157 - loss: 8.0856 - acc_top1: 0.3355 - acc_top5: 0.8363 - 20ms/step\n",
      "step  90/157 - loss: 1.9189 - acc_top1: 0.3345 - acc_top5: 0.8335 - 20ms/step\n",
      "step 100/157 - loss: 4.3536 - acc_top1: 0.3359 - acc_top5: 0.8348 - 20ms/step\n",
      "step 110/157 - loss: 2.1004 - acc_top1: 0.3349 - acc_top5: 0.8339 - 20ms/step\n",
      "step 120/157 - loss: 1.9061 - acc_top1: 0.3362 - acc_top5: 0.8350 - 20ms/step\n",
      "step 130/157 - loss: 3.9346 - acc_top1: 0.3326 - acc_top5: 0.8357 - 20ms/step\n",
      "step 140/157 - loss: 1.9580 - acc_top1: 0.3314 - acc_top5: 0.8363 - 20ms/step\n",
      "step 150/157 - loss: 5.1562 - acc_top1: 0.3325 - acc_top5: 0.8357 - 20ms/step\n",
      "step 157/157 - loss: 1.4673 - acc_top1: 0.3320 - acc_top5: 0.8360 - 20ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 2.2889 - acc_top1: 0.2906 - acc_top5: 0.8141 - 51ms/step\n",
      "step  20/782 - loss: 2.0750 - acc_top1: 0.3047 - acc_top5: 0.8094 - 51ms/step\n",
      "step  30/782 - loss: 2.2407 - acc_top1: 0.3125 - acc_top5: 0.8167 - 51ms/step\n",
      "step  40/782 - loss: 1.8311 - acc_top1: 0.3273 - acc_top5: 0.8262 - 51ms/step\n",
      "step  50/782 - loss: 1.8825 - acc_top1: 0.3272 - acc_top5: 0.8275 - 51ms/step\n",
      "step  60/782 - loss: 2.3075 - acc_top1: 0.3255 - acc_top5: 0.8292 - 50ms/step\n",
      "step  70/782 - loss: 3.5379 - acc_top1: 0.3283 - acc_top5: 0.8319 - 50ms/step\n",
      "step  80/782 - loss: 2.5087 - acc_top1: 0.3291 - acc_top5: 0.8328 - 50ms/step\n",
      "step  90/782 - loss: 1.7844 - acc_top1: 0.3281 - acc_top5: 0.8352 - 50ms/step\n",
      "step 100/782 - loss: 2.4258 - acc_top1: 0.3298 - acc_top5: 0.8378 - 50ms/step\n",
      "step 110/782 - loss: 1.8083 - acc_top1: 0.3301 - acc_top5: 0.8402 - 50ms/step\n",
      "step 120/782 - loss: 2.1726 - acc_top1: 0.3318 - acc_top5: 0.8396 - 50ms/step\n",
      "step 130/782 - loss: 2.5054 - acc_top1: 0.3319 - acc_top5: 0.8406 - 50ms/step\n",
      "step 140/782 - loss: 2.4746 - acc_top1: 0.3340 - acc_top5: 0.8416 - 50ms/step\n",
      "step 150/782 - loss: 1.9625 - acc_top1: 0.3332 - acc_top5: 0.8409 - 50ms/step\n",
      "step 160/782 - loss: 2.3037 - acc_top1: 0.3351 - acc_top5: 0.8412 - 50ms/step\n",
      "step 170/782 - loss: 2.4076 - acc_top1: 0.3339 - acc_top5: 0.8408 - 50ms/step\n",
      "step 180/782 - loss: 1.8734 - acc_top1: 0.3351 - acc_top5: 0.8397 - 50ms/step\n",
      "step 190/782 - loss: 1.8797 - acc_top1: 0.3341 - acc_top5: 0.8399 - 50ms/step\n",
      "step 200/782 - loss: 1.7263 - acc_top1: 0.3362 - acc_top5: 0.8415 - 50ms/step\n",
      "step 210/782 - loss: 2.4036 - acc_top1: 0.3362 - acc_top5: 0.8408 - 50ms/step\n",
      "step 220/782 - loss: 1.8947 - acc_top1: 0.3365 - acc_top5: 0.8418 - 50ms/step\n",
      "step 230/782 - loss: 1.9770 - acc_top1: 0.3385 - acc_top5: 0.8419 - 50ms/step\n",
      "step 240/782 - loss: 2.3990 - acc_top1: 0.3397 - acc_top5: 0.8424 - 50ms/step\n",
      "step 250/782 - loss: 1.8796 - acc_top1: 0.3401 - acc_top5: 0.8412 - 50ms/step\n",
      "step 260/782 - loss: 3.3261 - acc_top1: 0.3408 - acc_top5: 0.8424 - 50ms/step\n",
      "step 270/782 - loss: 1.7765 - acc_top1: 0.3408 - acc_top5: 0.8432 - 50ms/step\n",
      "step 280/782 - loss: 1.6126 - acc_top1: 0.3417 - acc_top5: 0.8441 - 50ms/step\n",
      "step 290/782 - loss: 2.1519 - acc_top1: 0.3429 - acc_top5: 0.8453 - 51ms/step\n",
      "step 300/782 - loss: 1.9481 - acc_top1: 0.3439 - acc_top5: 0.8467 - 51ms/step\n",
      "step 310/782 - loss: 2.3543 - acc_top1: 0.3443 - acc_top5: 0.8472 - 51ms/step\n",
      "step 320/782 - loss: 2.3736 - acc_top1: 0.3443 - acc_top5: 0.8476 - 51ms/step\n",
      "step 330/782 - loss: 2.7530 - acc_top1: 0.3449 - acc_top5: 0.8480 - 51ms/step\n",
      "step 340/782 - loss: 1.7698 - acc_top1: 0.3456 - acc_top5: 0.8483 - 51ms/step\n",
      "step 350/782 - loss: 2.1426 - acc_top1: 0.3460 - acc_top5: 0.8487 - 51ms/step\n",
      "step 360/782 - loss: 2.0936 - acc_top1: 0.3457 - acc_top5: 0.8485 - 51ms/step\n",
      "step 370/782 - loss: 1.5661 - acc_top1: 0.3473 - acc_top5: 0.8499 - 51ms/step\n",
      "step 380/782 - loss: 2.3390 - acc_top1: 0.3483 - acc_top5: 0.8498 - 51ms/step\n",
      "step 390/782 - loss: 2.5409 - acc_top1: 0.3494 - acc_top5: 0.8500 - 51ms/step\n",
      "step 400/782 - loss: 1.9553 - acc_top1: 0.3501 - acc_top5: 0.8504 - 51ms/step\n",
      "step 410/782 - loss: 3.3965 - acc_top1: 0.3510 - acc_top5: 0.8512 - 51ms/step\n",
      "step 420/782 - loss: 1.9523 - acc_top1: 0.3510 - acc_top5: 0.8512 - 51ms/step\n",
      "step 430/782 - loss: 2.6739 - acc_top1: 0.3521 - acc_top5: 0.8511 - 51ms/step\n",
      "step 440/782 - loss: 1.9786 - acc_top1: 0.3528 - acc_top5: 0.8517 - 51ms/step\n",
      "step 450/782 - loss: 2.1146 - acc_top1: 0.3530 - acc_top5: 0.8520 - 51ms/step\n",
      "step 460/782 - loss: 3.4523 - acc_top1: 0.3530 - acc_top5: 0.8517 - 51ms/step\n",
      "step 470/782 - loss: 2.1086 - acc_top1: 0.3532 - acc_top5: 0.8522 - 51ms/step\n",
      "step 480/782 - loss: 2.3980 - acc_top1: 0.3526 - acc_top5: 0.8523 - 51ms/step\n",
      "step 490/782 - loss: 2.4416 - acc_top1: 0.3537 - acc_top5: 0.8530 - 51ms/step\n",
      "step 500/782 - loss: 1.7053 - acc_top1: 0.3542 - acc_top5: 0.8536 - 51ms/step\n",
      "step 510/782 - loss: 2.0652 - acc_top1: 0.3542 - acc_top5: 0.8539 - 51ms/step\n",
      "step 520/782 - loss: 1.6854 - acc_top1: 0.3546 - acc_top5: 0.8542 - 51ms/step\n",
      "step 530/782 - loss: 2.0072 - acc_top1: 0.3550 - acc_top5: 0.8548 - 51ms/step\n",
      "step 540/782 - loss: 2.0608 - acc_top1: 0.3555 - acc_top5: 0.8550 - 51ms/step\n",
      "step 550/782 - loss: 2.0260 - acc_top1: 0.3551 - acc_top5: 0.8556 - 51ms/step\n",
      "step 560/782 - loss: 1.6942 - acc_top1: 0.3552 - acc_top5: 0.8559 - 51ms/step\n",
      "step 570/782 - loss: 1.8032 - acc_top1: 0.3557 - acc_top5: 0.8566 - 51ms/step\n",
      "step 580/782 - loss: 2.2513 - acc_top1: 0.3559 - acc_top5: 0.8564 - 51ms/step\n",
      "step 590/782 - loss: 2.1943 - acc_top1: 0.3557 - acc_top5: 0.8566 - 51ms/step\n",
      "step 600/782 - loss: 2.0336 - acc_top1: 0.3557 - acc_top5: 0.8571 - 51ms/step\n",
      "step 610/782 - loss: 2.7920 - acc_top1: 0.3559 - acc_top5: 0.8574 - 51ms/step\n",
      "step 620/782 - loss: 2.3171 - acc_top1: 0.3557 - acc_top5: 0.8571 - 51ms/step\n",
      "step 630/782 - loss: 2.4404 - acc_top1: 0.3563 - acc_top5: 0.8572 - 51ms/step\n",
      "step 640/782 - loss: 2.2972 - acc_top1: 0.3563 - acc_top5: 0.8571 - 51ms/step\n",
      "step 650/782 - loss: 2.3774 - acc_top1: 0.3562 - acc_top5: 0.8568 - 51ms/step\n",
      "step 660/782 - loss: 2.1556 - acc_top1: 0.3559 - acc_top5: 0.8571 - 51ms/step\n",
      "step 670/782 - loss: 2.5922 - acc_top1: 0.3562 - acc_top5: 0.8576 - 51ms/step\n",
      "step 680/782 - loss: 2.1847 - acc_top1: 0.3562 - acc_top5: 0.8575 - 51ms/step\n",
      "step 690/782 - loss: 1.5632 - acc_top1: 0.3560 - acc_top5: 0.8578 - 51ms/step\n",
      "step 700/782 - loss: 3.0915 - acc_top1: 0.3556 - acc_top5: 0.8574 - 51ms/step\n",
      "step 710/782 - loss: 1.5231 - acc_top1: 0.3561 - acc_top5: 0.8576 - 51ms/step\n",
      "step 720/782 - loss: 1.6971 - acc_top1: 0.3558 - acc_top5: 0.8576 - 51ms/step\n",
      "step 730/782 - loss: 1.9343 - acc_top1: 0.3554 - acc_top5: 0.8579 - 51ms/step\n",
      "step 740/782 - loss: 2.8904 - acc_top1: 0.3556 - acc_top5: 0.8579 - 51ms/step\n",
      "step 750/782 - loss: 2.1710 - acc_top1: 0.3553 - acc_top5: 0.8579 - 51ms/step\n",
      "step 760/782 - loss: 1.3906 - acc_top1: 0.3556 - acc_top5: 0.8582 - 51ms/step\n",
      "step 770/782 - loss: 1.7675 - acc_top1: 0.3558 - acc_top5: 0.8588 - 51ms/step\n",
      "step 780/782 - loss: 1.6535 - acc_top1: 0.3565 - acc_top5: 0.8589 - 51ms/step\n",
      "step 782/782 - loss: 3.6504 - acc_top1: 0.3565 - acc_top5: 0.8589 - 51ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\3\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.7259 - acc_top1: 0.3375 - acc_top5: 0.8922 - 19ms/step\n",
      "step  20/157 - loss: 22.7715 - acc_top1: 0.3453 - acc_top5: 0.8789 - 19ms/step\n",
      "step  30/157 - loss: 1.7456 - acc_top1: 0.3563 - acc_top5: 0.8703 - 19ms/step\n",
      "step  40/157 - loss: 4.4037 - acc_top1: 0.3602 - acc_top5: 0.8707 - 19ms/step\n",
      "step  50/157 - loss: 1.8630 - acc_top1: 0.3609 - acc_top5: 0.8722 - 19ms/step\n",
      "step  60/157 - loss: 1.4898 - acc_top1: 0.3602 - acc_top5: 0.8674 - 19ms/step\n",
      "step  70/157 - loss: 16.9707 - acc_top1: 0.3600 - acc_top5: 0.8650 - 19ms/step\n",
      "step  80/157 - loss: 15.4627 - acc_top1: 0.3631 - acc_top5: 0.8693 - 19ms/step\n",
      "step  90/157 - loss: 1.7634 - acc_top1: 0.3611 - acc_top5: 0.8691 - 19ms/step\n",
      "step 100/157 - loss: 3.2471 - acc_top1: 0.3603 - acc_top5: 0.8689 - 19ms/step\n",
      "step 110/157 - loss: 29.4836 - acc_top1: 0.3625 - acc_top5: 0.8680 - 19ms/step\n",
      "step 120/157 - loss: 1.7894 - acc_top1: 0.3634 - acc_top5: 0.8694 - 19ms/step\n",
      "step 130/157 - loss: 1.9727 - acc_top1: 0.3633 - acc_top5: 0.8680 - 19ms/step\n",
      "step 140/157 - loss: 1.8567 - acc_top1: 0.3622 - acc_top5: 0.8662 - 19ms/step\n",
      "step 150/157 - loss: 6.9131 - acc_top1: 0.3626 - acc_top5: 0.8677 - 19ms/step\n",
      "step 157/157 - loss: 1.2119 - acc_top1: 0.3606 - acc_top5: 0.8685 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 2.0424 - acc_top1: 0.3250 - acc_top5: 0.8391 - 52ms/step\n",
      "step  20/782 - loss: 1.7852 - acc_top1: 0.3211 - acc_top5: 0.8484 - 51ms/step\n",
      "step  30/782 - loss: 1.9626 - acc_top1: 0.3286 - acc_top5: 0.8490 - 50ms/step\n",
      "step  40/782 - loss: 1.7008 - acc_top1: 0.3375 - acc_top5: 0.8531 - 50ms/step\n",
      "step  50/782 - loss: 1.8049 - acc_top1: 0.3409 - acc_top5: 0.8591 - 50ms/step\n",
      "step  60/782 - loss: 2.4071 - acc_top1: 0.3417 - acc_top5: 0.8607 - 50ms/step\n",
      "step  70/782 - loss: 2.7138 - acc_top1: 0.3451 - acc_top5: 0.8605 - 50ms/step\n",
      "step  80/782 - loss: 2.3616 - acc_top1: 0.3463 - acc_top5: 0.8605 - 50ms/step\n",
      "step  90/782 - loss: 2.2987 - acc_top1: 0.3472 - acc_top5: 0.8648 - 50ms/step\n",
      "step 100/782 - loss: 2.4351 - acc_top1: 0.3481 - acc_top5: 0.8648 - 50ms/step\n",
      "step 110/782 - loss: 1.7612 - acc_top1: 0.3470 - acc_top5: 0.8645 - 50ms/step\n",
      "step 120/782 - loss: 4.2652 - acc_top1: 0.3483 - acc_top5: 0.8663 - 50ms/step\n",
      "step 130/782 - loss: 1.8592 - acc_top1: 0.3482 - acc_top5: 0.8671 - 50ms/step\n",
      "step 140/782 - loss: 2.7333 - acc_top1: 0.3490 - acc_top5: 0.8670 - 50ms/step\n",
      "step 150/782 - loss: 2.9190 - acc_top1: 0.3497 - acc_top5: 0.8673 - 50ms/step\n",
      "step 160/782 - loss: 1.9476 - acc_top1: 0.3540 - acc_top5: 0.8681 - 50ms/step\n",
      "step 170/782 - loss: 2.2619 - acc_top1: 0.3558 - acc_top5: 0.8691 - 50ms/step\n",
      "step 180/782 - loss: 2.2962 - acc_top1: 0.3574 - acc_top5: 0.8698 - 50ms/step\n",
      "step 190/782 - loss: 1.8506 - acc_top1: 0.3590 - acc_top5: 0.8688 - 50ms/step\n",
      "step 200/782 - loss: 2.2215 - acc_top1: 0.3602 - acc_top5: 0.8688 - 50ms/step\n",
      "step 210/782 - loss: 1.5935 - acc_top1: 0.3615 - acc_top5: 0.8708 - 50ms/step\n",
      "step 220/782 - loss: 3.0138 - acc_top1: 0.3622 - acc_top5: 0.8702 - 50ms/step\n",
      "step 230/782 - loss: 1.8393 - acc_top1: 0.3620 - acc_top5: 0.8692 - 50ms/step\n",
      "step 240/782 - loss: 1.7712 - acc_top1: 0.3626 - acc_top5: 0.8695 - 50ms/step\n",
      "step 250/782 - loss: 1.4820 - acc_top1: 0.3646 - acc_top5: 0.8712 - 50ms/step\n",
      "step 260/782 - loss: 1.5966 - acc_top1: 0.3657 - acc_top5: 0.8719 - 50ms/step\n",
      "step 270/782 - loss: 2.4296 - acc_top1: 0.3668 - acc_top5: 0.8719 - 50ms/step\n",
      "step 280/782 - loss: 2.4259 - acc_top1: 0.3673 - acc_top5: 0.8716 - 50ms/step\n",
      "step 290/782 - loss: 1.5462 - acc_top1: 0.3689 - acc_top5: 0.8721 - 50ms/step\n",
      "step 300/782 - loss: 2.2583 - acc_top1: 0.3692 - acc_top5: 0.8719 - 50ms/step\n",
      "step 310/782 - loss: 1.8930 - acc_top1: 0.3690 - acc_top5: 0.8723 - 50ms/step\n",
      "step 320/782 - loss: 1.8902 - acc_top1: 0.3685 - acc_top5: 0.8723 - 50ms/step\n",
      "step 330/782 - loss: 1.5226 - acc_top1: 0.3689 - acc_top5: 0.8725 - 50ms/step\n",
      "step 340/782 - loss: 1.6725 - acc_top1: 0.3684 - acc_top5: 0.8728 - 50ms/step\n",
      "step 350/782 - loss: 2.0042 - acc_top1: 0.3686 - acc_top5: 0.8725 - 50ms/step\n",
      "step 360/782 - loss: 2.0348 - acc_top1: 0.3700 - acc_top5: 0.8731 - 50ms/step\n",
      "step 370/782 - loss: 1.5985 - acc_top1: 0.3704 - acc_top5: 0.8736 - 50ms/step\n",
      "step 380/782 - loss: 1.9946 - acc_top1: 0.3714 - acc_top5: 0.8738 - 50ms/step\n",
      "step 390/782 - loss: 1.5960 - acc_top1: 0.3727 - acc_top5: 0.8746 - 50ms/step\n",
      "step 400/782 - loss: 1.4816 - acc_top1: 0.3738 - acc_top5: 0.8751 - 50ms/step\n",
      "step 410/782 - loss: 2.5577 - acc_top1: 0.3736 - acc_top5: 0.8753 - 50ms/step\n",
      "step 420/782 - loss: 1.9291 - acc_top1: 0.3731 - acc_top5: 0.8753 - 50ms/step\n",
      "step 430/782 - loss: 1.6681 - acc_top1: 0.3738 - acc_top5: 0.8758 - 50ms/step\n",
      "step 440/782 - loss: 2.1734 - acc_top1: 0.3748 - acc_top5: 0.8760 - 50ms/step\n",
      "step 450/782 - loss: 3.3509 - acc_top1: 0.3748 - acc_top5: 0.8767 - 50ms/step\n",
      "step 460/782 - loss: 1.5564 - acc_top1: 0.3755 - acc_top5: 0.8776 - 50ms/step\n",
      "step 470/782 - loss: 2.4797 - acc_top1: 0.3762 - acc_top5: 0.8782 - 50ms/step\n",
      "step 480/782 - loss: 1.7366 - acc_top1: 0.3770 - acc_top5: 0.8784 - 50ms/step\n",
      "step 490/782 - loss: 1.6043 - acc_top1: 0.3774 - acc_top5: 0.8785 - 50ms/step\n",
      "step 500/782 - loss: 1.7969 - acc_top1: 0.3777 - acc_top5: 0.8787 - 50ms/step\n",
      "step 510/782 - loss: 2.1826 - acc_top1: 0.3782 - acc_top5: 0.8787 - 50ms/step\n",
      "step 520/782 - loss: 2.5634 - acc_top1: 0.3794 - acc_top5: 0.8789 - 50ms/step\n",
      "step 530/782 - loss: 1.7178 - acc_top1: 0.3803 - acc_top5: 0.8790 - 50ms/step\n",
      "step 540/782 - loss: 2.3450 - acc_top1: 0.3804 - acc_top5: 0.8793 - 50ms/step\n",
      "step 550/782 - loss: 1.9790 - acc_top1: 0.3810 - acc_top5: 0.8793 - 50ms/step\n",
      "step 560/782 - loss: 1.8141 - acc_top1: 0.3816 - acc_top5: 0.8794 - 50ms/step\n",
      "step 570/782 - loss: 2.0187 - acc_top1: 0.3820 - acc_top5: 0.8797 - 50ms/step\n",
      "step 580/782 - loss: 1.5165 - acc_top1: 0.3822 - acc_top5: 0.8796 - 50ms/step\n",
      "step 590/782 - loss: 1.7375 - acc_top1: 0.3828 - acc_top5: 0.8800 - 50ms/step\n",
      "step 600/782 - loss: 1.6938 - acc_top1: 0.3835 - acc_top5: 0.8803 - 50ms/step\n",
      "step 610/782 - loss: 2.0222 - acc_top1: 0.3835 - acc_top5: 0.8800 - 50ms/step\n",
      "step 620/782 - loss: 1.6661 - acc_top1: 0.3836 - acc_top5: 0.8800 - 50ms/step\n",
      "step 630/782 - loss: 1.9857 - acc_top1: 0.3842 - acc_top5: 0.8806 - 50ms/step\n",
      "step 640/782 - loss: 1.9490 - acc_top1: 0.3843 - acc_top5: 0.8806 - 50ms/step\n",
      "step 650/782 - loss: 1.8277 - acc_top1: 0.3845 - acc_top5: 0.8806 - 50ms/step\n",
      "step 660/782 - loss: 2.7723 - acc_top1: 0.3853 - acc_top5: 0.8812 - 50ms/step\n",
      "step 670/782 - loss: 2.1398 - acc_top1: 0.3855 - acc_top5: 0.8812 - 50ms/step\n",
      "step 680/782 - loss: 1.9285 - acc_top1: 0.3860 - acc_top5: 0.8817 - 50ms/step\n",
      "step 690/782 - loss: 1.8070 - acc_top1: 0.3868 - acc_top5: 0.8817 - 50ms/step\n",
      "step 700/782 - loss: 1.6641 - acc_top1: 0.3872 - acc_top5: 0.8818 - 50ms/step\n",
      "step 710/782 - loss: 1.5518 - acc_top1: 0.3873 - acc_top5: 0.8819 - 50ms/step\n",
      "step 720/782 - loss: 1.7143 - acc_top1: 0.3874 - acc_top5: 0.8820 - 50ms/step\n",
      "step 730/782 - loss: 1.6443 - acc_top1: 0.3876 - acc_top5: 0.8820 - 50ms/step\n",
      "step 740/782 - loss: 1.7733 - acc_top1: 0.3875 - acc_top5: 0.8822 - 50ms/step\n",
      "step 750/782 - loss: 2.0182 - acc_top1: 0.3877 - acc_top5: 0.8824 - 50ms/step\n",
      "step 760/782 - loss: 1.6243 - acc_top1: 0.3882 - acc_top5: 0.8828 - 50ms/step\n",
      "step 770/782 - loss: 1.6896 - acc_top1: 0.3891 - acc_top5: 0.8833 - 50ms/step\n",
      "step 780/782 - loss: 1.4912 - acc_top1: 0.3897 - acc_top5: 0.8837 - 50ms/step\n",
      "step 782/782 - loss: 1.4627 - acc_top1: 0.3898 - acc_top5: 0.8837 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\4\n",
      "Eval begin...\n",
      "step  10/157 - loss: 6.4015 - acc_top1: 0.3922 - acc_top5: 0.8828 - 19ms/step\n",
      "step  20/157 - loss: 5.0847 - acc_top1: 0.4031 - acc_top5: 0.8836 - 19ms/step\n",
      "step  30/157 - loss: 3.0092 - acc_top1: 0.4073 - acc_top5: 0.8896 - 19ms/step\n",
      "step  40/157 - loss: 5.7605 - acc_top1: 0.4121 - acc_top5: 0.8910 - 19ms/step\n",
      "step  50/157 - loss: 1.5843 - acc_top1: 0.4113 - acc_top5: 0.8962 - 19ms/step\n",
      "step  60/157 - loss: 1.3731 - acc_top1: 0.4115 - acc_top5: 0.8953 - 19ms/step\n",
      "step  70/157 - loss: 19.8607 - acc_top1: 0.4118 - acc_top5: 0.8967 - 19ms/step\n",
      "step  80/157 - loss: 11.1366 - acc_top1: 0.4107 - acc_top5: 0.8984 - 19ms/step\n",
      "step  90/157 - loss: 1.5395 - acc_top1: 0.4108 - acc_top5: 0.8964 - 19ms/step\n",
      "step 100/157 - loss: 4.7138 - acc_top1: 0.4111 - acc_top5: 0.8977 - 19ms/step\n",
      "step 110/157 - loss: 1.9708 - acc_top1: 0.4105 - acc_top5: 0.8972 - 20ms/step\n",
      "step 120/157 - loss: 3.4398 - acc_top1: 0.4124 - acc_top5: 0.8983 - 20ms/step\n",
      "step 130/157 - loss: 1.8294 - acc_top1: 0.4126 - acc_top5: 0.8971 - 20ms/step\n",
      "step 140/157 - loss: 1.8784 - acc_top1: 0.4131 - acc_top5: 0.8967 - 20ms/step\n",
      "step 150/157 - loss: 11.2775 - acc_top1: 0.4143 - acc_top5: 0.8968 - 20ms/step\n",
      "step 157/157 - loss: 1.5142 - acc_top1: 0.4134 - acc_top5: 0.8961 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.4604 - acc_top1: 0.4328 - acc_top5: 0.9047 - 52ms/step\n",
      "step  20/782 - loss: 1.7157 - acc_top1: 0.4164 - acc_top5: 0.9094 - 51ms/step\n",
      "step  30/782 - loss: 2.1391 - acc_top1: 0.4188 - acc_top5: 0.9036 - 50ms/step\n",
      "step  40/782 - loss: 1.4085 - acc_top1: 0.4223 - acc_top5: 0.8941 - 50ms/step\n",
      "step  50/782 - loss: 2.4717 - acc_top1: 0.4159 - acc_top5: 0.8938 - 50ms/step\n",
      "step  60/782 - loss: 1.6238 - acc_top1: 0.4151 - acc_top5: 0.8930 - 50ms/step\n",
      "step  70/782 - loss: 1.6278 - acc_top1: 0.4147 - acc_top5: 0.8953 - 50ms/step\n",
      "step  80/782 - loss: 1.8731 - acc_top1: 0.4133 - acc_top5: 0.8943 - 50ms/step\n",
      "step  90/782 - loss: 1.8216 - acc_top1: 0.4135 - acc_top5: 0.8925 - 50ms/step\n",
      "step 100/782 - loss: 2.2364 - acc_top1: 0.4144 - acc_top5: 0.8950 - 50ms/step\n",
      "step 110/782 - loss: 2.2943 - acc_top1: 0.4166 - acc_top5: 0.8946 - 50ms/step\n",
      "step 120/782 - loss: 1.9517 - acc_top1: 0.4197 - acc_top5: 0.8951 - 50ms/step\n",
      "step 130/782 - loss: 2.5945 - acc_top1: 0.4190 - acc_top5: 0.8953 - 50ms/step\n",
      "step 140/782 - loss: 1.8826 - acc_top1: 0.4200 - acc_top5: 0.8946 - 50ms/step\n",
      "step 150/782 - loss: 2.0304 - acc_top1: 0.4181 - acc_top5: 0.8936 - 50ms/step\n",
      "step 160/782 - loss: 1.4075 - acc_top1: 0.4206 - acc_top5: 0.8943 - 50ms/step\n",
      "step 170/782 - loss: 1.5449 - acc_top1: 0.4214 - acc_top5: 0.8949 - 50ms/step\n",
      "step 180/782 - loss: 2.6943 - acc_top1: 0.4220 - acc_top5: 0.8948 - 50ms/step\n",
      "step 190/782 - loss: 1.7962 - acc_top1: 0.4215 - acc_top5: 0.8954 - 50ms/step\n",
      "step 200/782 - loss: 1.5704 - acc_top1: 0.4220 - acc_top5: 0.8951 - 50ms/step\n",
      "step 210/782 - loss: 1.9124 - acc_top1: 0.4237 - acc_top5: 0.8949 - 50ms/step\n",
      "step 220/782 - loss: 1.6822 - acc_top1: 0.4237 - acc_top5: 0.8956 - 50ms/step\n",
      "step 230/782 - loss: 2.2104 - acc_top1: 0.4230 - acc_top5: 0.8953 - 50ms/step\n",
      "step 240/782 - loss: 1.7355 - acc_top1: 0.4232 - acc_top5: 0.8958 - 50ms/step\n",
      "step 250/782 - loss: 1.7560 - acc_top1: 0.4244 - acc_top5: 0.8968 - 50ms/step\n",
      "step 260/782 - loss: 1.6017 - acc_top1: 0.4243 - acc_top5: 0.8977 - 50ms/step\n",
      "step 270/782 - loss: 1.7713 - acc_top1: 0.4252 - acc_top5: 0.8970 - 50ms/step\n",
      "step 280/782 - loss: 2.4026 - acc_top1: 0.4262 - acc_top5: 0.8968 - 50ms/step\n",
      "step 290/782 - loss: 2.7709 - acc_top1: 0.4258 - acc_top5: 0.8968 - 50ms/step\n",
      "step 300/782 - loss: 1.6334 - acc_top1: 0.4262 - acc_top5: 0.8970 - 50ms/step\n",
      "step 310/782 - loss: 2.1555 - acc_top1: 0.4258 - acc_top5: 0.8969 - 50ms/step\n",
      "step 320/782 - loss: 1.5451 - acc_top1: 0.4251 - acc_top5: 0.8967 - 50ms/step\n",
      "step 330/782 - loss: 2.2199 - acc_top1: 0.4257 - acc_top5: 0.8965 - 50ms/step\n",
      "step 340/782 - loss: 1.5001 - acc_top1: 0.4257 - acc_top5: 0.8968 - 50ms/step\n",
      "step 350/782 - loss: 1.7467 - acc_top1: 0.4261 - acc_top5: 0.8974 - 51ms/step\n",
      "step 360/782 - loss: 1.2583 - acc_top1: 0.4257 - acc_top5: 0.8975 - 51ms/step\n",
      "step 370/782 - loss: 1.6969 - acc_top1: 0.4258 - acc_top5: 0.8976 - 51ms/step\n",
      "step 380/782 - loss: 1.8889 - acc_top1: 0.4253 - acc_top5: 0.8975 - 51ms/step\n",
      "step 390/782 - loss: 1.5530 - acc_top1: 0.4256 - acc_top5: 0.8977 - 51ms/step\n",
      "step 400/782 - loss: 1.9170 - acc_top1: 0.4250 - acc_top5: 0.8970 - 51ms/step\n",
      "step 410/782 - loss: 1.8469 - acc_top1: 0.4257 - acc_top5: 0.8971 - 51ms/step\n",
      "step 420/782 - loss: 1.8203 - acc_top1: 0.4253 - acc_top5: 0.8966 - 51ms/step\n",
      "step 430/782 - loss: 1.7107 - acc_top1: 0.4258 - acc_top5: 0.8964 - 51ms/step\n",
      "step 440/782 - loss: 1.8282 - acc_top1: 0.4266 - acc_top5: 0.8967 - 51ms/step\n",
      "step 450/782 - loss: 1.4101 - acc_top1: 0.4261 - acc_top5: 0.8972 - 51ms/step\n",
      "step 460/782 - loss: 2.2162 - acc_top1: 0.4263 - acc_top5: 0.8970 - 51ms/step\n",
      "step 470/782 - loss: 2.2073 - acc_top1: 0.4263 - acc_top5: 0.8970 - 51ms/step\n",
      "step 480/782 - loss: 1.5029 - acc_top1: 0.4271 - acc_top5: 0.8973 - 51ms/step\n",
      "step 490/782 - loss: 2.0627 - acc_top1: 0.4266 - acc_top5: 0.8975 - 51ms/step\n",
      "step 500/782 - loss: 1.4824 - acc_top1: 0.4268 - acc_top5: 0.8977 - 51ms/step\n",
      "step 510/782 - loss: 1.9205 - acc_top1: 0.4261 - acc_top5: 0.8979 - 51ms/step\n",
      "step 520/782 - loss: 1.5476 - acc_top1: 0.4262 - acc_top5: 0.8980 - 51ms/step\n",
      "step 530/782 - loss: 1.8716 - acc_top1: 0.4270 - acc_top5: 0.8983 - 51ms/step\n",
      "step 540/782 - loss: 2.7422 - acc_top1: 0.4272 - acc_top5: 0.8983 - 51ms/step\n",
      "step 550/782 - loss: 2.0246 - acc_top1: 0.4276 - acc_top5: 0.8985 - 51ms/step\n",
      "step 560/782 - loss: 2.4683 - acc_top1: 0.4277 - acc_top5: 0.8987 - 51ms/step\n",
      "step 570/782 - loss: 1.5210 - acc_top1: 0.4283 - acc_top5: 0.8988 - 51ms/step\n",
      "step 580/782 - loss: 2.0103 - acc_top1: 0.4283 - acc_top5: 0.8985 - 51ms/step\n",
      "step 590/782 - loss: 2.1194 - acc_top1: 0.4289 - acc_top5: 0.8990 - 51ms/step\n",
      "step 600/782 - loss: 1.7601 - acc_top1: 0.4287 - acc_top5: 0.8989 - 51ms/step\n",
      "step 610/782 - loss: 1.5073 - acc_top1: 0.4286 - acc_top5: 0.8990 - 51ms/step\n",
      "step 620/782 - loss: 1.6257 - acc_top1: 0.4281 - acc_top5: 0.8986 - 51ms/step\n",
      "step 630/782 - loss: 1.5242 - acc_top1: 0.4282 - acc_top5: 0.8986 - 51ms/step\n",
      "step 640/782 - loss: 2.4380 - acc_top1: 0.4284 - acc_top5: 0.8987 - 51ms/step\n",
      "step 650/782 - loss: 1.4512 - acc_top1: 0.4287 - acc_top5: 0.8988 - 51ms/step\n",
      "step 660/782 - loss: 1.8396 - acc_top1: 0.4294 - acc_top5: 0.8990 - 51ms/step\n",
      "step 670/782 - loss: 2.4773 - acc_top1: 0.4295 - acc_top5: 0.8986 - 51ms/step\n",
      "step 680/782 - loss: 1.7683 - acc_top1: 0.4299 - acc_top5: 0.8987 - 51ms/step\n",
      "step 690/782 - loss: 1.3334 - acc_top1: 0.4301 - acc_top5: 0.8990 - 51ms/step\n",
      "step 700/782 - loss: 1.8012 - acc_top1: 0.4300 - acc_top5: 0.8988 - 51ms/step\n",
      "step 710/782 - loss: 1.8530 - acc_top1: 0.4300 - acc_top5: 0.8987 - 51ms/step\n",
      "step 720/782 - loss: 1.5615 - acc_top1: 0.4299 - acc_top5: 0.8988 - 51ms/step\n",
      "step 730/782 - loss: 2.4845 - acc_top1: 0.4301 - acc_top5: 0.8988 - 51ms/step\n",
      "step 740/782 - loss: 1.6266 - acc_top1: 0.4304 - acc_top5: 0.8986 - 51ms/step\n",
      "step 750/782 - loss: 1.6213 - acc_top1: 0.4304 - acc_top5: 0.8986 - 51ms/step\n",
      "step 760/782 - loss: 2.2983 - acc_top1: 0.4305 - acc_top5: 0.8986 - 51ms/step\n",
      "step 770/782 - loss: 1.5609 - acc_top1: 0.4304 - acc_top5: 0.8989 - 51ms/step\n",
      "step 780/782 - loss: 1.5870 - acc_top1: 0.4306 - acc_top5: 0.8989 - 51ms/step\n",
      "step 782/782 - loss: 3.6892 - acc_top1: 0.4305 - acc_top5: 0.8988 - 51ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\5\n",
      "Eval begin...\n",
      "step  10/157 - loss: 7.8622 - acc_top1: 0.4188 - acc_top5: 0.9094 - 19ms/step\n",
      "step  20/157 - loss: 1.8351 - acc_top1: 0.4367 - acc_top5: 0.9055 - 20ms/step\n",
      "step  30/157 - loss: 2.0817 - acc_top1: 0.4339 - acc_top5: 0.9036 - 20ms/step\n",
      "step  40/157 - loss: 19.4085 - acc_top1: 0.4352 - acc_top5: 0.9035 - 20ms/step\n",
      "step  50/157 - loss: 9.7425 - acc_top1: 0.4334 - acc_top5: 0.9062 - 20ms/step\n",
      "step  60/157 - loss: 18.3404 - acc_top1: 0.4310 - acc_top5: 0.9068 - 20ms/step\n",
      "step  70/157 - loss: 1.4460 - acc_top1: 0.4321 - acc_top5: 0.9058 - 20ms/step\n",
      "step  80/157 - loss: 1.5800 - acc_top1: 0.4344 - acc_top5: 0.9074 - 20ms/step\n",
      "step  90/157 - loss: 1.4483 - acc_top1: 0.4352 - acc_top5: 0.9064 - 20ms/step\n",
      "step 100/157 - loss: 2.0516 - acc_top1: 0.4358 - acc_top5: 0.9070 - 20ms/step\n",
      "step 110/157 - loss: 20.5576 - acc_top1: 0.4358 - acc_top5: 0.9067 - 20ms/step\n",
      "step 120/157 - loss: 30.6532 - acc_top1: 0.4388 - acc_top5: 0.9064 - 20ms/step\n",
      "step 130/157 - loss: 1.8199 - acc_top1: 0.4375 - acc_top5: 0.9064 - 20ms/step\n",
      "step 140/157 - loss: 1.5604 - acc_top1: 0.4365 - acc_top5: 0.9069 - 20ms/step\n",
      "step 150/157 - loss: 22.8179 - acc_top1: 0.4398 - acc_top5: 0.9065 - 20ms/step\n",
      "step 157/157 - loss: 1.2002 - acc_top1: 0.4389 - acc_top5: 0.9068 - 20ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.6703 - acc_top1: 0.4016 - acc_top5: 0.8766 - 54ms/step\n",
      "step  20/782 - loss: 2.1738 - acc_top1: 0.4023 - acc_top5: 0.8758 - 53ms/step\n",
      "step  30/782 - loss: 1.8232 - acc_top1: 0.4073 - acc_top5: 0.8828 - 53ms/step\n",
      "step  40/782 - loss: 2.0868 - acc_top1: 0.4117 - acc_top5: 0.8848 - 54ms/step\n",
      "step  50/782 - loss: 2.2194 - acc_top1: 0.4084 - acc_top5: 0.8866 - 55ms/step\n",
      "step  60/782 - loss: 2.2226 - acc_top1: 0.4099 - acc_top5: 0.8896 - 54ms/step\n",
      "step  70/782 - loss: 1.9494 - acc_top1: 0.4118 - acc_top5: 0.8895 - 53ms/step\n",
      "step  80/782 - loss: 1.7934 - acc_top1: 0.4172 - acc_top5: 0.8928 - 53ms/step\n",
      "step  90/782 - loss: 1.5970 - acc_top1: 0.4184 - acc_top5: 0.8953 - 53ms/step\n",
      "step 100/782 - loss: 1.7332 - acc_top1: 0.4194 - acc_top5: 0.8967 - 52ms/step\n",
      "step 110/782 - loss: 1.6660 - acc_top1: 0.4214 - acc_top5: 0.8946 - 52ms/step\n",
      "step 120/782 - loss: 1.6257 - acc_top1: 0.4237 - acc_top5: 0.8965 - 52ms/step\n",
      "step 130/782 - loss: 2.0114 - acc_top1: 0.4244 - acc_top5: 0.8981 - 52ms/step\n",
      "step 140/782 - loss: 1.4925 - acc_top1: 0.4265 - acc_top5: 0.8980 - 52ms/step\n",
      "step 150/782 - loss: 2.2259 - acc_top1: 0.4271 - acc_top5: 0.8983 - 51ms/step\n",
      "step 160/782 - loss: 2.2838 - acc_top1: 0.4270 - acc_top5: 0.8969 - 51ms/step\n",
      "step 170/782 - loss: 2.6528 - acc_top1: 0.4273 - acc_top5: 0.8975 - 51ms/step\n",
      "step 180/782 - loss: 1.6283 - acc_top1: 0.4266 - acc_top5: 0.8974 - 51ms/step\n",
      "step 190/782 - loss: 1.6996 - acc_top1: 0.4275 - acc_top5: 0.8982 - 51ms/step\n",
      "step 200/782 - loss: 1.8690 - acc_top1: 0.4273 - acc_top5: 0.8980 - 51ms/step\n",
      "step 210/782 - loss: 2.3723 - acc_top1: 0.4278 - acc_top5: 0.8981 - 51ms/step\n",
      "step 220/782 - loss: 1.6953 - acc_top1: 0.4288 - acc_top5: 0.8982 - 51ms/step\n",
      "step 230/782 - loss: 1.9899 - acc_top1: 0.4283 - acc_top5: 0.8986 - 51ms/step\n",
      "step 240/782 - loss: 1.7805 - acc_top1: 0.4279 - acc_top5: 0.8997 - 51ms/step\n",
      "step 250/782 - loss: 1.5360 - acc_top1: 0.4280 - acc_top5: 0.8997 - 51ms/step\n",
      "step 260/782 - loss: 1.5673 - acc_top1: 0.4296 - acc_top5: 0.9002 - 51ms/step\n",
      "step 270/782 - loss: 2.0897 - acc_top1: 0.4304 - acc_top5: 0.9004 - 51ms/step\n",
      "step 280/782 - loss: 1.6844 - acc_top1: 0.4314 - acc_top5: 0.9008 - 51ms/step\n",
      "step 290/782 - loss: 2.4008 - acc_top1: 0.4310 - acc_top5: 0.9010 - 51ms/step\n",
      "step 300/782 - loss: 1.8030 - acc_top1: 0.4315 - acc_top5: 0.9016 - 51ms/step\n",
      "step 310/782 - loss: 2.2454 - acc_top1: 0.4320 - acc_top5: 0.9019 - 51ms/step\n",
      "step 320/782 - loss: 1.6978 - acc_top1: 0.4322 - acc_top5: 0.9025 - 51ms/step\n",
      "step 330/782 - loss: 2.0455 - acc_top1: 0.4332 - acc_top5: 0.9026 - 51ms/step\n",
      "step 340/782 - loss: 1.6034 - acc_top1: 0.4332 - acc_top5: 0.9029 - 51ms/step\n",
      "step 350/782 - loss: 1.6212 - acc_top1: 0.4333 - acc_top5: 0.9030 - 51ms/step\n",
      "step 360/782 - loss: 1.5407 - acc_top1: 0.4336 - acc_top5: 0.9032 - 51ms/step\n",
      "step 370/782 - loss: 1.7202 - acc_top1: 0.4343 - acc_top5: 0.9035 - 51ms/step\n",
      "step 380/782 - loss: 2.0385 - acc_top1: 0.4349 - acc_top5: 0.9036 - 51ms/step\n",
      "step 390/782 - loss: 1.6215 - acc_top1: 0.4354 - acc_top5: 0.9042 - 51ms/step\n",
      "step 400/782 - loss: 1.6497 - acc_top1: 0.4356 - acc_top5: 0.9047 - 51ms/step\n",
      "step 410/782 - loss: 1.9717 - acc_top1: 0.4353 - acc_top5: 0.9041 - 51ms/step\n",
      "step 420/782 - loss: 1.9807 - acc_top1: 0.4363 - acc_top5: 0.9044 - 51ms/step\n",
      "step 430/782 - loss: 1.8432 - acc_top1: 0.4369 - acc_top5: 0.9041 - 51ms/step\n",
      "step 440/782 - loss: 1.6405 - acc_top1: 0.4364 - acc_top5: 0.9040 - 51ms/step\n",
      "step 450/782 - loss: 1.7324 - acc_top1: 0.4371 - acc_top5: 0.9040 - 51ms/step\n",
      "step 460/782 - loss: 1.5892 - acc_top1: 0.4372 - acc_top5: 0.9046 - 51ms/step\n",
      "step 470/782 - loss: 2.0121 - acc_top1: 0.4380 - acc_top5: 0.9050 - 51ms/step\n",
      "step 480/782 - loss: 1.5033 - acc_top1: 0.4390 - acc_top5: 0.9053 - 51ms/step\n",
      "step 490/782 - loss: 2.0725 - acc_top1: 0.4388 - acc_top5: 0.9056 - 51ms/step\n",
      "step 500/782 - loss: 2.4416 - acc_top1: 0.4385 - acc_top5: 0.9054 - 51ms/step\n",
      "step 510/782 - loss: 2.2982 - acc_top1: 0.4386 - acc_top5: 0.9057 - 51ms/step\n",
      "step 520/782 - loss: 1.5688 - acc_top1: 0.4389 - acc_top5: 0.9059 - 51ms/step\n",
      "step 530/782 - loss: 1.5425 - acc_top1: 0.4395 - acc_top5: 0.9060 - 51ms/step\n",
      "step 540/782 - loss: 1.7361 - acc_top1: 0.4396 - acc_top5: 0.9061 - 51ms/step\n",
      "step 550/782 - loss: 1.9973 - acc_top1: 0.4399 - acc_top5: 0.9063 - 51ms/step\n",
      "step 560/782 - loss: 2.4938 - acc_top1: 0.4400 - acc_top5: 0.9063 - 51ms/step\n",
      "step 570/782 - loss: 1.8165 - acc_top1: 0.4397 - acc_top5: 0.9062 - 51ms/step\n",
      "step 580/782 - loss: 1.7415 - acc_top1: 0.4406 - acc_top5: 0.9065 - 51ms/step\n",
      "step 590/782 - loss: 1.6613 - acc_top1: 0.4409 - acc_top5: 0.9068 - 51ms/step\n",
      "step 600/782 - loss: 1.2835 - acc_top1: 0.4412 - acc_top5: 0.9071 - 51ms/step\n",
      "step 610/782 - loss: 1.6044 - acc_top1: 0.4412 - acc_top5: 0.9070 - 51ms/step\n",
      "step 620/782 - loss: 1.9490 - acc_top1: 0.4412 - acc_top5: 0.9072 - 51ms/step\n",
      "step 630/782 - loss: 1.5270 - acc_top1: 0.4411 - acc_top5: 0.9075 - 51ms/step\n",
      "step 640/782 - loss: 1.8612 - acc_top1: 0.4411 - acc_top5: 0.9078 - 51ms/step\n",
      "step 650/782 - loss: 2.3254 - acc_top1: 0.4412 - acc_top5: 0.9079 - 51ms/step\n",
      "step 660/782 - loss: 1.4091 - acc_top1: 0.4417 - acc_top5: 0.9078 - 51ms/step\n",
      "step 670/782 - loss: 1.4974 - acc_top1: 0.4420 - acc_top5: 0.9079 - 51ms/step\n",
      "step 680/782 - loss: 1.5069 - acc_top1: 0.4421 - acc_top5: 0.9081 - 51ms/step\n",
      "step 690/782 - loss: 1.8874 - acc_top1: 0.4426 - acc_top5: 0.9083 - 51ms/step\n",
      "step 700/782 - loss: 2.0490 - acc_top1: 0.4427 - acc_top5: 0.9081 - 51ms/step\n",
      "step 710/782 - loss: 1.2937 - acc_top1: 0.4432 - acc_top5: 0.9083 - 51ms/step\n",
      "step 720/782 - loss: 1.6230 - acc_top1: 0.4434 - acc_top5: 0.9082 - 51ms/step\n",
      "step 730/782 - loss: 1.6695 - acc_top1: 0.4433 - acc_top5: 0.9083 - 51ms/step\n",
      "step 740/782 - loss: 1.5477 - acc_top1: 0.4434 - acc_top5: 0.9086 - 51ms/step\n",
      "step 750/782 - loss: 1.5965 - acc_top1: 0.4431 - acc_top5: 0.9085 - 51ms/step\n",
      "step 760/782 - loss: 1.3953 - acc_top1: 0.4432 - acc_top5: 0.9086 - 51ms/step\n",
      "step 770/782 - loss: 1.5136 - acc_top1: 0.4435 - acc_top5: 0.9089 - 51ms/step\n",
      "step 780/782 - loss: 1.8949 - acc_top1: 0.4437 - acc_top5: 0.9090 - 51ms/step\n",
      "step 782/782 - loss: 1.4319 - acc_top1: 0.4438 - acc_top5: 0.9090 - 51ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\6\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.3560 - acc_top1: 0.4813 - acc_top5: 0.9281 - 20ms/step\n",
      "step  20/157 - loss: 1.6260 - acc_top1: 0.4820 - acc_top5: 0.9266 - 20ms/step\n",
      "step  30/157 - loss: 1.6370 - acc_top1: 0.4813 - acc_top5: 0.9219 - 20ms/step\n",
      "step  40/157 - loss: 1.6829 - acc_top1: 0.4824 - acc_top5: 0.9223 - 20ms/step\n",
      "step  50/157 - loss: 1.4199 - acc_top1: 0.4844 - acc_top5: 0.9259 - 20ms/step\n",
      "step  60/157 - loss: 1.1855 - acc_top1: 0.4831 - acc_top5: 0.9234 - 20ms/step\n",
      "step  70/157 - loss: 2.5940 - acc_top1: 0.4850 - acc_top5: 0.9237 - 20ms/step\n",
      "step  80/157 - loss: 1.4043 - acc_top1: 0.4857 - acc_top5: 0.9250 - 20ms/step\n",
      "step  90/157 - loss: 1.2770 - acc_top1: 0.4819 - acc_top5: 0.9262 - 19ms/step\n",
      "step 100/157 - loss: 1.4017 - acc_top1: 0.4813 - acc_top5: 0.9263 - 19ms/step\n",
      "step 110/157 - loss: 1.5421 - acc_top1: 0.4815 - acc_top5: 0.9250 - 19ms/step\n",
      "step 120/157 - loss: 1.4903 - acc_top1: 0.4813 - acc_top5: 0.9246 - 19ms/step\n",
      "step 130/157 - loss: 1.7005 - acc_top1: 0.4797 - acc_top5: 0.9234 - 19ms/step\n",
      "step 140/157 - loss: 1.3596 - acc_top1: 0.4775 - acc_top5: 0.9230 - 19ms/step\n",
      "step 150/157 - loss: 2.9638 - acc_top1: 0.4780 - acc_top5: 0.9229 - 19ms/step\n",
      "step 157/157 - loss: 1.3787 - acc_top1: 0.4766 - acc_top5: 0.9230 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.6357 - acc_top1: 0.4750 - acc_top5: 0.9219 - 52ms/step\n",
      "step  20/782 - loss: 1.4493 - acc_top1: 0.4531 - acc_top5: 0.9062 - 51ms/step\n",
      "step  30/782 - loss: 1.3491 - acc_top1: 0.4557 - acc_top5: 0.9109 - 51ms/step\n",
      "step  40/782 - loss: 1.5435 - acc_top1: 0.4609 - acc_top5: 0.9160 - 51ms/step\n",
      "step  50/782 - loss: 1.3350 - acc_top1: 0.4688 - acc_top5: 0.9194 - 51ms/step\n",
      "step  60/782 - loss: 1.4821 - acc_top1: 0.4711 - acc_top5: 0.9193 - 51ms/step\n",
      "step  70/782 - loss: 1.5156 - acc_top1: 0.4643 - acc_top5: 0.9158 - 51ms/step\n",
      "step  80/782 - loss: 1.4860 - acc_top1: 0.4625 - acc_top5: 0.9160 - 51ms/step\n",
      "step  90/782 - loss: 1.8098 - acc_top1: 0.4649 - acc_top5: 0.9196 - 51ms/step\n",
      "step 100/782 - loss: 1.7873 - acc_top1: 0.4597 - acc_top5: 0.9166 - 51ms/step\n",
      "step 110/782 - loss: 1.7410 - acc_top1: 0.4616 - acc_top5: 0.9175 - 51ms/step\n",
      "step 120/782 - loss: 1.9233 - acc_top1: 0.4607 - acc_top5: 0.9173 - 51ms/step\n",
      "step 130/782 - loss: 1.5546 - acc_top1: 0.4620 - acc_top5: 0.9163 - 51ms/step\n",
      "step 140/782 - loss: 1.9922 - acc_top1: 0.4655 - acc_top5: 0.9177 - 51ms/step\n",
      "step 150/782 - loss: 1.6344 - acc_top1: 0.4641 - acc_top5: 0.9179 - 51ms/step\n",
      "step 160/782 - loss: 1.5712 - acc_top1: 0.4637 - acc_top5: 0.9173 - 51ms/step\n",
      "step 170/782 - loss: 1.7435 - acc_top1: 0.4640 - acc_top5: 0.9187 - 51ms/step\n",
      "step 180/782 - loss: 1.6035 - acc_top1: 0.4641 - acc_top5: 0.9184 - 51ms/step\n",
      "step 190/782 - loss: 1.8858 - acc_top1: 0.4637 - acc_top5: 0.9187 - 51ms/step\n",
      "step 200/782 - loss: 1.5959 - acc_top1: 0.4629 - acc_top5: 0.9177 - 51ms/step\n",
      "step 210/782 - loss: 1.2714 - acc_top1: 0.4640 - acc_top5: 0.9173 - 51ms/step\n",
      "step 220/782 - loss: 1.6971 - acc_top1: 0.4649 - acc_top5: 0.9174 - 51ms/step\n",
      "step 230/782 - loss: 1.8856 - acc_top1: 0.4640 - acc_top5: 0.9168 - 51ms/step\n",
      "step 240/782 - loss: 1.8875 - acc_top1: 0.4653 - acc_top5: 0.9170 - 51ms/step\n",
      "step 250/782 - loss: 1.6253 - acc_top1: 0.4656 - acc_top5: 0.9175 - 51ms/step\n",
      "step 260/782 - loss: 1.4883 - acc_top1: 0.4671 - acc_top5: 0.9180 - 51ms/step\n",
      "step 270/782 - loss: 1.5824 - acc_top1: 0.4685 - acc_top5: 0.9179 - 51ms/step\n",
      "step 280/782 - loss: 1.3803 - acc_top1: 0.4679 - acc_top5: 0.9176 - 51ms/step\n",
      "step 290/782 - loss: 1.6502 - acc_top1: 0.4680 - acc_top5: 0.9179 - 51ms/step\n",
      "step 300/782 - loss: 1.5701 - acc_top1: 0.4686 - acc_top5: 0.9179 - 51ms/step\n",
      "step 310/782 - loss: 1.4698 - acc_top1: 0.4690 - acc_top5: 0.9182 - 51ms/step\n",
      "step 320/782 - loss: 1.3819 - acc_top1: 0.4701 - acc_top5: 0.9181 - 51ms/step\n",
      "step 330/782 - loss: 1.9647 - acc_top1: 0.4694 - acc_top5: 0.9179 - 51ms/step\n",
      "step 340/782 - loss: 1.7759 - acc_top1: 0.4696 - acc_top5: 0.9182 - 51ms/step\n",
      "step 350/782 - loss: 1.2289 - acc_top1: 0.4702 - acc_top5: 0.9181 - 51ms/step\n",
      "step 360/782 - loss: 1.6422 - acc_top1: 0.4704 - acc_top5: 0.9182 - 51ms/step\n",
      "step 370/782 - loss: 2.4735 - acc_top1: 0.4713 - acc_top5: 0.9180 - 51ms/step\n",
      "step 380/782 - loss: 1.5516 - acc_top1: 0.4709 - acc_top5: 0.9185 - 51ms/step\n",
      "step 390/782 - loss: 2.0262 - acc_top1: 0.4708 - acc_top5: 0.9185 - 51ms/step\n",
      "step 400/782 - loss: 1.3995 - acc_top1: 0.4707 - acc_top5: 0.9187 - 51ms/step\n",
      "step 410/782 - loss: 1.9033 - acc_top1: 0.4713 - acc_top5: 0.9192 - 51ms/step\n",
      "step 420/782 - loss: 1.8483 - acc_top1: 0.4713 - acc_top5: 0.9189 - 51ms/step\n",
      "step 430/782 - loss: 1.4270 - acc_top1: 0.4710 - acc_top5: 0.9188 - 51ms/step\n",
      "step 440/782 - loss: 1.2928 - acc_top1: 0.4717 - acc_top5: 0.9193 - 51ms/step\n",
      "step 450/782 - loss: 1.6208 - acc_top1: 0.4718 - acc_top5: 0.9192 - 51ms/step\n",
      "step 460/782 - loss: 1.3583 - acc_top1: 0.4720 - acc_top5: 0.9190 - 51ms/step\n",
      "step 470/782 - loss: 1.5603 - acc_top1: 0.4714 - acc_top5: 0.9187 - 51ms/step\n",
      "step 480/782 - loss: 1.5242 - acc_top1: 0.4729 - acc_top5: 0.9190 - 51ms/step\n",
      "step 490/782 - loss: 3.1270 - acc_top1: 0.4734 - acc_top5: 0.9191 - 51ms/step\n",
      "step 500/782 - loss: 1.5679 - acc_top1: 0.4741 - acc_top5: 0.9197 - 51ms/step\n",
      "step 510/782 - loss: 1.3917 - acc_top1: 0.4740 - acc_top5: 0.9197 - 51ms/step\n",
      "step 520/782 - loss: 1.6321 - acc_top1: 0.4736 - acc_top5: 0.9196 - 51ms/step\n",
      "step 530/782 - loss: 1.4391 - acc_top1: 0.4743 - acc_top5: 0.9198 - 51ms/step\n",
      "step 540/782 - loss: 1.9651 - acc_top1: 0.4743 - acc_top5: 0.9202 - 51ms/step\n",
      "step 550/782 - loss: 1.4854 - acc_top1: 0.4751 - acc_top5: 0.9203 - 51ms/step\n",
      "step 560/782 - loss: 1.7080 - acc_top1: 0.4755 - acc_top5: 0.9205 - 51ms/step\n",
      "step 570/782 - loss: 1.7011 - acc_top1: 0.4751 - acc_top5: 0.9205 - 51ms/step\n",
      "step 580/782 - loss: 2.2257 - acc_top1: 0.4747 - acc_top5: 0.9205 - 51ms/step\n",
      "step 590/782 - loss: 1.7062 - acc_top1: 0.4752 - acc_top5: 0.9205 - 51ms/step\n",
      "step 600/782 - loss: 1.4241 - acc_top1: 0.4752 - acc_top5: 0.9207 - 51ms/step\n",
      "step 610/782 - loss: 2.6677 - acc_top1: 0.4752 - acc_top5: 0.9209 - 51ms/step\n",
      "step 620/782 - loss: 1.6521 - acc_top1: 0.4758 - acc_top5: 0.9208 - 51ms/step\n",
      "step 630/782 - loss: 1.3289 - acc_top1: 0.4759 - acc_top5: 0.9209 - 51ms/step\n",
      "step 640/782 - loss: 2.0153 - acc_top1: 0.4761 - acc_top5: 0.9208 - 51ms/step\n",
      "step 650/782 - loss: 1.8070 - acc_top1: 0.4762 - acc_top5: 0.9206 - 51ms/step\n",
      "step 660/782 - loss: 1.4965 - acc_top1: 0.4771 - acc_top5: 0.9205 - 51ms/step\n",
      "step 670/782 - loss: 1.3786 - acc_top1: 0.4774 - acc_top5: 0.9207 - 51ms/step\n",
      "step 680/782 - loss: 1.1500 - acc_top1: 0.4781 - acc_top5: 0.9209 - 51ms/step\n",
      "step 690/782 - loss: 1.4314 - acc_top1: 0.4784 - acc_top5: 0.9209 - 51ms/step\n",
      "step 700/782 - loss: 1.5730 - acc_top1: 0.4784 - acc_top5: 0.9209 - 51ms/step\n",
      "step 710/782 - loss: 1.8583 - acc_top1: 0.4784 - acc_top5: 0.9210 - 51ms/step\n",
      "step 720/782 - loss: 2.4156 - acc_top1: 0.4781 - acc_top5: 0.9210 - 51ms/step\n",
      "step 730/782 - loss: 1.6408 - acc_top1: 0.4781 - acc_top5: 0.9211 - 51ms/step\n",
      "step 740/782 - loss: 1.6499 - acc_top1: 0.4786 - acc_top5: 0.9213 - 51ms/step\n",
      "step 750/782 - loss: 2.2503 - acc_top1: 0.4786 - acc_top5: 0.9211 - 51ms/step\n",
      "step 760/782 - loss: 2.0608 - acc_top1: 0.4789 - acc_top5: 0.9212 - 51ms/step\n",
      "step 770/782 - loss: 1.4533 - acc_top1: 0.4792 - acc_top5: 0.9213 - 51ms/step\n",
      "step 780/782 - loss: 1.6383 - acc_top1: 0.4792 - acc_top5: 0.9212 - 51ms/step\n",
      "step 782/782 - loss: 3.9895 - acc_top1: 0.4792 - acc_top5: 0.9212 - 51ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\7\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.2469 - acc_top1: 0.4891 - acc_top5: 0.9437 - 20ms/step\n",
      "step  20/157 - loss: 1.5912 - acc_top1: 0.4945 - acc_top5: 0.9297 - 20ms/step\n",
      "step  30/157 - loss: 1.5809 - acc_top1: 0.5000 - acc_top5: 0.9240 - 20ms/step\n",
      "step  40/157 - loss: 1.6568 - acc_top1: 0.5004 - acc_top5: 0.9262 - 20ms/step\n",
      "step  50/157 - loss: 2.2763 - acc_top1: 0.5016 - acc_top5: 0.9287 - 20ms/step\n",
      "step  60/157 - loss: 1.1730 - acc_top1: 0.5016 - acc_top5: 0.9260 - 20ms/step\n",
      "step  70/157 - loss: 9.5043 - acc_top1: 0.5016 - acc_top5: 0.9266 - 20ms/step\n",
      "step  80/157 - loss: 1.3340 - acc_top1: 0.5057 - acc_top5: 0.9275 - 19ms/step\n",
      "step  90/157 - loss: 1.2225 - acc_top1: 0.5026 - acc_top5: 0.9288 - 19ms/step\n",
      "step 100/157 - loss: 1.4781 - acc_top1: 0.4994 - acc_top5: 0.9294 - 19ms/step\n",
      "step 110/157 - loss: 3.0840 - acc_top1: 0.4979 - acc_top5: 0.9284 - 19ms/step\n",
      "step 120/157 - loss: 1.6962 - acc_top1: 0.4991 - acc_top5: 0.9298 - 19ms/step\n",
      "step 130/157 - loss: 1.5848 - acc_top1: 0.4983 - acc_top5: 0.9294 - 19ms/step\n",
      "step 140/157 - loss: 4.8475 - acc_top1: 0.4972 - acc_top5: 0.9295 - 19ms/step\n",
      "step 150/157 - loss: 8.6770 - acc_top1: 0.4985 - acc_top5: 0.9300 - 19ms/step\n",
      "step 157/157 - loss: 1.3789 - acc_top1: 0.4971 - acc_top5: 0.9302 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.7079 - acc_top1: 0.4641 - acc_top5: 0.9172 - 52ms/step\n",
      "step  20/782 - loss: 1.5436 - acc_top1: 0.4641 - acc_top5: 0.9219 - 51ms/step\n",
      "step  30/782 - loss: 1.9367 - acc_top1: 0.4625 - acc_top5: 0.9161 - 51ms/step\n",
      "step  40/782 - loss: 1.1971 - acc_top1: 0.4695 - acc_top5: 0.9230 - 51ms/step\n",
      "step  50/782 - loss: 1.4567 - acc_top1: 0.4759 - acc_top5: 0.9241 - 51ms/step\n",
      "step  60/782 - loss: 1.6614 - acc_top1: 0.4779 - acc_top5: 0.9227 - 50ms/step\n",
      "step  70/782 - loss: 1.2964 - acc_top1: 0.4808 - acc_top5: 0.9241 - 50ms/step\n",
      "step  80/782 - loss: 1.3988 - acc_top1: 0.4846 - acc_top5: 0.9248 - 50ms/step\n",
      "step  90/782 - loss: 1.4571 - acc_top1: 0.4872 - acc_top5: 0.9231 - 50ms/step\n",
      "step 100/782 - loss: 1.4453 - acc_top1: 0.4864 - acc_top5: 0.9241 - 50ms/step\n",
      "step 110/782 - loss: 1.3036 - acc_top1: 0.4898 - acc_top5: 0.9249 - 50ms/step\n",
      "step 120/782 - loss: 1.6122 - acc_top1: 0.4928 - acc_top5: 0.9260 - 50ms/step\n",
      "step 130/782 - loss: 1.2453 - acc_top1: 0.4940 - acc_top5: 0.9266 - 50ms/step\n",
      "step 140/782 - loss: 1.3815 - acc_top1: 0.4941 - acc_top5: 0.9272 - 50ms/step\n",
      "step 150/782 - loss: 1.4904 - acc_top1: 0.4955 - acc_top5: 0.9273 - 50ms/step\n",
      "step 160/782 - loss: 1.6462 - acc_top1: 0.4963 - acc_top5: 0.9275 - 50ms/step\n",
      "step 170/782 - loss: 1.2856 - acc_top1: 0.4960 - acc_top5: 0.9269 - 50ms/step\n",
      "step 180/782 - loss: 1.4985 - acc_top1: 0.4968 - acc_top5: 0.9270 - 50ms/step\n",
      "step 190/782 - loss: 1.1824 - acc_top1: 0.4975 - acc_top5: 0.9279 - 50ms/step\n",
      "step 200/782 - loss: 2.4168 - acc_top1: 0.4991 - acc_top5: 0.9276 - 50ms/step\n",
      "step 210/782 - loss: 1.3234 - acc_top1: 0.4972 - acc_top5: 0.9276 - 50ms/step\n",
      "step 220/782 - loss: 1.8366 - acc_top1: 0.4987 - acc_top5: 0.9273 - 50ms/step\n",
      "step 230/782 - loss: 2.2529 - acc_top1: 0.5001 - acc_top5: 0.9274 - 51ms/step\n",
      "step 240/782 - loss: 1.4584 - acc_top1: 0.5011 - acc_top5: 0.9273 - 51ms/step\n",
      "step 250/782 - loss: 1.6666 - acc_top1: 0.5006 - acc_top5: 0.9271 - 51ms/step\n",
      "step 260/782 - loss: 1.3639 - acc_top1: 0.5008 - acc_top5: 0.9275 - 51ms/step\n",
      "step 270/782 - loss: 1.1699 - acc_top1: 0.5019 - acc_top5: 0.9281 - 51ms/step\n",
      "step 280/782 - loss: 1.2988 - acc_top1: 0.5018 - acc_top5: 0.9282 - 51ms/step\n",
      "step 290/782 - loss: 1.2094 - acc_top1: 0.5020 - acc_top5: 0.9279 - 51ms/step\n",
      "step 300/782 - loss: 1.6911 - acc_top1: 0.5019 - acc_top5: 0.9281 - 51ms/step\n",
      "step 310/782 - loss: 1.6996 - acc_top1: 0.5008 - acc_top5: 0.9283 - 51ms/step\n",
      "step 320/782 - loss: 1.3214 - acc_top1: 0.5008 - acc_top5: 0.9281 - 51ms/step\n",
      "step 330/782 - loss: 1.7574 - acc_top1: 0.4996 - acc_top5: 0.9280 - 51ms/step\n",
      "step 340/782 - loss: 1.3144 - acc_top1: 0.4990 - acc_top5: 0.9279 - 51ms/step\n",
      "step 350/782 - loss: 1.4801 - acc_top1: 0.4998 - acc_top5: 0.9281 - 51ms/step\n",
      "step 360/782 - loss: 1.4691 - acc_top1: 0.4999 - acc_top5: 0.9280 - 51ms/step\n",
      "step 370/782 - loss: 1.7505 - acc_top1: 0.5000 - acc_top5: 0.9281 - 51ms/step\n",
      "step 380/782 - loss: 1.3996 - acc_top1: 0.5008 - acc_top5: 0.9286 - 51ms/step\n",
      "step 390/782 - loss: 1.2676 - acc_top1: 0.5011 - acc_top5: 0.9286 - 51ms/step\n",
      "step 400/782 - loss: 1.5700 - acc_top1: 0.5014 - acc_top5: 0.9291 - 51ms/step\n",
      "step 410/782 - loss: 1.3881 - acc_top1: 0.5013 - acc_top5: 0.9289 - 51ms/step\n",
      "step 420/782 - loss: 1.1742 - acc_top1: 0.5007 - acc_top5: 0.9290 - 51ms/step\n",
      "step 430/782 - loss: 1.6773 - acc_top1: 0.5009 - acc_top5: 0.9285 - 51ms/step\n",
      "step 440/782 - loss: 1.3243 - acc_top1: 0.5014 - acc_top5: 0.9287 - 51ms/step\n",
      "step 450/782 - loss: 1.5554 - acc_top1: 0.5009 - acc_top5: 0.9286 - 51ms/step\n",
      "step 460/782 - loss: 1.3515 - acc_top1: 0.5008 - acc_top5: 0.9284 - 51ms/step\n",
      "step 470/782 - loss: 1.7448 - acc_top1: 0.5009 - acc_top5: 0.9286 - 51ms/step\n",
      "step 480/782 - loss: 1.3136 - acc_top1: 0.5003 - acc_top5: 0.9287 - 51ms/step\n",
      "step 490/782 - loss: 1.3963 - acc_top1: 0.5004 - acc_top5: 0.9288 - 51ms/step\n",
      "step 500/782 - loss: 1.4276 - acc_top1: 0.5007 - acc_top5: 0.9289 - 51ms/step\n",
      "step 510/782 - loss: 1.5295 - acc_top1: 0.5011 - acc_top5: 0.9290 - 51ms/step\n",
      "step 520/782 - loss: 1.5447 - acc_top1: 0.5010 - acc_top5: 0.9291 - 51ms/step\n",
      "step 530/782 - loss: 2.2239 - acc_top1: 0.5010 - acc_top5: 0.9292 - 51ms/step\n",
      "step 540/782 - loss: 1.5265 - acc_top1: 0.5012 - acc_top5: 0.9293 - 51ms/step\n",
      "step 550/782 - loss: 1.5266 - acc_top1: 0.5012 - acc_top5: 0.9295 - 51ms/step\n",
      "step 560/782 - loss: 1.7575 - acc_top1: 0.5014 - acc_top5: 0.9294 - 51ms/step\n",
      "step 570/782 - loss: 1.3146 - acc_top1: 0.5014 - acc_top5: 0.9296 - 51ms/step\n",
      "step 580/782 - loss: 1.2230 - acc_top1: 0.5013 - acc_top5: 0.9297 - 51ms/step\n",
      "step 590/782 - loss: 1.3981 - acc_top1: 0.5019 - acc_top5: 0.9297 - 51ms/step\n",
      "step 600/782 - loss: 1.1367 - acc_top1: 0.5024 - acc_top5: 0.9299 - 51ms/step\n",
      "step 610/782 - loss: 1.7322 - acc_top1: 0.5023 - acc_top5: 0.9299 - 51ms/step\n",
      "step 620/782 - loss: 1.5008 - acc_top1: 0.5027 - acc_top5: 0.9297 - 51ms/step\n",
      "step 630/782 - loss: 1.2992 - acc_top1: 0.5029 - acc_top5: 0.9297 - 51ms/step\n",
      "step 640/782 - loss: 1.2192 - acc_top1: 0.5025 - acc_top5: 0.9298 - 51ms/step\n",
      "step 650/782 - loss: 1.4346 - acc_top1: 0.5030 - acc_top5: 0.9300 - 51ms/step\n",
      "step 660/782 - loss: 1.5707 - acc_top1: 0.5030 - acc_top5: 0.9298 - 51ms/step\n",
      "step 670/782 - loss: 1.5085 - acc_top1: 0.5031 - acc_top5: 0.9300 - 51ms/step\n",
      "step 680/782 - loss: 2.0447 - acc_top1: 0.5029 - acc_top5: 0.9301 - 51ms/step\n",
      "step 690/782 - loss: 1.6124 - acc_top1: 0.5031 - acc_top5: 0.9303 - 51ms/step\n",
      "step 700/782 - loss: 2.0988 - acc_top1: 0.5025 - acc_top5: 0.9304 - 51ms/step\n",
      "step 710/782 - loss: 1.2008 - acc_top1: 0.5030 - acc_top5: 0.9301 - 51ms/step\n",
      "step 720/782 - loss: 1.7923 - acc_top1: 0.5029 - acc_top5: 0.9303 - 51ms/step\n",
      "step 730/782 - loss: 1.4025 - acc_top1: 0.5029 - acc_top5: 0.9301 - 51ms/step\n",
      "step 740/782 - loss: 1.3146 - acc_top1: 0.5033 - acc_top5: 0.9298 - 51ms/step\n",
      "step 750/782 - loss: 1.8173 - acc_top1: 0.5034 - acc_top5: 0.9297 - 51ms/step\n",
      "step 760/782 - loss: 1.4190 - acc_top1: 0.5034 - acc_top5: 0.9296 - 51ms/step\n",
      "step 770/782 - loss: 1.3109 - acc_top1: 0.5034 - acc_top5: 0.9296 - 51ms/step\n",
      "step 780/782 - loss: 1.7549 - acc_top1: 0.5034 - acc_top5: 0.9295 - 51ms/step\n",
      "step 782/782 - loss: 1.7526 - acc_top1: 0.5033 - acc_top5: 0.9295 - 51ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\8\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.2240 - acc_top1: 0.5141 - acc_top5: 0.9547 - 19ms/step\n",
      "step  20/157 - loss: 1.5931 - acc_top1: 0.5047 - acc_top5: 0.9414 - 19ms/step\n",
      "step  30/157 - loss: 1.5404 - acc_top1: 0.5052 - acc_top5: 0.9344 - 19ms/step\n",
      "step  40/157 - loss: 1.7561 - acc_top1: 0.5059 - acc_top5: 0.9313 - 19ms/step\n",
      "step  50/157 - loss: 1.3656 - acc_top1: 0.5119 - acc_top5: 0.9319 - 19ms/step\n",
      "step  60/157 - loss: 1.1105 - acc_top1: 0.5130 - acc_top5: 0.9302 - 19ms/step\n",
      "step  70/157 - loss: 9.8876 - acc_top1: 0.5127 - acc_top5: 0.9299 - 19ms/step\n",
      "step  80/157 - loss: 1.3249 - acc_top1: 0.5174 - acc_top5: 0.9318 - 19ms/step\n",
      "step  90/157 - loss: 1.1946 - acc_top1: 0.5168 - acc_top5: 0.9330 - 19ms/step\n",
      "step 100/157 - loss: 1.2386 - acc_top1: 0.5169 - acc_top5: 0.9337 - 19ms/step\n",
      "step 110/157 - loss: 1.6204 - acc_top1: 0.5143 - acc_top5: 0.9325 - 19ms/step\n",
      "step 120/157 - loss: 1.9245 - acc_top1: 0.5164 - acc_top5: 0.9328 - 19ms/step\n",
      "step 130/157 - loss: 1.7052 - acc_top1: 0.5154 - acc_top5: 0.9327 - 19ms/step\n",
      "step 140/157 - loss: 4.6794 - acc_top1: 0.5153 - acc_top5: 0.9311 - 19ms/step\n",
      "step 150/157 - loss: 10.0631 - acc_top1: 0.5156 - acc_top5: 0.9305 - 19ms/step\n",
      "step 157/157 - loss: 1.0494 - acc_top1: 0.5148 - acc_top5: 0.9312 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.7938 - acc_top1: 0.4766 - acc_top5: 0.9125 - 52ms/step\n",
      "step  20/782 - loss: 1.4763 - acc_top1: 0.4891 - acc_top5: 0.9250 - 51ms/step\n",
      "step  30/782 - loss: 1.3501 - acc_top1: 0.4948 - acc_top5: 0.9229 - 51ms/step\n",
      "step  40/782 - loss: 1.2438 - acc_top1: 0.5012 - acc_top5: 0.9293 - 51ms/step\n",
      "step  50/782 - loss: 1.7984 - acc_top1: 0.5038 - acc_top5: 0.9291 - 51ms/step\n",
      "step  60/782 - loss: 1.2438 - acc_top1: 0.5018 - acc_top5: 0.9305 - 51ms/step\n",
      "step  70/782 - loss: 1.6098 - acc_top1: 0.5071 - acc_top5: 0.9295 - 51ms/step\n",
      "step  80/782 - loss: 1.4027 - acc_top1: 0.5086 - acc_top5: 0.9301 - 50ms/step\n",
      "step  90/782 - loss: 1.4797 - acc_top1: 0.5118 - acc_top5: 0.9304 - 50ms/step\n",
      "step 100/782 - loss: 1.4338 - acc_top1: 0.5119 - acc_top5: 0.9330 - 50ms/step\n",
      "step 110/782 - loss: 1.2660 - acc_top1: 0.5159 - acc_top5: 0.9344 - 50ms/step\n",
      "step 120/782 - loss: 1.8432 - acc_top1: 0.5161 - acc_top5: 0.9340 - 50ms/step\n",
      "step 130/782 - loss: 1.4917 - acc_top1: 0.5150 - acc_top5: 0.9353 - 50ms/step\n",
      "step 140/782 - loss: 1.2228 - acc_top1: 0.5173 - acc_top5: 0.9353 - 50ms/step\n",
      "step 150/782 - loss: 1.4845 - acc_top1: 0.5161 - acc_top5: 0.9341 - 50ms/step\n",
      "step 160/782 - loss: 1.3015 - acc_top1: 0.5162 - acc_top5: 0.9339 - 50ms/step\n",
      "step 170/782 - loss: 1.5116 - acc_top1: 0.5167 - acc_top5: 0.9344 - 50ms/step\n",
      "step 180/782 - loss: 1.2787 - acc_top1: 0.5182 - acc_top5: 0.9353 - 50ms/step\n",
      "step 190/782 - loss: 1.4994 - acc_top1: 0.5183 - acc_top5: 0.9357 - 50ms/step\n",
      "step 200/782 - loss: 1.3144 - acc_top1: 0.5190 - acc_top5: 0.9363 - 50ms/step\n",
      "step 210/782 - loss: 1.5577 - acc_top1: 0.5196 - acc_top5: 0.9358 - 50ms/step\n",
      "step 220/782 - loss: 1.3401 - acc_top1: 0.5210 - acc_top5: 0.9362 - 50ms/step\n",
      "step 230/782 - loss: 1.2739 - acc_top1: 0.5211 - acc_top5: 0.9365 - 50ms/step\n",
      "step 240/782 - loss: 1.3763 - acc_top1: 0.5210 - acc_top5: 0.9364 - 51ms/step\n",
      "step 250/782 - loss: 1.2576 - acc_top1: 0.5208 - acc_top5: 0.9364 - 50ms/step\n",
      "step 260/782 - loss: 1.5956 - acc_top1: 0.5207 - acc_top5: 0.9368 - 50ms/step\n",
      "step 270/782 - loss: 1.2146 - acc_top1: 0.5217 - acc_top5: 0.9366 - 51ms/step\n",
      "step 280/782 - loss: 1.3346 - acc_top1: 0.5221 - acc_top5: 0.9368 - 51ms/step\n",
      "step 290/782 - loss: 1.7019 - acc_top1: 0.5230 - acc_top5: 0.9367 - 51ms/step\n",
      "step 300/782 - loss: 1.3616 - acc_top1: 0.5229 - acc_top5: 0.9369 - 51ms/step\n",
      "step 310/782 - loss: 2.0730 - acc_top1: 0.5223 - acc_top5: 0.9364 - 51ms/step\n",
      "step 320/782 - loss: 1.3865 - acc_top1: 0.5227 - acc_top5: 0.9363 - 51ms/step\n",
      "step 330/782 - loss: 1.1442 - acc_top1: 0.5237 - acc_top5: 0.9366 - 51ms/step\n",
      "step 340/782 - loss: 1.3114 - acc_top1: 0.5240 - acc_top5: 0.9367 - 51ms/step\n",
      "step 350/782 - loss: 1.2457 - acc_top1: 0.5242 - acc_top5: 0.9371 - 51ms/step\n",
      "step 360/782 - loss: 1.6430 - acc_top1: 0.5239 - acc_top5: 0.9368 - 51ms/step\n",
      "step 370/782 - loss: 1.3260 - acc_top1: 0.5226 - acc_top5: 0.9367 - 51ms/step\n",
      "step 380/782 - loss: 1.3185 - acc_top1: 0.5224 - acc_top5: 0.9366 - 51ms/step\n",
      "step 390/782 - loss: 1.4611 - acc_top1: 0.5220 - acc_top5: 0.9367 - 51ms/step\n",
      "step 400/782 - loss: 1.5135 - acc_top1: 0.5220 - acc_top5: 0.9367 - 51ms/step\n",
      "step 410/782 - loss: 1.6454 - acc_top1: 0.5217 - acc_top5: 0.9367 - 52ms/step\n",
      "step 420/782 - loss: 1.5038 - acc_top1: 0.5210 - acc_top5: 0.9367 - 52ms/step\n",
      "step 430/782 - loss: 1.3505 - acc_top1: 0.5210 - acc_top5: 0.9366 - 52ms/step\n",
      "step 440/782 - loss: 1.3919 - acc_top1: 0.5211 - acc_top5: 0.9369 - 52ms/step\n",
      "step 450/782 - loss: 1.5721 - acc_top1: 0.5206 - acc_top5: 0.9370 - 52ms/step\n",
      "step 460/782 - loss: 1.5518 - acc_top1: 0.5204 - acc_top5: 0.9370 - 52ms/step\n",
      "step 470/782 - loss: 1.3762 - acc_top1: 0.5207 - acc_top5: 0.9376 - 51ms/step\n",
      "step 480/782 - loss: 2.3415 - acc_top1: 0.5207 - acc_top5: 0.9376 - 51ms/step\n",
      "step 490/782 - loss: 1.2077 - acc_top1: 0.5207 - acc_top5: 0.9375 - 51ms/step\n",
      "step 500/782 - loss: 1.6529 - acc_top1: 0.5201 - acc_top5: 0.9376 - 51ms/step\n",
      "step 510/782 - loss: 1.1595 - acc_top1: 0.5198 - acc_top5: 0.9376 - 51ms/step\n",
      "step 520/782 - loss: 1.1967 - acc_top1: 0.5199 - acc_top5: 0.9378 - 51ms/step\n",
      "step 530/782 - loss: 1.4346 - acc_top1: 0.5200 - acc_top5: 0.9379 - 51ms/step\n",
      "step 540/782 - loss: 1.4428 - acc_top1: 0.5198 - acc_top5: 0.9380 - 51ms/step\n",
      "step 550/782 - loss: 1.5971 - acc_top1: 0.5203 - acc_top5: 0.9380 - 51ms/step\n",
      "step 560/782 - loss: 1.8973 - acc_top1: 0.5209 - acc_top5: 0.9381 - 51ms/step\n",
      "step 570/782 - loss: 1.3459 - acc_top1: 0.5205 - acc_top5: 0.9377 - 51ms/step\n",
      "step 580/782 - loss: 1.1885 - acc_top1: 0.5207 - acc_top5: 0.9379 - 52ms/step\n",
      "step 590/782 - loss: 1.2605 - acc_top1: 0.5206 - acc_top5: 0.9377 - 52ms/step\n",
      "step 600/782 - loss: 1.4916 - acc_top1: 0.5208 - acc_top5: 0.9375 - 52ms/step\n",
      "step 610/782 - loss: 1.4131 - acc_top1: 0.5204 - acc_top5: 0.9377 - 52ms/step\n",
      "step 620/782 - loss: 1.1915 - acc_top1: 0.5210 - acc_top5: 0.9379 - 52ms/step\n",
      "step 630/782 - loss: 1.8110 - acc_top1: 0.5214 - acc_top5: 0.9376 - 52ms/step\n",
      "step 640/782 - loss: 1.2354 - acc_top1: 0.5216 - acc_top5: 0.9378 - 52ms/step\n",
      "step 650/782 - loss: 1.3710 - acc_top1: 0.5219 - acc_top5: 0.9377 - 52ms/step\n",
      "step 660/782 - loss: 1.4447 - acc_top1: 0.5217 - acc_top5: 0.9376 - 52ms/step\n",
      "step 670/782 - loss: 1.4426 - acc_top1: 0.5222 - acc_top5: 0.9379 - 52ms/step\n",
      "step 680/782 - loss: 1.4791 - acc_top1: 0.5228 - acc_top5: 0.9381 - 52ms/step\n",
      "step 690/782 - loss: 1.3513 - acc_top1: 0.5231 - acc_top5: 0.9381 - 52ms/step\n",
      "step 700/782 - loss: 1.3453 - acc_top1: 0.5231 - acc_top5: 0.9382 - 52ms/step\n",
      "step 710/782 - loss: 1.2786 - acc_top1: 0.5227 - acc_top5: 0.9383 - 52ms/step\n",
      "step 720/782 - loss: 1.4212 - acc_top1: 0.5227 - acc_top5: 0.9381 - 52ms/step\n",
      "step 730/782 - loss: 1.3241 - acc_top1: 0.5230 - acc_top5: 0.9381 - 52ms/step\n",
      "step 740/782 - loss: 1.3550 - acc_top1: 0.5231 - acc_top5: 0.9381 - 52ms/step\n",
      "step 750/782 - loss: 1.1253 - acc_top1: 0.5234 - acc_top5: 0.9382 - 52ms/step\n",
      "step 760/782 - loss: 1.1642 - acc_top1: 0.5234 - acc_top5: 0.9383 - 52ms/step\n",
      "step 770/782 - loss: 1.7017 - acc_top1: 0.5233 - acc_top5: 0.9383 - 52ms/step\n",
      "step 780/782 - loss: 1.6229 - acc_top1: 0.5237 - acc_top5: 0.9385 - 52ms/step\n",
      "step 782/782 - loss: 2.6065 - acc_top1: 0.5237 - acc_top5: 0.9384 - 52ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\9\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.0805 - acc_top1: 0.5406 - acc_top5: 0.9484 - 22ms/step\n",
      "step  20/157 - loss: 1.6857 - acc_top1: 0.5383 - acc_top5: 0.9461 - 21ms/step\n",
      "step  30/157 - loss: 5.2895 - acc_top1: 0.5307 - acc_top5: 0.9432 - 21ms/step\n",
      "step  40/157 - loss: 1.7510 - acc_top1: 0.5262 - acc_top5: 0.9418 - 20ms/step\n",
      "step  50/157 - loss: 2.8883 - acc_top1: 0.5297 - acc_top5: 0.9422 - 20ms/step\n",
      "step  60/157 - loss: 1.1065 - acc_top1: 0.5310 - acc_top5: 0.9401 - 20ms/step\n",
      "step  70/157 - loss: 23.8275 - acc_top1: 0.5321 - acc_top5: 0.9400 - 20ms/step\n",
      "step  80/157 - loss: 5.0137 - acc_top1: 0.5363 - acc_top5: 0.9416 - 20ms/step\n",
      "step  90/157 - loss: 2.1187 - acc_top1: 0.5337 - acc_top5: 0.9425 - 20ms/step\n",
      "step 100/157 - loss: 3.5189 - acc_top1: 0.5303 - acc_top5: 0.9423 - 20ms/step\n",
      "step 110/157 - loss: 2.8596 - acc_top1: 0.5295 - acc_top5: 0.9409 - 20ms/step\n",
      "step 120/157 - loss: 2.0275 - acc_top1: 0.5314 - acc_top5: 0.9408 - 20ms/step\n",
      "step 130/157 - loss: 1.5544 - acc_top1: 0.5305 - acc_top5: 0.9406 - 20ms/step\n",
      "step 140/157 - loss: 4.0473 - acc_top1: 0.5326 - acc_top5: 0.9401 - 20ms/step\n",
      "step 150/157 - loss: 11.9283 - acc_top1: 0.5320 - acc_top5: 0.9392 - 20ms/step\n",
      "step 157/157 - loss: 0.9384 - acc_top1: 0.5313 - acc_top5: 0.9399 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.1140 - acc_top1: 0.5703 - acc_top5: 0.9500 - 65ms/step\n",
      "step  20/782 - loss: 1.5796 - acc_top1: 0.5555 - acc_top5: 0.9461 - 63ms/step\n",
      "step  30/782 - loss: 1.3438 - acc_top1: 0.5484 - acc_top5: 0.9474 - 60ms/step\n",
      "step  40/782 - loss: 1.3744 - acc_top1: 0.5523 - acc_top5: 0.9449 - 59ms/step\n",
      "step  50/782 - loss: 1.3409 - acc_top1: 0.5478 - acc_top5: 0.9459 - 58ms/step\n",
      "step  60/782 - loss: 1.4336 - acc_top1: 0.5440 - acc_top5: 0.9448 - 58ms/step\n",
      "step  70/782 - loss: 1.2939 - acc_top1: 0.5451 - acc_top5: 0.9471 - 57ms/step\n",
      "step  80/782 - loss: 1.3393 - acc_top1: 0.5518 - acc_top5: 0.9475 - 57ms/step\n",
      "step  90/782 - loss: 1.3942 - acc_top1: 0.5526 - acc_top5: 0.9493 - 57ms/step\n",
      "step 100/782 - loss: 1.8664 - acc_top1: 0.5516 - acc_top5: 0.9502 - 57ms/step\n",
      "step 110/782 - loss: 1.3214 - acc_top1: 0.5491 - acc_top5: 0.9496 - 57ms/step\n",
      "step 120/782 - loss: 1.4708 - acc_top1: 0.5453 - acc_top5: 0.9497 - 56ms/step\n",
      "step 130/782 - loss: 1.1511 - acc_top1: 0.5463 - acc_top5: 0.9489 - 56ms/step\n",
      "step 140/782 - loss: 1.3610 - acc_top1: 0.5440 - acc_top5: 0.9488 - 56ms/step\n",
      "step 150/782 - loss: 1.2487 - acc_top1: 0.5450 - acc_top5: 0.9484 - 56ms/step\n",
      "step 160/782 - loss: 1.8476 - acc_top1: 0.5454 - acc_top5: 0.9480 - 55ms/step\n",
      "step 170/782 - loss: 1.1953 - acc_top1: 0.5456 - acc_top5: 0.9473 - 55ms/step\n",
      "step 180/782 - loss: 1.5247 - acc_top1: 0.5451 - acc_top5: 0.9460 - 55ms/step\n",
      "step 190/782 - loss: 1.5186 - acc_top1: 0.5475 - acc_top5: 0.9455 - 55ms/step\n",
      "step 200/782 - loss: 1.4022 - acc_top1: 0.5465 - acc_top5: 0.9454 - 54ms/step\n",
      "step 210/782 - loss: 1.7498 - acc_top1: 0.5458 - acc_top5: 0.9456 - 54ms/step\n",
      "step 220/782 - loss: 1.4214 - acc_top1: 0.5455 - acc_top5: 0.9457 - 54ms/step\n",
      "step 230/782 - loss: 1.1097 - acc_top1: 0.5459 - acc_top5: 0.9456 - 54ms/step\n",
      "step 240/782 - loss: 1.3139 - acc_top1: 0.5464 - acc_top5: 0.9464 - 54ms/step\n",
      "step 250/782 - loss: 1.6405 - acc_top1: 0.5464 - acc_top5: 0.9456 - 54ms/step\n",
      "step 260/782 - loss: 1.2749 - acc_top1: 0.5470 - acc_top5: 0.9462 - 54ms/step\n",
      "step 270/782 - loss: 1.6022 - acc_top1: 0.5470 - acc_top5: 0.9459 - 54ms/step\n",
      "step 280/782 - loss: 1.2452 - acc_top1: 0.5464 - acc_top5: 0.9459 - 54ms/step\n",
      "step 290/782 - loss: 1.1484 - acc_top1: 0.5463 - acc_top5: 0.9461 - 54ms/step\n",
      "step 300/782 - loss: 1.3186 - acc_top1: 0.5459 - acc_top5: 0.9462 - 55ms/step\n",
      "step 310/782 - loss: 1.0826 - acc_top1: 0.5463 - acc_top5: 0.9461 - 55ms/step\n",
      "step 320/782 - loss: 1.2806 - acc_top1: 0.5448 - acc_top5: 0.9458 - 54ms/step\n",
      "step 330/782 - loss: 1.3682 - acc_top1: 0.5452 - acc_top5: 0.9456 - 55ms/step\n",
      "step 340/782 - loss: 1.1202 - acc_top1: 0.5465 - acc_top5: 0.9462 - 55ms/step\n",
      "step 350/782 - loss: 1.4743 - acc_top1: 0.5468 - acc_top5: 0.9466 - 55ms/step\n",
      "step 360/782 - loss: 1.3958 - acc_top1: 0.5462 - acc_top5: 0.9460 - 55ms/step\n",
      "step 370/782 - loss: 1.1512 - acc_top1: 0.5458 - acc_top5: 0.9461 - 55ms/step\n",
      "step 380/782 - loss: 1.1660 - acc_top1: 0.5460 - acc_top5: 0.9460 - 54ms/step\n",
      "step 390/782 - loss: 1.1206 - acc_top1: 0.5470 - acc_top5: 0.9457 - 54ms/step\n",
      "step 400/782 - loss: 1.1559 - acc_top1: 0.5475 - acc_top5: 0.9457 - 54ms/step\n",
      "step 410/782 - loss: 1.2613 - acc_top1: 0.5474 - acc_top5: 0.9459 - 54ms/step\n",
      "step 420/782 - loss: 1.4221 - acc_top1: 0.5466 - acc_top5: 0.9463 - 54ms/step\n",
      "step 430/782 - loss: 1.2737 - acc_top1: 0.5468 - acc_top5: 0.9465 - 54ms/step\n",
      "step 440/782 - loss: 1.5675 - acc_top1: 0.5462 - acc_top5: 0.9467 - 54ms/step\n",
      "step 450/782 - loss: 1.3785 - acc_top1: 0.5465 - acc_top5: 0.9467 - 54ms/step\n",
      "step 460/782 - loss: 1.4099 - acc_top1: 0.5463 - acc_top5: 0.9462 - 54ms/step\n",
      "step 470/782 - loss: 1.2584 - acc_top1: 0.5462 - acc_top5: 0.9461 - 54ms/step\n",
      "step 480/782 - loss: 1.6687 - acc_top1: 0.5461 - acc_top5: 0.9461 - 54ms/step\n",
      "step 490/782 - loss: 1.7115 - acc_top1: 0.5459 - acc_top5: 0.9459 - 54ms/step\n",
      "step 500/782 - loss: 1.5320 - acc_top1: 0.5456 - acc_top5: 0.9457 - 54ms/step\n",
      "step 510/782 - loss: 1.3283 - acc_top1: 0.5456 - acc_top5: 0.9459 - 54ms/step\n",
      "step 520/782 - loss: 1.6040 - acc_top1: 0.5448 - acc_top5: 0.9459 - 54ms/step\n",
      "step 530/782 - loss: 1.3380 - acc_top1: 0.5449 - acc_top5: 0.9456 - 54ms/step\n",
      "step 540/782 - loss: 1.5062 - acc_top1: 0.5442 - acc_top5: 0.9455 - 54ms/step\n",
      "step 550/782 - loss: 1.7871 - acc_top1: 0.5439 - acc_top5: 0.9456 - 54ms/step\n",
      "step 560/782 - loss: 1.2355 - acc_top1: 0.5436 - acc_top5: 0.9455 - 54ms/step\n",
      "step 570/782 - loss: 1.7407 - acc_top1: 0.5431 - acc_top5: 0.9454 - 54ms/step\n",
      "step 580/782 - loss: 1.4486 - acc_top1: 0.5434 - acc_top5: 0.9452 - 54ms/step\n",
      "step 590/782 - loss: 1.3627 - acc_top1: 0.5435 - acc_top5: 0.9450 - 54ms/step\n",
      "step 600/782 - loss: 2.0671 - acc_top1: 0.5434 - acc_top5: 0.9451 - 54ms/step\n",
      "step 610/782 - loss: 1.8450 - acc_top1: 0.5434 - acc_top5: 0.9451 - 54ms/step\n",
      "step 620/782 - loss: 1.4834 - acc_top1: 0.5432 - acc_top5: 0.9450 - 54ms/step\n",
      "step 630/782 - loss: 1.4007 - acc_top1: 0.5431 - acc_top5: 0.9449 - 54ms/step\n",
      "step 640/782 - loss: 1.4339 - acc_top1: 0.5431 - acc_top5: 0.9449 - 54ms/step\n",
      "step 650/782 - loss: 1.2748 - acc_top1: 0.5429 - acc_top5: 0.9449 - 54ms/step\n",
      "step 660/782 - loss: 1.4552 - acc_top1: 0.5427 - acc_top5: 0.9446 - 54ms/step\n",
      "step 670/782 - loss: 1.3498 - acc_top1: 0.5432 - acc_top5: 0.9449 - 54ms/step\n",
      "step 680/782 - loss: 1.2280 - acc_top1: 0.5435 - acc_top5: 0.9448 - 54ms/step\n",
      "step 690/782 - loss: 1.2999 - acc_top1: 0.5434 - acc_top5: 0.9446 - 54ms/step\n",
      "step 700/782 - loss: 1.3436 - acc_top1: 0.5435 - acc_top5: 0.9446 - 54ms/step\n",
      "step 710/782 - loss: 1.4036 - acc_top1: 0.5434 - acc_top5: 0.9445 - 54ms/step\n",
      "step 720/782 - loss: 1.2077 - acc_top1: 0.5431 - acc_top5: 0.9444 - 54ms/step\n",
      "step 730/782 - loss: 1.2015 - acc_top1: 0.5432 - acc_top5: 0.9444 - 53ms/step\n",
      "step 740/782 - loss: 1.1713 - acc_top1: 0.5433 - acc_top5: 0.9444 - 53ms/step\n",
      "step 750/782 - loss: 1.2785 - acc_top1: 0.5437 - acc_top5: 0.9445 - 53ms/step\n",
      "step 760/782 - loss: 1.2522 - acc_top1: 0.5439 - acc_top5: 0.9445 - 53ms/step\n",
      "step 770/782 - loss: 1.1845 - acc_top1: 0.5440 - acc_top5: 0.9446 - 53ms/step\n",
      "step 780/782 - loss: 1.2311 - acc_top1: 0.5442 - acc_top5: 0.9445 - 53ms/step\n",
      "step 782/782 - loss: 2.1867 - acc_top1: 0.5443 - acc_top5: 0.9444 - 53ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\10\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.0377 - acc_top1: 0.5469 - acc_top5: 0.9469 - 19ms/step\n",
      "step  20/157 - loss: 1.9456 - acc_top1: 0.5414 - acc_top5: 0.9453 - 19ms/step\n",
      "step  30/157 - loss: 1.4603 - acc_top1: 0.5484 - acc_top5: 0.9427 - 19ms/step\n",
      "step  40/157 - loss: 1.8104 - acc_top1: 0.5469 - acc_top5: 0.9430 - 19ms/step\n",
      "step  50/157 - loss: 1.4076 - acc_top1: 0.5434 - acc_top5: 0.9416 - 19ms/step\n",
      "step  60/157 - loss: 2.1878 - acc_top1: 0.5461 - acc_top5: 0.9401 - 19ms/step\n",
      "step  70/157 - loss: 5.0762 - acc_top1: 0.5460 - acc_top5: 0.9402 - 19ms/step\n",
      "step  80/157 - loss: 1.4095 - acc_top1: 0.5512 - acc_top5: 0.9414 - 19ms/step\n",
      "step  90/157 - loss: 1.0911 - acc_top1: 0.5495 - acc_top5: 0.9427 - 19ms/step\n",
      "step 100/157 - loss: 1.2299 - acc_top1: 0.5470 - acc_top5: 0.9431 - 19ms/step\n",
      "step 110/157 - loss: 2.0797 - acc_top1: 0.5484 - acc_top5: 0.9418 - 19ms/step\n",
      "step 120/157 - loss: 9.9621 - acc_top1: 0.5482 - acc_top5: 0.9419 - 19ms/step\n",
      "step 130/157 - loss: 1.5314 - acc_top1: 0.5482 - acc_top5: 0.9416 - 19ms/step\n",
      "step 140/157 - loss: 3.2751 - acc_top1: 0.5470 - acc_top5: 0.9415 - 19ms/step\n",
      "step 150/157 - loss: 6.7431 - acc_top1: 0.5469 - acc_top5: 0.9413 - 19ms/step\n",
      "step 157/157 - loss: 0.9402 - acc_top1: 0.5458 - acc_top5: 0.9417 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.5356 - acc_top1: 0.5297 - acc_top5: 0.9437 - 53ms/step\n",
      "step  20/782 - loss: 1.1596 - acc_top1: 0.5641 - acc_top5: 0.9437 - 56ms/step\n",
      "step  30/782 - loss: 1.1322 - acc_top1: 0.5615 - acc_top5: 0.9453 - 54ms/step\n",
      "step  40/782 - loss: 2.0643 - acc_top1: 0.5508 - acc_top5: 0.9480 - 53ms/step\n",
      "step  50/782 - loss: 1.0275 - acc_top1: 0.5550 - acc_top5: 0.9503 - 53ms/step\n",
      "step  60/782 - loss: 1.1614 - acc_top1: 0.5539 - acc_top5: 0.9497 - 53ms/step\n",
      "step  70/782 - loss: 1.1996 - acc_top1: 0.5596 - acc_top5: 0.9500 - 54ms/step\n",
      "step  80/782 - loss: 1.3258 - acc_top1: 0.5611 - acc_top5: 0.9490 - 54ms/step\n",
      "step  90/782 - loss: 1.2639 - acc_top1: 0.5590 - acc_top5: 0.9495 - 54ms/step\n",
      "step 100/782 - loss: 1.4220 - acc_top1: 0.5608 - acc_top5: 0.9484 - 54ms/step\n",
      "step 110/782 - loss: 1.5033 - acc_top1: 0.5624 - acc_top5: 0.9486 - 54ms/step\n",
      "step 120/782 - loss: 1.1429 - acc_top1: 0.5632 - acc_top5: 0.9487 - 54ms/step\n",
      "step 130/782 - loss: 1.5894 - acc_top1: 0.5647 - acc_top5: 0.9500 - 54ms/step\n",
      "step 140/782 - loss: 1.2058 - acc_top1: 0.5632 - acc_top5: 0.9485 - 54ms/step\n",
      "step 150/782 - loss: 1.1303 - acc_top1: 0.5621 - acc_top5: 0.9483 - 54ms/step\n",
      "step 160/782 - loss: 1.5408 - acc_top1: 0.5626 - acc_top5: 0.9484 - 55ms/step\n",
      "step 170/782 - loss: 1.1673 - acc_top1: 0.5619 - acc_top5: 0.9484 - 55ms/step\n",
      "step 180/782 - loss: 1.1914 - acc_top1: 0.5618 - acc_top5: 0.9495 - 55ms/step\n",
      "step 190/782 - loss: 1.1834 - acc_top1: 0.5610 - acc_top5: 0.9493 - 54ms/step\n",
      "step 200/782 - loss: 1.3817 - acc_top1: 0.5604 - acc_top5: 0.9487 - 55ms/step\n",
      "step 210/782 - loss: 1.6224 - acc_top1: 0.5612 - acc_top5: 0.9496 - 54ms/step\n",
      "step 220/782 - loss: 1.0655 - acc_top1: 0.5625 - acc_top5: 0.9494 - 54ms/step\n",
      "step 230/782 - loss: 1.4325 - acc_top1: 0.5622 - acc_top5: 0.9488 - 54ms/step\n",
      "step 240/782 - loss: 1.4683 - acc_top1: 0.5593 - acc_top5: 0.9488 - 54ms/step\n",
      "step 250/782 - loss: 1.3809 - acc_top1: 0.5589 - acc_top5: 0.9486 - 54ms/step\n",
      "step 260/782 - loss: 1.3482 - acc_top1: 0.5577 - acc_top5: 0.9483 - 54ms/step\n",
      "step 270/782 - loss: 1.1932 - acc_top1: 0.5579 - acc_top5: 0.9486 - 54ms/step\n",
      "step 280/782 - loss: 1.1309 - acc_top1: 0.5580 - acc_top5: 0.9493 - 54ms/step\n",
      "step 290/782 - loss: 1.2432 - acc_top1: 0.5577 - acc_top5: 0.9494 - 54ms/step\n",
      "step 300/782 - loss: 1.3119 - acc_top1: 0.5574 - acc_top5: 0.9493 - 54ms/step\n",
      "step 310/782 - loss: 1.3661 - acc_top1: 0.5581 - acc_top5: 0.9491 - 53ms/step\n",
      "step 320/782 - loss: 1.5192 - acc_top1: 0.5589 - acc_top5: 0.9492 - 53ms/step\n",
      "step 330/782 - loss: 1.6363 - acc_top1: 0.5582 - acc_top5: 0.9491 - 53ms/step\n",
      "step 340/782 - loss: 1.3821 - acc_top1: 0.5584 - acc_top5: 0.9492 - 53ms/step\n",
      "step 350/782 - loss: 1.1692 - acc_top1: 0.5596 - acc_top5: 0.9491 - 53ms/step\n",
      "step 360/782 - loss: 0.9435 - acc_top1: 0.5592 - acc_top5: 0.9492 - 54ms/step\n",
      "step 370/782 - loss: 1.5422 - acc_top1: 0.5588 - acc_top5: 0.9490 - 54ms/step\n",
      "step 380/782 - loss: 1.4073 - acc_top1: 0.5588 - acc_top5: 0.9486 - 54ms/step\n",
      "step 390/782 - loss: 1.0863 - acc_top1: 0.5589 - acc_top5: 0.9488 - 54ms/step\n",
      "step 400/782 - loss: 1.2721 - acc_top1: 0.5591 - acc_top5: 0.9486 - 54ms/step\n",
      "step 410/782 - loss: 1.3576 - acc_top1: 0.5588 - acc_top5: 0.9485 - 54ms/step\n",
      "step 420/782 - loss: 1.3444 - acc_top1: 0.5581 - acc_top5: 0.9488 - 54ms/step\n",
      "step 430/782 - loss: 1.1189 - acc_top1: 0.5585 - acc_top5: 0.9487 - 54ms/step\n",
      "step 440/782 - loss: 1.0382 - acc_top1: 0.5583 - acc_top5: 0.9485 - 54ms/step\n",
      "step 450/782 - loss: 1.3188 - acc_top1: 0.5578 - acc_top5: 0.9486 - 54ms/step\n",
      "step 460/782 - loss: 1.0458 - acc_top1: 0.5582 - acc_top5: 0.9489 - 54ms/step\n",
      "step 470/782 - loss: 1.3647 - acc_top1: 0.5582 - acc_top5: 0.9489 - 54ms/step\n",
      "step 480/782 - loss: 1.4459 - acc_top1: 0.5589 - acc_top5: 0.9493 - 54ms/step\n",
      "step 490/782 - loss: 1.4906 - acc_top1: 0.5581 - acc_top5: 0.9490 - 54ms/step\n",
      "step 500/782 - loss: 1.1513 - acc_top1: 0.5579 - acc_top5: 0.9492 - 54ms/step\n",
      "step 510/782 - loss: 1.1709 - acc_top1: 0.5583 - acc_top5: 0.9492 - 54ms/step\n",
      "step 520/782 - loss: 1.4998 - acc_top1: 0.5592 - acc_top5: 0.9493 - 54ms/step\n",
      "step 530/782 - loss: 1.1280 - acc_top1: 0.5590 - acc_top5: 0.9495 - 54ms/step\n",
      "step 540/782 - loss: 1.2293 - acc_top1: 0.5591 - acc_top5: 0.9497 - 54ms/step\n",
      "step 550/782 - loss: 1.0162 - acc_top1: 0.5596 - acc_top5: 0.9498 - 54ms/step\n",
      "step 560/782 - loss: 1.2463 - acc_top1: 0.5594 - acc_top5: 0.9496 - 54ms/step\n",
      "step 570/782 - loss: 1.1810 - acc_top1: 0.5595 - acc_top5: 0.9495 - 54ms/step\n",
      "step 580/782 - loss: 1.0255 - acc_top1: 0.5598 - acc_top5: 0.9493 - 54ms/step\n",
      "step 590/782 - loss: 1.3006 - acc_top1: 0.5598 - acc_top5: 0.9491 - 54ms/step\n",
      "step 600/782 - loss: 1.3527 - acc_top1: 0.5603 - acc_top5: 0.9490 - 54ms/step\n",
      "step 610/782 - loss: 1.2546 - acc_top1: 0.5608 - acc_top5: 0.9493 - 54ms/step\n",
      "step 620/782 - loss: 1.4740 - acc_top1: 0.5612 - acc_top5: 0.9494 - 54ms/step\n",
      "step 630/782 - loss: 1.1403 - acc_top1: 0.5609 - acc_top5: 0.9493 - 54ms/step\n",
      "step 640/782 - loss: 1.2326 - acc_top1: 0.5617 - acc_top5: 0.9495 - 54ms/step\n",
      "step 650/782 - loss: 0.9608 - acc_top1: 0.5619 - acc_top5: 0.9493 - 54ms/step\n",
      "step 660/782 - loss: 1.1774 - acc_top1: 0.5619 - acc_top5: 0.9495 - 54ms/step\n",
      "step 670/782 - loss: 1.1584 - acc_top1: 0.5614 - acc_top5: 0.9495 - 54ms/step\n",
      "step 680/782 - loss: 1.5160 - acc_top1: 0.5615 - acc_top5: 0.9497 - 54ms/step\n",
      "step 690/782 - loss: 1.2760 - acc_top1: 0.5617 - acc_top5: 0.9497 - 54ms/step\n",
      "step 700/782 - loss: 1.2199 - acc_top1: 0.5617 - acc_top5: 0.9496 - 54ms/step\n",
      "step 710/782 - loss: 1.0047 - acc_top1: 0.5621 - acc_top5: 0.9495 - 54ms/step\n",
      "step 720/782 - loss: 1.6128 - acc_top1: 0.5618 - acc_top5: 0.9494 - 54ms/step\n",
      "step 730/782 - loss: 1.1996 - acc_top1: 0.5619 - acc_top5: 0.9493 - 54ms/step\n",
      "step 740/782 - loss: 0.9098 - acc_top1: 0.5625 - acc_top5: 0.9495 - 54ms/step\n",
      "step 750/782 - loss: 1.5895 - acc_top1: 0.5630 - acc_top5: 0.9494 - 54ms/step\n",
      "step 760/782 - loss: 1.3824 - acc_top1: 0.5631 - acc_top5: 0.9496 - 54ms/step\n",
      "step 770/782 - loss: 1.3378 - acc_top1: 0.5628 - acc_top5: 0.9495 - 54ms/step\n",
      "step 780/782 - loss: 1.0898 - acc_top1: 0.5630 - acc_top5: 0.9496 - 54ms/step\n",
      "step 782/782 - loss: 2.4534 - acc_top1: 0.5630 - acc_top5: 0.9497 - 54ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\11\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.2956 - acc_top1: 0.5594 - acc_top5: 0.9437 - 20ms/step\n",
      "step  20/157 - loss: 1.4490 - acc_top1: 0.5492 - acc_top5: 0.9406 - 22ms/step\n",
      "step  30/157 - loss: 1.4258 - acc_top1: 0.5536 - acc_top5: 0.9401 - 22ms/step\n",
      "step  40/157 - loss: 1.7683 - acc_top1: 0.5535 - acc_top5: 0.9391 - 22ms/step\n",
      "step  50/157 - loss: 1.2842 - acc_top1: 0.5525 - acc_top5: 0.9397 - 22ms/step\n",
      "step  60/157 - loss: 0.9508 - acc_top1: 0.5539 - acc_top5: 0.9388 - 21ms/step\n",
      "step  70/157 - loss: 1.7722 - acc_top1: 0.5540 - acc_top5: 0.9391 - 21ms/step\n",
      "step  80/157 - loss: 1.4966 - acc_top1: 0.5582 - acc_top5: 0.9414 - 21ms/step\n",
      "step  90/157 - loss: 1.2161 - acc_top1: 0.5545 - acc_top5: 0.9424 - 21ms/step\n",
      "step 100/157 - loss: 1.2152 - acc_top1: 0.5513 - acc_top5: 0.9422 - 21ms/step\n",
      "step 110/157 - loss: 1.5720 - acc_top1: 0.5473 - acc_top5: 0.9399 - 21ms/step\n",
      "step 120/157 - loss: 1.4492 - acc_top1: 0.5452 - acc_top5: 0.9404 - 20ms/step\n",
      "step 130/157 - loss: 1.4576 - acc_top1: 0.5436 - acc_top5: 0.9409 - 20ms/step\n",
      "step 140/157 - loss: 1.2794 - acc_top1: 0.5432 - acc_top5: 0.9405 - 20ms/step\n",
      "step 150/157 - loss: 2.8647 - acc_top1: 0.5432 - acc_top5: 0.9403 - 20ms/step\n",
      "step 157/157 - loss: 0.8843 - acc_top1: 0.5417 - acc_top5: 0.9408 - 20ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.1269 - acc_top1: 0.5578 - acc_top5: 0.9578 - 54ms/step\n",
      "step  20/782 - loss: 1.0326 - acc_top1: 0.5586 - acc_top5: 0.9547 - 52ms/step\n",
      "step  30/782 - loss: 1.2382 - acc_top1: 0.5630 - acc_top5: 0.9552 - 54ms/step\n",
      "step  40/782 - loss: 0.9621 - acc_top1: 0.5672 - acc_top5: 0.9551 - 54ms/step\n",
      "step  50/782 - loss: 1.1235 - acc_top1: 0.5741 - acc_top5: 0.9569 - 53ms/step\n",
      "step  60/782 - loss: 1.2315 - acc_top1: 0.5768 - acc_top5: 0.9563 - 53ms/step\n",
      "step  70/782 - loss: 1.3317 - acc_top1: 0.5779 - acc_top5: 0.9556 - 53ms/step\n",
      "step  80/782 - loss: 1.1742 - acc_top1: 0.5779 - acc_top5: 0.9557 - 53ms/step\n",
      "step  90/782 - loss: 1.2230 - acc_top1: 0.5783 - acc_top5: 0.9549 - 53ms/step\n",
      "step 100/782 - loss: 1.3335 - acc_top1: 0.5772 - acc_top5: 0.9559 - 53ms/step\n",
      "step 110/782 - loss: 1.2986 - acc_top1: 0.5776 - acc_top5: 0.9567 - 52ms/step\n",
      "step 120/782 - loss: 1.3287 - acc_top1: 0.5805 - acc_top5: 0.9574 - 52ms/step\n",
      "step 130/782 - loss: 0.9694 - acc_top1: 0.5820 - acc_top5: 0.9573 - 52ms/step\n",
      "step 140/782 - loss: 1.2475 - acc_top1: 0.5813 - acc_top5: 0.9580 - 52ms/step\n",
      "step 150/782 - loss: 1.3965 - acc_top1: 0.5826 - acc_top5: 0.9579 - 52ms/step\n",
      "step 160/782 - loss: 1.3110 - acc_top1: 0.5831 - acc_top5: 0.9580 - 52ms/step\n",
      "step 170/782 - loss: 1.2497 - acc_top1: 0.5828 - acc_top5: 0.9576 - 52ms/step\n",
      "step 180/782 - loss: 1.2848 - acc_top1: 0.5840 - acc_top5: 0.9572 - 52ms/step\n",
      "step 190/782 - loss: 1.1685 - acc_top1: 0.5845 - acc_top5: 0.9568 - 52ms/step\n",
      "step 200/782 - loss: 1.1885 - acc_top1: 0.5847 - acc_top5: 0.9573 - 52ms/step\n",
      "step 210/782 - loss: 1.1992 - acc_top1: 0.5836 - acc_top5: 0.9575 - 51ms/step\n",
      "step 220/782 - loss: 1.0446 - acc_top1: 0.5833 - acc_top5: 0.9577 - 52ms/step\n",
      "step 230/782 - loss: 1.8769 - acc_top1: 0.5813 - acc_top5: 0.9571 - 52ms/step\n",
      "step 240/782 - loss: 1.3977 - acc_top1: 0.5794 - acc_top5: 0.9564 - 51ms/step\n",
      "step 250/782 - loss: 1.2652 - acc_top1: 0.5786 - acc_top5: 0.9559 - 52ms/step\n",
      "step 260/782 - loss: 1.2290 - acc_top1: 0.5784 - acc_top5: 0.9555 - 52ms/step\n",
      "step 270/782 - loss: 1.0968 - acc_top1: 0.5792 - acc_top5: 0.9557 - 51ms/step\n",
      "step 280/782 - loss: 1.8082 - acc_top1: 0.5794 - acc_top5: 0.9559 - 52ms/step\n",
      "step 290/782 - loss: 1.3202 - acc_top1: 0.5796 - acc_top5: 0.9557 - 52ms/step\n",
      "step 300/782 - loss: 1.1953 - acc_top1: 0.5787 - acc_top5: 0.9556 - 51ms/step\n",
      "step 310/782 - loss: 1.0225 - acc_top1: 0.5797 - acc_top5: 0.9554 - 51ms/step\n",
      "step 320/782 - loss: 1.1098 - acc_top1: 0.5798 - acc_top5: 0.9555 - 51ms/step\n",
      "step 330/782 - loss: 1.7536 - acc_top1: 0.5794 - acc_top5: 0.9554 - 51ms/step\n",
      "step 340/782 - loss: 1.0676 - acc_top1: 0.5804 - acc_top5: 0.9554 - 51ms/step\n",
      "step 350/782 - loss: 1.2524 - acc_top1: 0.5805 - acc_top5: 0.9552 - 51ms/step\n",
      "step 360/782 - loss: 1.2085 - acc_top1: 0.5805 - acc_top5: 0.9554 - 51ms/step\n",
      "step 370/782 - loss: 1.1370 - acc_top1: 0.5812 - acc_top5: 0.9554 - 51ms/step\n",
      "step 380/782 - loss: 1.3206 - acc_top1: 0.5806 - acc_top5: 0.9556 - 51ms/step\n",
      "step 390/782 - loss: 1.1822 - acc_top1: 0.5808 - acc_top5: 0.9558 - 51ms/step\n",
      "step 400/782 - loss: 1.2036 - acc_top1: 0.5805 - acc_top5: 0.9557 - 51ms/step\n",
      "step 410/782 - loss: 1.1138 - acc_top1: 0.5805 - acc_top5: 0.9558 - 51ms/step\n",
      "step 420/782 - loss: 1.1798 - acc_top1: 0.5802 - acc_top5: 0.9556 - 51ms/step\n",
      "step 430/782 - loss: 1.2632 - acc_top1: 0.5796 - acc_top5: 0.9556 - 51ms/step\n",
      "step 440/782 - loss: 1.3196 - acc_top1: 0.5798 - acc_top5: 0.9555 - 51ms/step\n",
      "step 450/782 - loss: 1.3049 - acc_top1: 0.5799 - acc_top5: 0.9553 - 51ms/step\n",
      "step 460/782 - loss: 1.6352 - acc_top1: 0.5794 - acc_top5: 0.9552 - 51ms/step\n",
      "step 470/782 - loss: 1.6697 - acc_top1: 0.5790 - acc_top5: 0.9551 - 51ms/step\n",
      "step 480/782 - loss: 1.0549 - acc_top1: 0.5791 - acc_top5: 0.9551 - 51ms/step\n",
      "step 490/782 - loss: 1.4549 - acc_top1: 0.5792 - acc_top5: 0.9552 - 51ms/step\n",
      "step 500/782 - loss: 1.0875 - acc_top1: 0.5795 - acc_top5: 0.9557 - 51ms/step\n",
      "step 510/782 - loss: 1.1668 - acc_top1: 0.5797 - acc_top5: 0.9558 - 51ms/step\n",
      "step 520/782 - loss: 1.1629 - acc_top1: 0.5799 - acc_top5: 0.9557 - 51ms/step\n",
      "step 530/782 - loss: 1.2658 - acc_top1: 0.5801 - acc_top5: 0.9558 - 51ms/step\n",
      "step 540/782 - loss: 1.3434 - acc_top1: 0.5797 - acc_top5: 0.9559 - 51ms/step\n",
      "step 550/782 - loss: 1.4362 - acc_top1: 0.5792 - acc_top5: 0.9559 - 51ms/step\n",
      "step 560/782 - loss: 0.9763 - acc_top1: 0.5791 - acc_top5: 0.9556 - 51ms/step\n",
      "step 570/782 - loss: 1.1976 - acc_top1: 0.5794 - acc_top5: 0.9557 - 51ms/step\n",
      "step 580/782 - loss: 1.1114 - acc_top1: 0.5797 - acc_top5: 0.9557 - 51ms/step\n",
      "step 590/782 - loss: 1.1721 - acc_top1: 0.5801 - acc_top5: 0.9555 - 51ms/step\n",
      "step 600/782 - loss: 1.1459 - acc_top1: 0.5807 - acc_top5: 0.9554 - 51ms/step\n",
      "step 610/782 - loss: 1.0315 - acc_top1: 0.5809 - acc_top5: 0.9556 - 51ms/step\n",
      "step 620/782 - loss: 1.1557 - acc_top1: 0.5806 - acc_top5: 0.9558 - 51ms/step\n",
      "step 630/782 - loss: 1.4718 - acc_top1: 0.5808 - acc_top5: 0.9556 - 51ms/step\n",
      "step 640/782 - loss: 1.2384 - acc_top1: 0.5814 - acc_top5: 0.9557 - 51ms/step\n",
      "step 650/782 - loss: 1.0924 - acc_top1: 0.5817 - acc_top5: 0.9556 - 51ms/step\n",
      "step 660/782 - loss: 1.0270 - acc_top1: 0.5819 - acc_top5: 0.9556 - 51ms/step\n",
      "step 670/782 - loss: 1.0677 - acc_top1: 0.5821 - acc_top5: 0.9556 - 51ms/step\n",
      "step 680/782 - loss: 1.1007 - acc_top1: 0.5822 - acc_top5: 0.9556 - 51ms/step\n",
      "step 690/782 - loss: 1.3542 - acc_top1: 0.5819 - acc_top5: 0.9554 - 51ms/step\n",
      "step 700/782 - loss: 1.3365 - acc_top1: 0.5816 - acc_top5: 0.9553 - 51ms/step\n",
      "step 710/782 - loss: 1.1616 - acc_top1: 0.5815 - acc_top5: 0.9553 - 51ms/step\n",
      "step 720/782 - loss: 1.4912 - acc_top1: 0.5813 - acc_top5: 0.9551 - 51ms/step\n",
      "step 730/782 - loss: 1.2840 - acc_top1: 0.5810 - acc_top5: 0.9551 - 51ms/step\n",
      "step 740/782 - loss: 1.0196 - acc_top1: 0.5811 - acc_top5: 0.9550 - 51ms/step\n",
      "step 750/782 - loss: 1.1180 - acc_top1: 0.5815 - acc_top5: 0.9552 - 51ms/step\n",
      "step 760/782 - loss: 0.8081 - acc_top1: 0.5817 - acc_top5: 0.9551 - 51ms/step\n",
      "step 770/782 - loss: 1.1146 - acc_top1: 0.5821 - acc_top5: 0.9554 - 51ms/step\n",
      "step 780/782 - loss: 1.2722 - acc_top1: 0.5816 - acc_top5: 0.9554 - 51ms/step\n",
      "step 782/782 - loss: 2.3836 - acc_top1: 0.5815 - acc_top5: 0.9554 - 51ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\12\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.0764 - acc_top1: 0.5594 - acc_top5: 0.9563 - 22ms/step\n",
      "step  20/157 - loss: 1.4776 - acc_top1: 0.5703 - acc_top5: 0.9563 - 21ms/step\n",
      "step  30/157 - loss: 1.4489 - acc_top1: 0.5750 - acc_top5: 0.9500 - 21ms/step\n",
      "step  40/157 - loss: 1.7414 - acc_top1: 0.5691 - acc_top5: 0.9500 - 21ms/step\n",
      "step  50/157 - loss: 1.2705 - acc_top1: 0.5769 - acc_top5: 0.9494 - 20ms/step\n",
      "step  60/157 - loss: 1.0289 - acc_top1: 0.5802 - acc_top5: 0.9477 - 21ms/step\n",
      "step  70/157 - loss: 1.4515 - acc_top1: 0.5842 - acc_top5: 0.9473 - 21ms/step\n",
      "step  80/157 - loss: 1.3846 - acc_top1: 0.5840 - acc_top5: 0.9480 - 21ms/step\n",
      "step  90/157 - loss: 1.0638 - acc_top1: 0.5818 - acc_top5: 0.9493 - 21ms/step\n",
      "step 100/157 - loss: 1.0073 - acc_top1: 0.5777 - acc_top5: 0.9498 - 21ms/step\n",
      "step 110/157 - loss: 1.6207 - acc_top1: 0.5723 - acc_top5: 0.9483 - 21ms/step\n",
      "step 120/157 - loss: 1.2644 - acc_top1: 0.5708 - acc_top5: 0.9474 - 21ms/step\n",
      "step 130/157 - loss: 1.4700 - acc_top1: 0.5710 - acc_top5: 0.9475 - 21ms/step\n",
      "step 140/157 - loss: 1.2539 - acc_top1: 0.5714 - acc_top5: 0.9469 - 21ms/step\n",
      "step 150/157 - loss: 1.7356 - acc_top1: 0.5708 - acc_top5: 0.9464 - 21ms/step\n",
      "step 157/157 - loss: 1.0033 - acc_top1: 0.5696 - acc_top5: 0.9465 - 21ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.1884 - acc_top1: 0.6000 - acc_top5: 0.9609 - 57ms/step\n",
      "step  20/782 - loss: 0.9691 - acc_top1: 0.5969 - acc_top5: 0.9578 - 56ms/step\n",
      "step  30/782 - loss: 1.2280 - acc_top1: 0.5911 - acc_top5: 0.9583 - 55ms/step\n",
      "step  40/782 - loss: 1.2827 - acc_top1: 0.5945 - acc_top5: 0.9578 - 54ms/step\n",
      "step  50/782 - loss: 1.0704 - acc_top1: 0.5994 - acc_top5: 0.9600 - 53ms/step\n",
      "step  60/782 - loss: 0.9954 - acc_top1: 0.6021 - acc_top5: 0.9609 - 54ms/step\n",
      "step  70/782 - loss: 0.9274 - acc_top1: 0.6065 - acc_top5: 0.9621 - 54ms/step\n",
      "step  80/782 - loss: 1.2718 - acc_top1: 0.6025 - acc_top5: 0.9607 - 53ms/step\n",
      "step  90/782 - loss: 1.2196 - acc_top1: 0.6007 - acc_top5: 0.9620 - 53ms/step\n",
      "step 100/782 - loss: 1.3618 - acc_top1: 0.5961 - acc_top5: 0.9614 - 53ms/step\n",
      "step 110/782 - loss: 1.1900 - acc_top1: 0.5959 - acc_top5: 0.9612 - 53ms/step\n",
      "step 120/782 - loss: 1.0033 - acc_top1: 0.5966 - acc_top5: 0.9613 - 53ms/step\n",
      "step 130/782 - loss: 1.2683 - acc_top1: 0.5938 - acc_top5: 0.9608 - 53ms/step\n",
      "step 140/782 - loss: 0.9434 - acc_top1: 0.5940 - acc_top5: 0.9612 - 52ms/step\n",
      "step 150/782 - loss: 0.9940 - acc_top1: 0.5929 - acc_top5: 0.9602 - 52ms/step\n",
      "step 160/782 - loss: 1.2184 - acc_top1: 0.5947 - acc_top5: 0.9604 - 52ms/step\n",
      "step 170/782 - loss: 1.4054 - acc_top1: 0.5934 - acc_top5: 0.9604 - 52ms/step\n",
      "step 180/782 - loss: 1.1428 - acc_top1: 0.5935 - acc_top5: 0.9605 - 52ms/step\n",
      "step 190/782 - loss: 0.9878 - acc_top1: 0.5935 - acc_top5: 0.9600 - 52ms/step\n",
      "step 200/782 - loss: 1.3203 - acc_top1: 0.5943 - acc_top5: 0.9599 - 52ms/step\n",
      "step 210/782 - loss: 1.0137 - acc_top1: 0.5955 - acc_top5: 0.9600 - 52ms/step\n",
      "step 220/782 - loss: 1.3462 - acc_top1: 0.5961 - acc_top5: 0.9600 - 52ms/step\n",
      "step 230/782 - loss: 1.4071 - acc_top1: 0.5953 - acc_top5: 0.9603 - 52ms/step\n",
      "step 240/782 - loss: 1.2057 - acc_top1: 0.5958 - acc_top5: 0.9601 - 52ms/step\n",
      "step 250/782 - loss: 1.3540 - acc_top1: 0.5944 - acc_top5: 0.9599 - 52ms/step\n",
      "step 260/782 - loss: 1.2459 - acc_top1: 0.5954 - acc_top5: 0.9596 - 52ms/step\n",
      "step 270/782 - loss: 1.2987 - acc_top1: 0.5951 - acc_top5: 0.9598 - 52ms/step\n",
      "step 280/782 - loss: 1.3565 - acc_top1: 0.5938 - acc_top5: 0.9599 - 52ms/step\n",
      "step 290/782 - loss: 1.1282 - acc_top1: 0.5946 - acc_top5: 0.9597 - 52ms/step\n",
      "step 300/782 - loss: 1.1444 - acc_top1: 0.5956 - acc_top5: 0.9599 - 52ms/step\n",
      "step 310/782 - loss: 1.0994 - acc_top1: 0.5962 - acc_top5: 0.9596 - 52ms/step\n",
      "step 320/782 - loss: 1.0404 - acc_top1: 0.5957 - acc_top5: 0.9592 - 52ms/step\n",
      "step 330/782 - loss: 1.1535 - acc_top1: 0.5963 - acc_top5: 0.9592 - 52ms/step\n",
      "step 340/782 - loss: 1.1367 - acc_top1: 0.5958 - acc_top5: 0.9591 - 52ms/step\n",
      "step 350/782 - loss: 1.2531 - acc_top1: 0.5958 - acc_top5: 0.9593 - 52ms/step\n",
      "step 360/782 - loss: 1.0622 - acc_top1: 0.5958 - acc_top5: 0.9591 - 52ms/step\n",
      "step 370/782 - loss: 0.9591 - acc_top1: 0.5957 - acc_top5: 0.9590 - 52ms/step\n",
      "step 380/782 - loss: 1.0571 - acc_top1: 0.5957 - acc_top5: 0.9589 - 51ms/step\n",
      "step 390/782 - loss: 0.9205 - acc_top1: 0.5955 - acc_top5: 0.9590 - 51ms/step\n",
      "step 400/782 - loss: 1.3161 - acc_top1: 0.5957 - acc_top5: 0.9586 - 51ms/step\n",
      "step 410/782 - loss: 1.7225 - acc_top1: 0.5955 - acc_top5: 0.9587 - 51ms/step\n",
      "step 420/782 - loss: 1.2176 - acc_top1: 0.5945 - acc_top5: 0.9586 - 51ms/step\n",
      "step 430/782 - loss: 1.3753 - acc_top1: 0.5943 - acc_top5: 0.9588 - 51ms/step\n",
      "step 440/782 - loss: 1.1805 - acc_top1: 0.5939 - acc_top5: 0.9586 - 51ms/step\n",
      "step 450/782 - loss: 1.2877 - acc_top1: 0.5942 - acc_top5: 0.9587 - 51ms/step\n",
      "step 460/782 - loss: 1.2500 - acc_top1: 0.5948 - acc_top5: 0.9585 - 51ms/step\n",
      "step 470/782 - loss: 1.2102 - acc_top1: 0.5955 - acc_top5: 0.9581 - 51ms/step\n",
      "step 480/782 - loss: 1.2028 - acc_top1: 0.5955 - acc_top5: 0.9580 - 51ms/step\n",
      "step 490/782 - loss: 1.5930 - acc_top1: 0.5961 - acc_top5: 0.9583 - 51ms/step\n",
      "step 500/782 - loss: 1.2164 - acc_top1: 0.5964 - acc_top5: 0.9584 - 51ms/step\n",
      "step 510/782 - loss: 1.4210 - acc_top1: 0.5958 - acc_top5: 0.9586 - 51ms/step\n",
      "step 520/782 - loss: 1.0010 - acc_top1: 0.5955 - acc_top5: 0.9586 - 51ms/step\n",
      "step 530/782 - loss: 1.1329 - acc_top1: 0.5953 - acc_top5: 0.9585 - 52ms/step\n",
      "step 540/782 - loss: 1.3027 - acc_top1: 0.5951 - acc_top5: 0.9582 - 52ms/step\n",
      "step 550/782 - loss: 1.4472 - acc_top1: 0.5948 - acc_top5: 0.9584 - 52ms/step\n",
      "step 560/782 - loss: 1.4722 - acc_top1: 0.5948 - acc_top5: 0.9581 - 52ms/step\n",
      "step 570/782 - loss: 1.0993 - acc_top1: 0.5952 - acc_top5: 0.9583 - 52ms/step\n",
      "step 580/782 - loss: 1.1719 - acc_top1: 0.5949 - acc_top5: 0.9582 - 52ms/step\n",
      "step 590/782 - loss: 1.3442 - acc_top1: 0.5950 - acc_top5: 0.9584 - 52ms/step\n",
      "step 600/782 - loss: 1.4062 - acc_top1: 0.5954 - acc_top5: 0.9584 - 52ms/step\n",
      "step 610/782 - loss: 1.1384 - acc_top1: 0.5953 - acc_top5: 0.9586 - 52ms/step\n",
      "step 620/782 - loss: 1.2901 - acc_top1: 0.5952 - acc_top5: 0.9586 - 52ms/step\n",
      "step 630/782 - loss: 1.5142 - acc_top1: 0.5953 - acc_top5: 0.9588 - 52ms/step\n",
      "step 640/782 - loss: 1.2580 - acc_top1: 0.5951 - acc_top5: 0.9589 - 52ms/step\n",
      "step 650/782 - loss: 1.2923 - acc_top1: 0.5955 - acc_top5: 0.9590 - 52ms/step\n",
      "step 660/782 - loss: 1.3640 - acc_top1: 0.5952 - acc_top5: 0.9589 - 52ms/step\n",
      "step 670/782 - loss: 1.0977 - acc_top1: 0.5952 - acc_top5: 0.9588 - 52ms/step\n",
      "step 680/782 - loss: 1.4358 - acc_top1: 0.5952 - acc_top5: 0.9588 - 52ms/step\n",
      "step 690/782 - loss: 1.1701 - acc_top1: 0.5954 - acc_top5: 0.9589 - 52ms/step\n",
      "step 700/782 - loss: 1.3184 - acc_top1: 0.5956 - acc_top5: 0.9588 - 52ms/step\n",
      "step 710/782 - loss: 1.0938 - acc_top1: 0.5954 - acc_top5: 0.9588 - 52ms/step\n",
      "step 720/782 - loss: 1.0171 - acc_top1: 0.5955 - acc_top5: 0.9588 - 52ms/step\n",
      "step 730/782 - loss: 1.1499 - acc_top1: 0.5958 - acc_top5: 0.9588 - 52ms/step\n",
      "step 740/782 - loss: 0.9448 - acc_top1: 0.5954 - acc_top5: 0.9589 - 52ms/step\n",
      "step 750/782 - loss: 1.3191 - acc_top1: 0.5954 - acc_top5: 0.9590 - 52ms/step\n",
      "step 760/782 - loss: 1.1987 - acc_top1: 0.5956 - acc_top5: 0.9592 - 52ms/step\n",
      "step 770/782 - loss: 1.1749 - acc_top1: 0.5955 - acc_top5: 0.9590 - 52ms/step\n",
      "step 780/782 - loss: 1.3913 - acc_top1: 0.5951 - acc_top5: 0.9588 - 52ms/step\n",
      "step 782/782 - loss: 1.9117 - acc_top1: 0.5950 - acc_top5: 0.9588 - 52ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\13\n",
      "Eval begin...\n",
      "step  10/157 - loss: 3.5366 - acc_top1: 0.5750 - acc_top5: 0.9500 - 23ms/step\n",
      "step  20/157 - loss: 4.8695 - acc_top1: 0.5758 - acc_top5: 0.9523 - 22ms/step\n",
      "step  30/157 - loss: 9.6183 - acc_top1: 0.5714 - acc_top5: 0.9484 - 22ms/step\n",
      "step  40/157 - loss: 1.7387 - acc_top1: 0.5672 - acc_top5: 0.9516 - 21ms/step\n",
      "step  50/157 - loss: 3.7840 - acc_top1: 0.5741 - acc_top5: 0.9497 - 22ms/step\n",
      "step  60/157 - loss: 5.5299 - acc_top1: 0.5797 - acc_top5: 0.9471 - 22ms/step\n",
      "step  70/157 - loss: 35.0276 - acc_top1: 0.5781 - acc_top5: 0.9471 - 21ms/step\n",
      "step  80/157 - loss: 8.3328 - acc_top1: 0.5816 - acc_top5: 0.9469 - 21ms/step\n",
      "step  90/157 - loss: 4.4062 - acc_top1: 0.5804 - acc_top5: 0.9486 - 21ms/step\n",
      "step 100/157 - loss: 9.9501 - acc_top1: 0.5766 - acc_top5: 0.9487 - 21ms/step\n",
      "step 110/157 - loss: 2.3393 - acc_top1: 0.5753 - acc_top5: 0.9479 - 21ms/step\n",
      "step 120/157 - loss: 5.1188 - acc_top1: 0.5727 - acc_top5: 0.9480 - 21ms/step\n",
      "step 130/157 - loss: 1.4806 - acc_top1: 0.5727 - acc_top5: 0.9478 - 21ms/step\n",
      "step 140/157 - loss: 5.4029 - acc_top1: 0.5746 - acc_top5: 0.9475 - 21ms/step\n",
      "step 150/157 - loss: 21.1695 - acc_top1: 0.5744 - acc_top5: 0.9474 - 21ms/step\n",
      "step 157/157 - loss: 0.7218 - acc_top1: 0.5735 - acc_top5: 0.9471 - 21ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.0119 - acc_top1: 0.6047 - acc_top5: 0.9578 - 57ms/step\n",
      "step  20/782 - loss: 1.0740 - acc_top1: 0.6227 - acc_top5: 0.9664 - 56ms/step\n",
      "step  30/782 - loss: 1.2650 - acc_top1: 0.6188 - acc_top5: 0.9641 - 57ms/step\n",
      "step  40/782 - loss: 1.1625 - acc_top1: 0.6176 - acc_top5: 0.9641 - 56ms/step\n",
      "step  50/782 - loss: 0.7729 - acc_top1: 0.6206 - acc_top5: 0.9650 - 56ms/step\n",
      "step  60/782 - loss: 1.4157 - acc_top1: 0.6242 - acc_top5: 0.9648 - 57ms/step\n",
      "step  70/782 - loss: 1.3448 - acc_top1: 0.6261 - acc_top5: 0.9652 - 57ms/step\n",
      "step  80/782 - loss: 0.8817 - acc_top1: 0.6285 - acc_top5: 0.9650 - 57ms/step\n",
      "step  90/782 - loss: 1.1135 - acc_top1: 0.6314 - acc_top5: 0.9651 - 57ms/step\n",
      "step 100/782 - loss: 0.9987 - acc_top1: 0.6342 - acc_top5: 0.9652 - 57ms/step\n",
      "step 110/782 - loss: 1.1184 - acc_top1: 0.6317 - acc_top5: 0.9641 - 56ms/step\n",
      "step 120/782 - loss: 1.0082 - acc_top1: 0.6305 - acc_top5: 0.9639 - 56ms/step\n",
      "step 130/782 - loss: 1.3378 - acc_top1: 0.6303 - acc_top5: 0.9633 - 56ms/step\n",
      "step 140/782 - loss: 0.7828 - acc_top1: 0.6319 - acc_top5: 0.9638 - 56ms/step\n",
      "step 150/782 - loss: 1.0587 - acc_top1: 0.6317 - acc_top5: 0.9629 - 55ms/step\n",
      "step 160/782 - loss: 1.1438 - acc_top1: 0.6314 - acc_top5: 0.9635 - 55ms/step\n",
      "step 170/782 - loss: 0.8958 - acc_top1: 0.6310 - acc_top5: 0.9635 - 55ms/step\n",
      "step 180/782 - loss: 1.0499 - acc_top1: 0.6297 - acc_top5: 0.9639 - 55ms/step\n",
      "step 190/782 - loss: 1.0727 - acc_top1: 0.6299 - acc_top5: 0.9638 - 55ms/step\n",
      "step 200/782 - loss: 1.1528 - acc_top1: 0.6285 - acc_top5: 0.9636 - 55ms/step\n",
      "step 210/782 - loss: 1.0131 - acc_top1: 0.6280 - acc_top5: 0.9635 - 55ms/step\n",
      "step 220/782 - loss: 1.0885 - acc_top1: 0.6277 - acc_top5: 0.9632 - 55ms/step\n",
      "step 230/782 - loss: 1.3909 - acc_top1: 0.6276 - acc_top5: 0.9634 - 55ms/step\n",
      "step 240/782 - loss: 1.0104 - acc_top1: 0.6272 - acc_top5: 0.9637 - 55ms/step\n",
      "step 250/782 - loss: 1.0314 - acc_top1: 0.6272 - acc_top5: 0.9642 - 55ms/step\n",
      "step 260/782 - loss: 1.5345 - acc_top1: 0.6253 - acc_top5: 0.9643 - 55ms/step\n",
      "step 270/782 - loss: 1.0936 - acc_top1: 0.6260 - acc_top5: 0.9641 - 55ms/step\n",
      "step 280/782 - loss: 0.9461 - acc_top1: 0.6271 - acc_top5: 0.9642 - 55ms/step\n",
      "step 290/782 - loss: 1.0808 - acc_top1: 0.6265 - acc_top5: 0.9639 - 55ms/step\n",
      "step 300/782 - loss: 1.5636 - acc_top1: 0.6257 - acc_top5: 0.9635 - 55ms/step\n",
      "step 310/782 - loss: 0.9454 - acc_top1: 0.6260 - acc_top5: 0.9640 - 55ms/step\n",
      "step 320/782 - loss: 1.3618 - acc_top1: 0.6259 - acc_top5: 0.9643 - 55ms/step\n",
      "step 330/782 - loss: 1.2140 - acc_top1: 0.6259 - acc_top5: 0.9640 - 55ms/step\n",
      "step 340/782 - loss: 0.9715 - acc_top1: 0.6255 - acc_top5: 0.9643 - 55ms/step\n",
      "step 350/782 - loss: 1.2393 - acc_top1: 0.6247 - acc_top5: 0.9644 - 55ms/step\n",
      "step 360/782 - loss: 1.3645 - acc_top1: 0.6238 - acc_top5: 0.9645 - 55ms/step\n",
      "step 370/782 - loss: 1.3200 - acc_top1: 0.6234 - acc_top5: 0.9645 - 55ms/step\n",
      "step 380/782 - loss: 0.9184 - acc_top1: 0.6241 - acc_top5: 0.9644 - 55ms/step\n",
      "step 390/782 - loss: 1.0790 - acc_top1: 0.6237 - acc_top5: 0.9645 - 55ms/step\n",
      "step 400/782 - loss: 1.3164 - acc_top1: 0.6236 - acc_top5: 0.9643 - 55ms/step\n",
      "step 410/782 - loss: 1.0742 - acc_top1: 0.6235 - acc_top5: 0.9642 - 55ms/step\n",
      "step 420/782 - loss: 1.4923 - acc_top1: 0.6235 - acc_top5: 0.9642 - 55ms/step\n",
      "step 430/782 - loss: 1.0054 - acc_top1: 0.6226 - acc_top5: 0.9638 - 55ms/step\n",
      "step 440/782 - loss: 1.0559 - acc_top1: 0.6218 - acc_top5: 0.9640 - 54ms/step\n",
      "step 450/782 - loss: 1.4171 - acc_top1: 0.6214 - acc_top5: 0.9637 - 54ms/step\n",
      "step 460/782 - loss: 0.9432 - acc_top1: 0.6212 - acc_top5: 0.9637 - 54ms/step\n",
      "step 470/782 - loss: 1.0920 - acc_top1: 0.6216 - acc_top5: 0.9638 - 54ms/step\n",
      "step 480/782 - loss: 0.9448 - acc_top1: 0.6221 - acc_top5: 0.9640 - 54ms/step\n",
      "step 490/782 - loss: 1.2234 - acc_top1: 0.6224 - acc_top5: 0.9640 - 54ms/step\n",
      "step 500/782 - loss: 0.8884 - acc_top1: 0.6225 - acc_top5: 0.9642 - 54ms/step\n",
      "step 510/782 - loss: 0.9885 - acc_top1: 0.6229 - acc_top5: 0.9639 - 54ms/step\n",
      "step 520/782 - loss: 0.8023 - acc_top1: 0.6227 - acc_top5: 0.9640 - 54ms/step\n",
      "step 530/782 - loss: 0.9947 - acc_top1: 0.6227 - acc_top5: 0.9641 - 54ms/step\n",
      "step 540/782 - loss: 1.2023 - acc_top1: 0.6227 - acc_top5: 0.9641 - 54ms/step\n",
      "step 550/782 - loss: 1.6918 - acc_top1: 0.6226 - acc_top5: 0.9639 - 54ms/step\n",
      "step 560/782 - loss: 1.0556 - acc_top1: 0.6225 - acc_top5: 0.9636 - 54ms/step\n",
      "step 570/782 - loss: 1.1676 - acc_top1: 0.6225 - acc_top5: 0.9636 - 54ms/step\n",
      "step 580/782 - loss: 1.0899 - acc_top1: 0.6221 - acc_top5: 0.9637 - 54ms/step\n",
      "step 590/782 - loss: 1.0800 - acc_top1: 0.6218 - acc_top5: 0.9638 - 54ms/step\n",
      "step 600/782 - loss: 1.1543 - acc_top1: 0.6218 - acc_top5: 0.9637 - 54ms/step\n",
      "step 610/782 - loss: 0.8583 - acc_top1: 0.6218 - acc_top5: 0.9636 - 54ms/step\n",
      "step 620/782 - loss: 1.2036 - acc_top1: 0.6215 - acc_top5: 0.9637 - 54ms/step\n",
      "step 630/782 - loss: 1.4186 - acc_top1: 0.6209 - acc_top5: 0.9636 - 54ms/step\n",
      "step 640/782 - loss: 1.1762 - acc_top1: 0.6206 - acc_top5: 0.9636 - 54ms/step\n",
      "step 650/782 - loss: 1.0536 - acc_top1: 0.6206 - acc_top5: 0.9634 - 54ms/step\n",
      "step 660/782 - loss: 1.0421 - acc_top1: 0.6203 - acc_top5: 0.9634 - 54ms/step\n",
      "step 670/782 - loss: 0.9871 - acc_top1: 0.6207 - acc_top5: 0.9636 - 54ms/step\n",
      "step 680/782 - loss: 1.1187 - acc_top1: 0.6208 - acc_top5: 0.9636 - 54ms/step\n",
      "step 690/782 - loss: 1.0472 - acc_top1: 0.6214 - acc_top5: 0.9636 - 54ms/step\n",
      "step 700/782 - loss: 1.3867 - acc_top1: 0.6214 - acc_top5: 0.9636 - 54ms/step\n",
      "step 710/782 - loss: 1.1802 - acc_top1: 0.6215 - acc_top5: 0.9639 - 54ms/step\n",
      "step 720/782 - loss: 1.2886 - acc_top1: 0.6214 - acc_top5: 0.9640 - 54ms/step\n",
      "step 730/782 - loss: 1.5220 - acc_top1: 0.6211 - acc_top5: 0.9639 - 54ms/step\n",
      "step 740/782 - loss: 1.0308 - acc_top1: 0.6210 - acc_top5: 0.9640 - 54ms/step\n",
      "step 750/782 - loss: 0.9950 - acc_top1: 0.6209 - acc_top5: 0.9639 - 54ms/step\n",
      "step 760/782 - loss: 1.0307 - acc_top1: 0.6208 - acc_top5: 0.9639 - 54ms/step\n",
      "step 770/782 - loss: 1.1797 - acc_top1: 0.6206 - acc_top5: 0.9638 - 54ms/step\n",
      "step 780/782 - loss: 0.8210 - acc_top1: 0.6203 - acc_top5: 0.9638 - 54ms/step\n",
      "step 782/782 - loss: 1.8815 - acc_top1: 0.6202 - acc_top5: 0.9638 - 54ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\14\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.9430 - acc_top1: 0.5813 - acc_top5: 0.9516 - 21ms/step\n",
      "step  20/157 - loss: 1.9769 - acc_top1: 0.5750 - acc_top5: 0.9500 - 22ms/step\n",
      "step  30/157 - loss: 17.2037 - acc_top1: 0.5766 - acc_top5: 0.9448 - 21ms/step\n",
      "step  40/157 - loss: 1.7384 - acc_top1: 0.5723 - acc_top5: 0.9461 - 20ms/step\n",
      "step  50/157 - loss: 2.3779 - acc_top1: 0.5766 - acc_top5: 0.9453 - 21ms/step\n",
      "step  60/157 - loss: 1.4679 - acc_top1: 0.5779 - acc_top5: 0.9414 - 21ms/step\n",
      "step  70/157 - loss: 67.7577 - acc_top1: 0.5754 - acc_top5: 0.9397 - 21ms/step\n",
      "step  80/157 - loss: 13.4706 - acc_top1: 0.5789 - acc_top5: 0.9406 - 21ms/step\n",
      "step  90/157 - loss: 5.9935 - acc_top1: 0.5766 - acc_top5: 0.9425 - 21ms/step\n",
      "step 100/157 - loss: 14.9336 - acc_top1: 0.5755 - acc_top5: 0.9442 - 21ms/step\n",
      "step 110/157 - loss: 1.5471 - acc_top1: 0.5744 - acc_top5: 0.9435 - 21ms/step\n",
      "step 120/157 - loss: 1.4582 - acc_top1: 0.5754 - acc_top5: 0.9432 - 21ms/step\n",
      "step 130/157 - loss: 1.4303 - acc_top1: 0.5757 - acc_top5: 0.9433 - 21ms/step\n",
      "step 140/157 - loss: 2.5082 - acc_top1: 0.5767 - acc_top5: 0.9435 - 21ms/step\n",
      "step 150/157 - loss: 27.0449 - acc_top1: 0.5771 - acc_top5: 0.9432 - 20ms/step\n",
      "step 157/157 - loss: 0.6129 - acc_top1: 0.5766 - acc_top5: 0.9432 - 20ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.0301 - acc_top1: 0.6438 - acc_top5: 0.9609 - 53ms/step\n",
      "step  20/782 - loss: 1.0342 - acc_top1: 0.6453 - acc_top5: 0.9672 - 52ms/step\n",
      "step  30/782 - loss: 0.9664 - acc_top1: 0.6391 - acc_top5: 0.9688 - 51ms/step\n",
      "step  40/782 - loss: 1.0045 - acc_top1: 0.6406 - acc_top5: 0.9684 - 51ms/step\n",
      "step  50/782 - loss: 1.0749 - acc_top1: 0.6425 - acc_top5: 0.9669 - 51ms/step\n",
      "step  60/782 - loss: 1.4048 - acc_top1: 0.6385 - acc_top5: 0.9688 - 51ms/step\n",
      "step  70/782 - loss: 1.0881 - acc_top1: 0.6386 - acc_top5: 0.9690 - 51ms/step\n",
      "step  80/782 - loss: 0.9207 - acc_top1: 0.6371 - acc_top5: 0.9695 - 50ms/step\n",
      "step  90/782 - loss: 1.2537 - acc_top1: 0.6372 - acc_top5: 0.9691 - 50ms/step\n",
      "step 100/782 - loss: 1.1926 - acc_top1: 0.6406 - acc_top5: 0.9695 - 50ms/step\n",
      "step 110/782 - loss: 0.8627 - acc_top1: 0.6399 - acc_top5: 0.9695 - 50ms/step\n",
      "step 120/782 - loss: 0.7534 - acc_top1: 0.6404 - acc_top5: 0.9689 - 50ms/step\n",
      "step 130/782 - loss: 1.0509 - acc_top1: 0.6413 - acc_top5: 0.9689 - 50ms/step\n",
      "step 140/782 - loss: 1.0915 - acc_top1: 0.6415 - acc_top5: 0.9681 - 51ms/step\n",
      "step 150/782 - loss: 0.7992 - acc_top1: 0.6427 - acc_top5: 0.9689 - 51ms/step\n",
      "step 160/782 - loss: 1.0589 - acc_top1: 0.6417 - acc_top5: 0.9688 - 51ms/step\n",
      "step 170/782 - loss: 0.9302 - acc_top1: 0.6427 - acc_top5: 0.9689 - 51ms/step\n",
      "step 180/782 - loss: 0.9233 - acc_top1: 0.6441 - acc_top5: 0.9693 - 51ms/step\n",
      "step 190/782 - loss: 1.2470 - acc_top1: 0.6424 - acc_top5: 0.9685 - 51ms/step\n",
      "step 200/782 - loss: 0.9770 - acc_top1: 0.6403 - acc_top5: 0.9684 - 51ms/step\n",
      "step 210/782 - loss: 1.2429 - acc_top1: 0.6406 - acc_top5: 0.9685 - 51ms/step\n",
      "step 220/782 - loss: 0.9287 - acc_top1: 0.6415 - acc_top5: 0.9683 - 51ms/step\n",
      "step 230/782 - loss: 1.2226 - acc_top1: 0.6412 - acc_top5: 0.9685 - 51ms/step\n",
      "step 240/782 - loss: 1.0426 - acc_top1: 0.6415 - acc_top5: 0.9688 - 51ms/step\n",
      "step 250/782 - loss: 0.9479 - acc_top1: 0.6408 - acc_top5: 0.9694 - 51ms/step\n",
      "step 260/782 - loss: 1.2000 - acc_top1: 0.6409 - acc_top5: 0.9691 - 51ms/step\n",
      "step 270/782 - loss: 0.8457 - acc_top1: 0.6409 - acc_top5: 0.9690 - 51ms/step\n",
      "step 280/782 - loss: 1.0998 - acc_top1: 0.6398 - acc_top5: 0.9690 - 51ms/step\n",
      "step 290/782 - loss: 0.8551 - acc_top1: 0.6383 - acc_top5: 0.9689 - 51ms/step\n",
      "step 300/782 - loss: 0.9170 - acc_top1: 0.6373 - acc_top5: 0.9687 - 51ms/step\n",
      "step 310/782 - loss: 0.9521 - acc_top1: 0.6381 - acc_top5: 0.9685 - 51ms/step\n",
      "step 320/782 - loss: 0.9504 - acc_top1: 0.6368 - acc_top5: 0.9685 - 51ms/step\n",
      "step 330/782 - loss: 1.1729 - acc_top1: 0.6367 - acc_top5: 0.9684 - 51ms/step\n",
      "step 340/782 - loss: 0.8952 - acc_top1: 0.6365 - acc_top5: 0.9685 - 51ms/step\n",
      "step 350/782 - loss: 0.9014 - acc_top1: 0.6367 - acc_top5: 0.9687 - 51ms/step\n",
      "step 360/782 - loss: 1.0210 - acc_top1: 0.6359 - acc_top5: 0.9687 - 51ms/step\n",
      "step 370/782 - loss: 1.3334 - acc_top1: 0.6366 - acc_top5: 0.9690 - 51ms/step\n",
      "step 380/782 - loss: 1.0782 - acc_top1: 0.6362 - acc_top5: 0.9691 - 51ms/step\n",
      "step 390/782 - loss: 1.3537 - acc_top1: 0.6364 - acc_top5: 0.9691 - 51ms/step\n",
      "step 400/782 - loss: 1.2188 - acc_top1: 0.6365 - acc_top5: 0.9689 - 51ms/step\n",
      "step 410/782 - loss: 1.2648 - acc_top1: 0.6367 - acc_top5: 0.9688 - 51ms/step\n",
      "step 420/782 - loss: 1.1528 - acc_top1: 0.6364 - acc_top5: 0.9687 - 51ms/step\n",
      "step 430/782 - loss: 1.0395 - acc_top1: 0.6363 - acc_top5: 0.9686 - 51ms/step\n",
      "step 440/782 - loss: 1.0736 - acc_top1: 0.6365 - acc_top5: 0.9682 - 51ms/step\n",
      "step 450/782 - loss: 0.8533 - acc_top1: 0.6366 - acc_top5: 0.9683 - 51ms/step\n",
      "step 460/782 - loss: 0.9758 - acc_top1: 0.6362 - acc_top5: 0.9683 - 51ms/step\n",
      "step 470/782 - loss: 1.2753 - acc_top1: 0.6361 - acc_top5: 0.9684 - 51ms/step\n",
      "step 480/782 - loss: 1.3575 - acc_top1: 0.6361 - acc_top5: 0.9684 - 51ms/step\n",
      "step 490/782 - loss: 1.3186 - acc_top1: 0.6359 - acc_top5: 0.9684 - 51ms/step\n",
      "step 500/782 - loss: 0.8336 - acc_top1: 0.6356 - acc_top5: 0.9685 - 51ms/step\n",
      "step 510/782 - loss: 1.1008 - acc_top1: 0.6357 - acc_top5: 0.9683 - 51ms/step\n",
      "step 520/782 - loss: 0.7639 - acc_top1: 0.6357 - acc_top5: 0.9682 - 51ms/step\n",
      "step 530/782 - loss: 1.1984 - acc_top1: 0.6357 - acc_top5: 0.9682 - 51ms/step\n",
      "step 540/782 - loss: 0.7160 - acc_top1: 0.6361 - acc_top5: 0.9682 - 51ms/step\n",
      "step 550/782 - loss: 1.0618 - acc_top1: 0.6366 - acc_top5: 0.9685 - 51ms/step\n",
      "step 560/782 - loss: 1.0544 - acc_top1: 0.6367 - acc_top5: 0.9685 - 51ms/step\n",
      "step 570/782 - loss: 1.2428 - acc_top1: 0.6367 - acc_top5: 0.9685 - 51ms/step\n",
      "step 580/782 - loss: 0.9433 - acc_top1: 0.6370 - acc_top5: 0.9687 - 51ms/step\n",
      "step 590/782 - loss: 0.7694 - acc_top1: 0.6372 - acc_top5: 0.9686 - 51ms/step\n",
      "step 600/782 - loss: 1.0770 - acc_top1: 0.6374 - acc_top5: 0.9686 - 51ms/step\n",
      "step 610/782 - loss: 0.9582 - acc_top1: 0.6374 - acc_top5: 0.9685 - 51ms/step\n",
      "step 620/782 - loss: 1.1920 - acc_top1: 0.6374 - acc_top5: 0.9685 - 51ms/step\n",
      "step 630/782 - loss: 1.0095 - acc_top1: 0.6373 - acc_top5: 0.9687 - 51ms/step\n",
      "step 640/782 - loss: 0.9705 - acc_top1: 0.6372 - acc_top5: 0.9686 - 51ms/step\n",
      "step 650/782 - loss: 1.1843 - acc_top1: 0.6374 - acc_top5: 0.9685 - 51ms/step\n",
      "step 660/782 - loss: 1.0804 - acc_top1: 0.6369 - acc_top5: 0.9685 - 51ms/step\n",
      "step 670/782 - loss: 1.1270 - acc_top1: 0.6367 - acc_top5: 0.9685 - 51ms/step\n",
      "step 680/782 - loss: 1.2548 - acc_top1: 0.6368 - acc_top5: 0.9685 - 50ms/step\n",
      "step 690/782 - loss: 1.1201 - acc_top1: 0.6370 - acc_top5: 0.9686 - 50ms/step\n",
      "step 700/782 - loss: 1.3081 - acc_top1: 0.6367 - acc_top5: 0.9686 - 50ms/step\n",
      "step 710/782 - loss: 0.9146 - acc_top1: 0.6365 - acc_top5: 0.9686 - 50ms/step\n",
      "step 720/782 - loss: 1.1032 - acc_top1: 0.6365 - acc_top5: 0.9685 - 50ms/step\n",
      "step 730/782 - loss: 1.4584 - acc_top1: 0.6364 - acc_top5: 0.9684 - 50ms/step\n",
      "step 740/782 - loss: 1.2979 - acc_top1: 0.6363 - acc_top5: 0.9684 - 50ms/step\n",
      "step 750/782 - loss: 0.9207 - acc_top1: 0.6366 - acc_top5: 0.9684 - 50ms/step\n",
      "step 760/782 - loss: 1.2192 - acc_top1: 0.6366 - acc_top5: 0.9685 - 50ms/step\n",
      "step 770/782 - loss: 1.2215 - acc_top1: 0.6361 - acc_top5: 0.9684 - 50ms/step\n",
      "step 780/782 - loss: 1.0879 - acc_top1: 0.6359 - acc_top5: 0.9683 - 50ms/step\n",
      "step 782/782 - loss: 1.8124 - acc_top1: 0.6359 - acc_top5: 0.9684 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\15\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.1809 - acc_top1: 0.5953 - acc_top5: 0.9641 - 20ms/step\n",
      "step  20/157 - loss: 1.3280 - acc_top1: 0.6070 - acc_top5: 0.9633 - 19ms/step\n",
      "step  30/157 - loss: 4.1031 - acc_top1: 0.6000 - acc_top5: 0.9557 - 19ms/step\n",
      "step  40/157 - loss: 1.7684 - acc_top1: 0.6000 - acc_top5: 0.9551 - 19ms/step\n",
      "step  50/157 - loss: 1.0471 - acc_top1: 0.5969 - acc_top5: 0.9537 - 19ms/step\n",
      "step  60/157 - loss: 0.9754 - acc_top1: 0.5956 - acc_top5: 0.9534 - 19ms/step\n",
      "step  70/157 - loss: 16.1484 - acc_top1: 0.5958 - acc_top5: 0.9520 - 19ms/step\n",
      "step  80/157 - loss: 4.3552 - acc_top1: 0.6008 - acc_top5: 0.9525 - 19ms/step\n",
      "step  90/157 - loss: 1.3219 - acc_top1: 0.5993 - acc_top5: 0.9531 - 19ms/step\n",
      "step 100/157 - loss: 3.4388 - acc_top1: 0.5962 - acc_top5: 0.9541 - 19ms/step\n",
      "step 110/157 - loss: 1.4960 - acc_top1: 0.5925 - acc_top5: 0.9530 - 19ms/step\n",
      "step 120/157 - loss: 1.1016 - acc_top1: 0.5918 - acc_top5: 0.9531 - 19ms/step\n",
      "step 130/157 - loss: 1.4818 - acc_top1: 0.5928 - acc_top5: 0.9535 - 19ms/step\n",
      "step 140/157 - loss: 1.0503 - acc_top1: 0.5953 - acc_top5: 0.9530 - 19ms/step\n",
      "step 150/157 - loss: 7.5579 - acc_top1: 0.5955 - acc_top5: 0.9527 - 19ms/step\n",
      "step 157/157 - loss: 0.6641 - acc_top1: 0.5947 - acc_top5: 0.9533 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.0880 - acc_top1: 0.6266 - acc_top5: 0.9797 - 52ms/step\n",
      "step  20/782 - loss: 1.1770 - acc_top1: 0.6422 - acc_top5: 0.9805 - 51ms/step\n",
      "step  30/782 - loss: 1.0156 - acc_top1: 0.6578 - acc_top5: 0.9781 - 51ms/step\n",
      "step  40/782 - loss: 0.7200 - acc_top1: 0.6605 - acc_top5: 0.9777 - 51ms/step\n",
      "step  50/782 - loss: 1.1873 - acc_top1: 0.6578 - acc_top5: 0.9741 - 51ms/step\n",
      "step  60/782 - loss: 0.9158 - acc_top1: 0.6609 - acc_top5: 0.9719 - 51ms/step\n",
      "step  70/782 - loss: 0.8412 - acc_top1: 0.6676 - acc_top5: 0.9741 - 51ms/step\n",
      "step  80/782 - loss: 0.9702 - acc_top1: 0.6625 - acc_top5: 0.9750 - 51ms/step\n",
      "step  90/782 - loss: 1.1031 - acc_top1: 0.6642 - acc_top5: 0.9740 - 51ms/step\n",
      "step 100/782 - loss: 1.2314 - acc_top1: 0.6645 - acc_top5: 0.9725 - 51ms/step\n",
      "step 110/782 - loss: 0.9922 - acc_top1: 0.6625 - acc_top5: 0.9722 - 50ms/step\n",
      "step 120/782 - loss: 0.9155 - acc_top1: 0.6629 - acc_top5: 0.9727 - 51ms/step\n",
      "step 130/782 - loss: 0.9523 - acc_top1: 0.6631 - acc_top5: 0.9724 - 51ms/step\n",
      "step 140/782 - loss: 1.2486 - acc_top1: 0.6626 - acc_top5: 0.9733 - 51ms/step\n",
      "step 150/782 - loss: 1.2437 - acc_top1: 0.6639 - acc_top5: 0.9734 - 50ms/step\n",
      "step 160/782 - loss: 0.8102 - acc_top1: 0.6619 - acc_top5: 0.9729 - 50ms/step\n",
      "step 170/782 - loss: 0.9464 - acc_top1: 0.6627 - acc_top5: 0.9733 - 50ms/step\n",
      "step 180/782 - loss: 0.8682 - acc_top1: 0.6637 - acc_top5: 0.9733 - 50ms/step\n",
      "step 190/782 - loss: 0.9152 - acc_top1: 0.6654 - acc_top5: 0.9729 - 51ms/step\n",
      "step 200/782 - loss: 1.2470 - acc_top1: 0.6644 - acc_top5: 0.9723 - 51ms/step\n",
      "step 210/782 - loss: 0.8627 - acc_top1: 0.6661 - acc_top5: 0.9722 - 51ms/step\n",
      "step 220/782 - loss: 0.9937 - acc_top1: 0.6656 - acc_top5: 0.9722 - 51ms/step\n",
      "step 230/782 - loss: 1.2312 - acc_top1: 0.6647 - acc_top5: 0.9720 - 51ms/step\n",
      "step 240/782 - loss: 1.0422 - acc_top1: 0.6641 - acc_top5: 0.9723 - 51ms/step\n",
      "step 250/782 - loss: 0.9677 - acc_top1: 0.6648 - acc_top5: 0.9722 - 51ms/step\n",
      "step 260/782 - loss: 0.8555 - acc_top1: 0.6653 - acc_top5: 0.9724 - 51ms/step\n",
      "step 270/782 - loss: 1.0736 - acc_top1: 0.6655 - acc_top5: 0.9725 - 51ms/step\n",
      "step 280/782 - loss: 1.0077 - acc_top1: 0.6660 - acc_top5: 0.9725 - 51ms/step\n",
      "step 290/782 - loss: 1.1670 - acc_top1: 0.6651 - acc_top5: 0.9725 - 51ms/step\n",
      "step 300/782 - loss: 1.0011 - acc_top1: 0.6655 - acc_top5: 0.9726 - 51ms/step\n",
      "step 310/782 - loss: 0.9655 - acc_top1: 0.6650 - acc_top5: 0.9728 - 51ms/step\n",
      "step 320/782 - loss: 1.1643 - acc_top1: 0.6648 - acc_top5: 0.9725 - 51ms/step\n",
      "step 330/782 - loss: 1.4327 - acc_top1: 0.6648 - acc_top5: 0.9722 - 51ms/step\n",
      "step 340/782 - loss: 0.9215 - acc_top1: 0.6638 - acc_top5: 0.9720 - 51ms/step\n",
      "step 350/782 - loss: 0.8891 - acc_top1: 0.6639 - acc_top5: 0.9721 - 51ms/step\n",
      "step 360/782 - loss: 0.7783 - acc_top1: 0.6641 - acc_top5: 0.9720 - 51ms/step\n",
      "step 370/782 - loss: 0.8576 - acc_top1: 0.6641 - acc_top5: 0.9718 - 51ms/step\n",
      "step 380/782 - loss: 1.3821 - acc_top1: 0.6642 - acc_top5: 0.9714 - 51ms/step\n",
      "step 390/782 - loss: 0.9825 - acc_top1: 0.6643 - acc_top5: 0.9715 - 51ms/step\n",
      "step 400/782 - loss: 0.9615 - acc_top1: 0.6650 - acc_top5: 0.9713 - 51ms/step\n",
      "step 410/782 - loss: 0.9413 - acc_top1: 0.6655 - acc_top5: 0.9716 - 51ms/step\n",
      "step 420/782 - loss: 0.9758 - acc_top1: 0.6647 - acc_top5: 0.9716 - 51ms/step\n",
      "step 430/782 - loss: 0.8109 - acc_top1: 0.6642 - acc_top5: 0.9716 - 51ms/step\n",
      "step 440/782 - loss: 0.8200 - acc_top1: 0.6635 - acc_top5: 0.9718 - 51ms/step\n",
      "step 450/782 - loss: 0.9484 - acc_top1: 0.6630 - acc_top5: 0.9714 - 51ms/step\n",
      "step 460/782 - loss: 0.9526 - acc_top1: 0.6624 - acc_top5: 0.9714 - 51ms/step\n",
      "step 470/782 - loss: 0.7750 - acc_top1: 0.6624 - acc_top5: 0.9710 - 51ms/step\n",
      "step 480/782 - loss: 1.1160 - acc_top1: 0.6618 - acc_top5: 0.9709 - 51ms/step\n",
      "step 490/782 - loss: 0.8689 - acc_top1: 0.6619 - acc_top5: 0.9710 - 51ms/step\n",
      "step 500/782 - loss: 1.2888 - acc_top1: 0.6607 - acc_top5: 0.9709 - 51ms/step\n",
      "step 510/782 - loss: 1.5411 - acc_top1: 0.6592 - acc_top5: 0.9707 - 51ms/step\n",
      "step 520/782 - loss: 1.1164 - acc_top1: 0.6589 - acc_top5: 0.9705 - 51ms/step\n",
      "step 530/782 - loss: 1.3073 - acc_top1: 0.6581 - acc_top5: 0.9705 - 51ms/step\n",
      "step 540/782 - loss: 0.9601 - acc_top1: 0.6578 - acc_top5: 0.9704 - 51ms/step\n",
      "step 550/782 - loss: 0.8877 - acc_top1: 0.6575 - acc_top5: 0.9704 - 51ms/step\n",
      "step 560/782 - loss: 1.1380 - acc_top1: 0.6574 - acc_top5: 0.9705 - 51ms/step\n",
      "step 570/782 - loss: 0.8350 - acc_top1: 0.6573 - acc_top5: 0.9706 - 51ms/step\n",
      "step 580/782 - loss: 0.8421 - acc_top1: 0.6571 - acc_top5: 0.9707 - 51ms/step\n",
      "step 590/782 - loss: 0.9452 - acc_top1: 0.6572 - acc_top5: 0.9706 - 51ms/step\n",
      "step 600/782 - loss: 1.2008 - acc_top1: 0.6569 - acc_top5: 0.9703 - 51ms/step\n",
      "step 610/782 - loss: 1.0489 - acc_top1: 0.6564 - acc_top5: 0.9704 - 51ms/step\n",
      "step 620/782 - loss: 0.7570 - acc_top1: 0.6559 - acc_top5: 0.9704 - 51ms/step\n",
      "step 630/782 - loss: 1.0725 - acc_top1: 0.6554 - acc_top5: 0.9703 - 51ms/step\n",
      "step 640/782 - loss: 1.0482 - acc_top1: 0.6555 - acc_top5: 0.9704 - 51ms/step\n",
      "step 650/782 - loss: 1.0522 - acc_top1: 0.6557 - acc_top5: 0.9704 - 51ms/step\n",
      "step 660/782 - loss: 0.8251 - acc_top1: 0.6554 - acc_top5: 0.9704 - 51ms/step\n",
      "step 670/782 - loss: 1.1627 - acc_top1: 0.6552 - acc_top5: 0.9702 - 51ms/step\n",
      "step 680/782 - loss: 1.0482 - acc_top1: 0.6547 - acc_top5: 0.9701 - 51ms/step\n",
      "step 690/782 - loss: 1.1530 - acc_top1: 0.6550 - acc_top5: 0.9702 - 51ms/step\n",
      "step 700/782 - loss: 0.9102 - acc_top1: 0.6546 - acc_top5: 0.9704 - 51ms/step\n",
      "step 710/782 - loss: 0.7200 - acc_top1: 0.6546 - acc_top5: 0.9703 - 51ms/step\n",
      "step 720/782 - loss: 0.8327 - acc_top1: 0.6544 - acc_top5: 0.9702 - 51ms/step\n",
      "step 730/782 - loss: 0.9653 - acc_top1: 0.6548 - acc_top5: 0.9703 - 51ms/step\n",
      "step 740/782 - loss: 0.9088 - acc_top1: 0.6549 - acc_top5: 0.9702 - 51ms/step\n",
      "step 750/782 - loss: 1.3887 - acc_top1: 0.6545 - acc_top5: 0.9701 - 51ms/step\n",
      "step 760/782 - loss: 1.1140 - acc_top1: 0.6544 - acc_top5: 0.9702 - 51ms/step\n",
      "step 770/782 - loss: 1.1541 - acc_top1: 0.6541 - acc_top5: 0.9700 - 51ms/step\n",
      "step 780/782 - loss: 0.9509 - acc_top1: 0.6541 - acc_top5: 0.9700 - 51ms/step\n",
      "step 782/782 - loss: 1.9278 - acc_top1: 0.6540 - acc_top5: 0.9700 - 51ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\16\n",
      "Eval begin...\n",
      "step  10/157 - loss: 0.8189 - acc_top1: 0.6078 - acc_top5: 0.9641 - 19ms/step\n",
      "step  20/157 - loss: 1.3944 - acc_top1: 0.6086 - acc_top5: 0.9625 - 19ms/step\n",
      "step  30/157 - loss: 1.2661 - acc_top1: 0.6047 - acc_top5: 0.9589 - 18ms/step\n",
      "step  40/157 - loss: 1.6574 - acc_top1: 0.5992 - acc_top5: 0.9551 - 18ms/step\n",
      "step  50/157 - loss: 1.0552 - acc_top1: 0.5994 - acc_top5: 0.9553 - 19ms/step\n",
      "step  60/157 - loss: 1.0454 - acc_top1: 0.6003 - acc_top5: 0.9513 - 18ms/step\n",
      "step  70/157 - loss: 3.2378 - acc_top1: 0.5991 - acc_top5: 0.9502 - 18ms/step\n",
      "step  80/157 - loss: 1.1950 - acc_top1: 0.6020 - acc_top5: 0.9508 - 18ms/step\n",
      "step  90/157 - loss: 0.8544 - acc_top1: 0.6040 - acc_top5: 0.9533 - 18ms/step\n",
      "step 100/157 - loss: 1.0644 - acc_top1: 0.6031 - acc_top5: 0.9537 - 18ms/step\n",
      "step 110/157 - loss: 1.4990 - acc_top1: 0.6004 - acc_top5: 0.9534 - 18ms/step\n",
      "step 120/157 - loss: 1.1551 - acc_top1: 0.6000 - acc_top5: 0.9534 - 18ms/step\n",
      "step 130/157 - loss: 1.3080 - acc_top1: 0.5995 - acc_top5: 0.9530 - 18ms/step\n",
      "step 140/157 - loss: 1.1511 - acc_top1: 0.5991 - acc_top5: 0.9523 - 18ms/step\n",
      "step 150/157 - loss: 2.8772 - acc_top1: 0.6000 - acc_top5: 0.9522 - 18ms/step\n",
      "step 157/157 - loss: 0.6735 - acc_top1: 0.5996 - acc_top5: 0.9524 - 18ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.3252 - acc_top1: 0.6438 - acc_top5: 0.9641 - 51ms/step\n",
      "step  20/782 - loss: 1.2880 - acc_top1: 0.6617 - acc_top5: 0.9688 - 51ms/step\n",
      "step  30/782 - loss: 1.1644 - acc_top1: 0.6500 - acc_top5: 0.9677 - 51ms/step\n",
      "step  40/782 - loss: 0.9572 - acc_top1: 0.6398 - acc_top5: 0.9633 - 50ms/step\n",
      "step  50/782 - loss: 0.9063 - acc_top1: 0.6328 - acc_top5: 0.9647 - 51ms/step\n",
      "step  60/782 - loss: 0.9689 - acc_top1: 0.6294 - acc_top5: 0.9617 - 51ms/step\n",
      "step  70/782 - loss: 1.0136 - acc_top1: 0.6268 - acc_top5: 0.9616 - 50ms/step\n",
      "step  80/782 - loss: 0.9436 - acc_top1: 0.6307 - acc_top5: 0.9627 - 50ms/step\n",
      "step  90/782 - loss: 1.2593 - acc_top1: 0.6312 - acc_top5: 0.9630 - 50ms/step\n",
      "step 100/782 - loss: 1.0930 - acc_top1: 0.6283 - acc_top5: 0.9622 - 51ms/step\n",
      "step 110/782 - loss: 1.2550 - acc_top1: 0.6240 - acc_top5: 0.9624 - 51ms/step\n",
      "step 120/782 - loss: 1.0504 - acc_top1: 0.6242 - acc_top5: 0.9641 - 51ms/step\n",
      "step 130/782 - loss: 1.1267 - acc_top1: 0.6252 - acc_top5: 0.9649 - 51ms/step\n",
      "step 140/782 - loss: 1.1423 - acc_top1: 0.6246 - acc_top5: 0.9652 - 51ms/step\n",
      "step 150/782 - loss: 1.0167 - acc_top1: 0.6236 - acc_top5: 0.9643 - 51ms/step\n",
      "step 160/782 - loss: 1.1092 - acc_top1: 0.6256 - acc_top5: 0.9642 - 51ms/step\n",
      "step 170/782 - loss: 1.0811 - acc_top1: 0.6258 - acc_top5: 0.9645 - 50ms/step\n",
      "step 180/782 - loss: 1.1577 - acc_top1: 0.6271 - acc_top5: 0.9652 - 51ms/step\n",
      "step 190/782 - loss: 1.0424 - acc_top1: 0.6277 - acc_top5: 0.9655 - 50ms/step\n",
      "step 200/782 - loss: 1.2143 - acc_top1: 0.6279 - acc_top5: 0.9658 - 50ms/step\n",
      "step 210/782 - loss: 0.9763 - acc_top1: 0.6301 - acc_top5: 0.9662 - 50ms/step\n",
      "step 220/782 - loss: 0.9869 - acc_top1: 0.6292 - acc_top5: 0.9663 - 50ms/step\n",
      "step 230/782 - loss: 0.9686 - acc_top1: 0.6298 - acc_top5: 0.9668 - 50ms/step\n",
      "step 240/782 - loss: 0.9961 - acc_top1: 0.6292 - acc_top5: 0.9665 - 51ms/step\n",
      "step 250/782 - loss: 1.1141 - acc_top1: 0.6299 - acc_top5: 0.9669 - 51ms/step\n",
      "step 260/782 - loss: 0.9063 - acc_top1: 0.6316 - acc_top5: 0.9668 - 51ms/step\n",
      "step 270/782 - loss: 1.1614 - acc_top1: 0.6315 - acc_top5: 0.9672 - 51ms/step\n",
      "step 280/782 - loss: 1.0427 - acc_top1: 0.6333 - acc_top5: 0.9672 - 51ms/step\n",
      "step 290/782 - loss: 1.0027 - acc_top1: 0.6351 - acc_top5: 0.9673 - 51ms/step\n",
      "step 300/782 - loss: 0.8259 - acc_top1: 0.6357 - acc_top5: 0.9676 - 51ms/step\n",
      "step 310/782 - loss: 1.0255 - acc_top1: 0.6363 - acc_top5: 0.9678 - 51ms/step\n",
      "step 320/782 - loss: 1.0589 - acc_top1: 0.6362 - acc_top5: 0.9675 - 51ms/step\n",
      "step 330/782 - loss: 0.9937 - acc_top1: 0.6367 - acc_top5: 0.9676 - 51ms/step\n",
      "step 340/782 - loss: 1.0484 - acc_top1: 0.6370 - acc_top5: 0.9681 - 51ms/step\n",
      "step 350/782 - loss: 1.0646 - acc_top1: 0.6378 - acc_top5: 0.9681 - 51ms/step\n",
      "step 360/782 - loss: 0.9586 - acc_top1: 0.6392 - acc_top5: 0.9682 - 51ms/step\n",
      "step 370/782 - loss: 0.9572 - acc_top1: 0.6396 - acc_top5: 0.9684 - 51ms/step\n",
      "step 380/782 - loss: 1.1285 - acc_top1: 0.6397 - acc_top5: 0.9685 - 51ms/step\n",
      "step 390/782 - loss: 1.1728 - acc_top1: 0.6406 - acc_top5: 0.9685 - 51ms/step\n",
      "step 400/782 - loss: 0.9792 - acc_top1: 0.6416 - acc_top5: 0.9687 - 51ms/step\n",
      "step 410/782 - loss: 0.8728 - acc_top1: 0.6423 - acc_top5: 0.9691 - 51ms/step\n",
      "step 420/782 - loss: 0.9302 - acc_top1: 0.6421 - acc_top5: 0.9690 - 51ms/step\n",
      "step 430/782 - loss: 1.1790 - acc_top1: 0.6420 - acc_top5: 0.9691 - 51ms/step\n",
      "step 440/782 - loss: 1.1201 - acc_top1: 0.6422 - acc_top5: 0.9692 - 51ms/step\n",
      "step 450/782 - loss: 0.8281 - acc_top1: 0.6432 - acc_top5: 0.9693 - 51ms/step\n",
      "step 460/782 - loss: 0.9981 - acc_top1: 0.6434 - acc_top5: 0.9695 - 51ms/step\n",
      "step 470/782 - loss: 0.8029 - acc_top1: 0.6435 - acc_top5: 0.9695 - 51ms/step\n",
      "step 480/782 - loss: 0.7549 - acc_top1: 0.6439 - acc_top5: 0.9698 - 51ms/step\n",
      "step 490/782 - loss: 0.7819 - acc_top1: 0.6446 - acc_top5: 0.9697 - 51ms/step\n",
      "step 500/782 - loss: 0.9394 - acc_top1: 0.6447 - acc_top5: 0.9697 - 51ms/step\n",
      "step 510/782 - loss: 1.0926 - acc_top1: 0.6452 - acc_top5: 0.9698 - 51ms/step\n",
      "step 520/782 - loss: 1.1424 - acc_top1: 0.6451 - acc_top5: 0.9698 - 51ms/step\n",
      "step 530/782 - loss: 0.9495 - acc_top1: 0.6451 - acc_top5: 0.9697 - 51ms/step\n",
      "step 540/782 - loss: 0.8766 - acc_top1: 0.6455 - acc_top5: 0.9699 - 51ms/step\n",
      "step 550/782 - loss: 0.8614 - acc_top1: 0.6455 - acc_top5: 0.9700 - 51ms/step\n",
      "step 560/782 - loss: 1.0309 - acc_top1: 0.6455 - acc_top5: 0.9700 - 51ms/step\n",
      "step 570/782 - loss: 1.1987 - acc_top1: 0.6452 - acc_top5: 0.9701 - 51ms/step\n",
      "step 580/782 - loss: 0.8120 - acc_top1: 0.6452 - acc_top5: 0.9700 - 51ms/step\n",
      "step 590/782 - loss: 0.9921 - acc_top1: 0.6451 - acc_top5: 0.9701 - 51ms/step\n",
      "step 600/782 - loss: 1.1789 - acc_top1: 0.6446 - acc_top5: 0.9700 - 50ms/step\n",
      "step 610/782 - loss: 1.1923 - acc_top1: 0.6446 - acc_top5: 0.9701 - 50ms/step\n",
      "step 620/782 - loss: 0.9441 - acc_top1: 0.6448 - acc_top5: 0.9701 - 50ms/step\n",
      "step 630/782 - loss: 1.0671 - acc_top1: 0.6445 - acc_top5: 0.9701 - 50ms/step\n",
      "step 640/782 - loss: 1.1746 - acc_top1: 0.6444 - acc_top5: 0.9699 - 50ms/step\n",
      "step 650/782 - loss: 1.0094 - acc_top1: 0.6438 - acc_top5: 0.9699 - 51ms/step\n",
      "step 660/782 - loss: 0.8298 - acc_top1: 0.6442 - acc_top5: 0.9698 - 51ms/step\n",
      "step 670/782 - loss: 1.1133 - acc_top1: 0.6442 - acc_top5: 0.9698 - 51ms/step\n",
      "step 680/782 - loss: 0.7098 - acc_top1: 0.6442 - acc_top5: 0.9698 - 50ms/step\n",
      "step 690/782 - loss: 0.8374 - acc_top1: 0.6443 - acc_top5: 0.9698 - 50ms/step\n",
      "step 700/782 - loss: 0.9735 - acc_top1: 0.6448 - acc_top5: 0.9700 - 50ms/step\n",
      "step 710/782 - loss: 1.4791 - acc_top1: 0.6444 - acc_top5: 0.9700 - 50ms/step\n",
      "step 720/782 - loss: 0.9744 - acc_top1: 0.6441 - acc_top5: 0.9700 - 50ms/step\n",
      "step 730/782 - loss: 1.0812 - acc_top1: 0.6442 - acc_top5: 0.9700 - 50ms/step\n",
      "step 740/782 - loss: 0.9330 - acc_top1: 0.6441 - acc_top5: 0.9702 - 50ms/step\n",
      "step 750/782 - loss: 0.9121 - acc_top1: 0.6440 - acc_top5: 0.9702 - 50ms/step\n",
      "step 760/782 - loss: 1.1028 - acc_top1: 0.6444 - acc_top5: 0.9703 - 50ms/step\n",
      "step 770/782 - loss: 0.7529 - acc_top1: 0.6444 - acc_top5: 0.9703 - 50ms/step\n",
      "step 780/782 - loss: 1.0233 - acc_top1: 0.6448 - acc_top5: 0.9704 - 50ms/step\n",
      "step 782/782 - loss: 1.3874 - acc_top1: 0.6448 - acc_top5: 0.9704 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\17\n",
      "Eval begin...\n",
      "step  10/157 - loss: 0.8728 - acc_top1: 0.6312 - acc_top5: 0.9641 - 20ms/step\n",
      "step  20/157 - loss: 1.3578 - acc_top1: 0.6234 - acc_top5: 0.9617 - 19ms/step\n",
      "step  30/157 - loss: 1.2927 - acc_top1: 0.6182 - acc_top5: 0.9563 - 19ms/step\n",
      "step  40/157 - loss: 1.6356 - acc_top1: 0.6098 - acc_top5: 0.9570 - 19ms/step\n",
      "step  50/157 - loss: 1.2533 - acc_top1: 0.6081 - acc_top5: 0.9566 - 19ms/step\n",
      "step  60/157 - loss: 2.8060 - acc_top1: 0.6065 - acc_top5: 0.9521 - 19ms/step\n",
      "step  70/157 - loss: 4.6828 - acc_top1: 0.6051 - acc_top5: 0.9500 - 19ms/step\n",
      "step  80/157 - loss: 1.2931 - acc_top1: 0.6078 - acc_top5: 0.9504 - 19ms/step\n",
      "step  90/157 - loss: 0.8643 - acc_top1: 0.6076 - acc_top5: 0.9521 - 19ms/step\n",
      "step 100/157 - loss: 1.1201 - acc_top1: 0.6045 - acc_top5: 0.9511 - 19ms/step\n",
      "step 110/157 - loss: 3.3154 - acc_top1: 0.6043 - acc_top5: 0.9509 - 19ms/step\n",
      "step 120/157 - loss: 2.2191 - acc_top1: 0.6023 - acc_top5: 0.9509 - 19ms/step\n",
      "step 130/157 - loss: 1.4164 - acc_top1: 0.6019 - acc_top5: 0.9508 - 19ms/step\n",
      "step 140/157 - loss: 2.4072 - acc_top1: 0.6017 - acc_top5: 0.9509 - 19ms/step\n",
      "step 150/157 - loss: 4.2440 - acc_top1: 0.6003 - acc_top5: 0.9516 - 19ms/step\n",
      "step 157/157 - loss: 0.6721 - acc_top1: 0.5984 - acc_top5: 0.9518 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.0191 - acc_top1: 0.6891 - acc_top5: 0.9781 - 51ms/step\n",
      "step  20/782 - loss: 0.7533 - acc_top1: 0.6805 - acc_top5: 0.9797 - 53ms/step\n",
      "step  30/782 - loss: 0.7436 - acc_top1: 0.6932 - acc_top5: 0.9818 - 53ms/step\n",
      "step  40/782 - loss: 0.8441 - acc_top1: 0.6883 - acc_top5: 0.9809 - 54ms/step\n",
      "step  50/782 - loss: 0.8411 - acc_top1: 0.6863 - acc_top5: 0.9788 - 53ms/step\n",
      "step  60/782 - loss: 0.7571 - acc_top1: 0.6875 - acc_top5: 0.9786 - 53ms/step\n",
      "step  70/782 - loss: 0.7294 - acc_top1: 0.6906 - acc_top5: 0.9783 - 52ms/step\n",
      "step  80/782 - loss: 0.7800 - acc_top1: 0.6912 - acc_top5: 0.9795 - 52ms/step\n",
      "step  90/782 - loss: 1.0636 - acc_top1: 0.6951 - acc_top5: 0.9800 - 52ms/step\n",
      "step 100/782 - loss: 0.6376 - acc_top1: 0.6983 - acc_top5: 0.9808 - 52ms/step\n",
      "step 110/782 - loss: 1.2634 - acc_top1: 0.6974 - acc_top5: 0.9803 - 52ms/step\n",
      "step 120/782 - loss: 0.5953 - acc_top1: 0.6996 - acc_top5: 0.9807 - 51ms/step\n",
      "step 130/782 - loss: 0.7823 - acc_top1: 0.7006 - acc_top5: 0.9809 - 51ms/step\n",
      "step 140/782 - loss: 0.6368 - acc_top1: 0.7020 - acc_top5: 0.9809 - 51ms/step\n",
      "step 150/782 - loss: 0.6501 - acc_top1: 0.7008 - acc_top5: 0.9807 - 51ms/step\n",
      "step 160/782 - loss: 1.1264 - acc_top1: 0.6987 - acc_top5: 0.9810 - 51ms/step\n",
      "step 170/782 - loss: 1.3149 - acc_top1: 0.6983 - acc_top5: 0.9801 - 51ms/step\n",
      "step 180/782 - loss: 0.6486 - acc_top1: 0.6973 - acc_top5: 0.9801 - 51ms/step\n",
      "step 190/782 - loss: 0.7751 - acc_top1: 0.6957 - acc_top5: 0.9801 - 51ms/step\n",
      "step 200/782 - loss: 0.9952 - acc_top1: 0.6948 - acc_top5: 0.9803 - 51ms/step\n",
      "step 210/782 - loss: 1.0134 - acc_top1: 0.6939 - acc_top5: 0.9804 - 51ms/step\n",
      "step 220/782 - loss: 1.2935 - acc_top1: 0.6926 - acc_top5: 0.9803 - 51ms/step\n",
      "step 230/782 - loss: 1.0137 - acc_top1: 0.6925 - acc_top5: 0.9800 - 51ms/step\n",
      "step 240/782 - loss: 0.7809 - acc_top1: 0.6920 - acc_top5: 0.9797 - 51ms/step\n",
      "step 250/782 - loss: 1.3553 - acc_top1: 0.6919 - acc_top5: 0.9795 - 51ms/step\n",
      "step 260/782 - loss: 0.8532 - acc_top1: 0.6903 - acc_top5: 0.9796 - 51ms/step\n",
      "step 270/782 - loss: 0.8751 - acc_top1: 0.6893 - acc_top5: 0.9795 - 51ms/step\n",
      "step 280/782 - loss: 1.1313 - acc_top1: 0.6890 - acc_top5: 0.9795 - 51ms/step\n",
      "step 290/782 - loss: 0.7038 - acc_top1: 0.6898 - acc_top5: 0.9796 - 51ms/step\n",
      "step 300/782 - loss: 1.1576 - acc_top1: 0.6893 - acc_top5: 0.9793 - 51ms/step\n",
      "step 310/782 - loss: 0.8681 - acc_top1: 0.6897 - acc_top5: 0.9791 - 51ms/step\n",
      "step 320/782 - loss: 0.8964 - acc_top1: 0.6896 - acc_top5: 0.9792 - 51ms/step\n",
      "step 330/782 - loss: 0.8581 - acc_top1: 0.6902 - acc_top5: 0.9793 - 51ms/step\n",
      "step 340/782 - loss: 0.7462 - acc_top1: 0.6905 - acc_top5: 0.9792 - 51ms/step\n",
      "step 350/782 - loss: 0.9767 - acc_top1: 0.6900 - acc_top5: 0.9791 - 50ms/step\n",
      "step 360/782 - loss: 0.9225 - acc_top1: 0.6896 - acc_top5: 0.9793 - 50ms/step\n",
      "step 370/782 - loss: 0.7598 - acc_top1: 0.6888 - acc_top5: 0.9791 - 50ms/step\n",
      "step 380/782 - loss: 0.7238 - acc_top1: 0.6900 - acc_top5: 0.9792 - 51ms/step\n",
      "step 390/782 - loss: 0.9203 - acc_top1: 0.6896 - acc_top5: 0.9793 - 51ms/step\n",
      "step 400/782 - loss: 1.0277 - acc_top1: 0.6893 - acc_top5: 0.9795 - 51ms/step\n",
      "step 410/782 - loss: 0.7305 - acc_top1: 0.6892 - acc_top5: 0.9793 - 51ms/step\n",
      "step 420/782 - loss: 0.7357 - acc_top1: 0.6890 - acc_top5: 0.9795 - 51ms/step\n",
      "step 430/782 - loss: 1.0302 - acc_top1: 0.6886 - acc_top5: 0.9793 - 51ms/step\n",
      "step 440/782 - loss: 0.9860 - acc_top1: 0.6876 - acc_top5: 0.9790 - 51ms/step\n",
      "step 450/782 - loss: 1.0345 - acc_top1: 0.6878 - acc_top5: 0.9790 - 51ms/step\n",
      "step 460/782 - loss: 0.9152 - acc_top1: 0.6874 - acc_top5: 0.9790 - 51ms/step\n",
      "step 470/782 - loss: 1.0869 - acc_top1: 0.6871 - acc_top5: 0.9787 - 51ms/step\n",
      "step 480/782 - loss: 0.8055 - acc_top1: 0.6863 - acc_top5: 0.9785 - 51ms/step\n",
      "step 490/782 - loss: 1.0754 - acc_top1: 0.6860 - acc_top5: 0.9783 - 51ms/step\n",
      "step 500/782 - loss: 0.7901 - acc_top1: 0.6858 - acc_top5: 0.9782 - 51ms/step\n",
      "step 510/782 - loss: 0.9933 - acc_top1: 0.6851 - acc_top5: 0.9782 - 51ms/step\n",
      "step 520/782 - loss: 0.7998 - acc_top1: 0.6853 - acc_top5: 0.9782 - 51ms/step\n",
      "step 530/782 - loss: 1.0501 - acc_top1: 0.6847 - acc_top5: 0.9781 - 51ms/step\n",
      "step 540/782 - loss: 1.0175 - acc_top1: 0.6843 - acc_top5: 0.9780 - 51ms/step\n",
      "step 550/782 - loss: 0.9111 - acc_top1: 0.6843 - acc_top5: 0.9779 - 51ms/step\n",
      "step 560/782 - loss: 0.9171 - acc_top1: 0.6849 - acc_top5: 0.9780 - 51ms/step\n",
      "step 570/782 - loss: 0.7531 - acc_top1: 0.6851 - acc_top5: 0.9782 - 51ms/step\n",
      "step 580/782 - loss: 0.9430 - acc_top1: 0.6855 - acc_top5: 0.9782 - 51ms/step\n",
      "step 590/782 - loss: 0.8841 - acc_top1: 0.6857 - acc_top5: 0.9783 - 51ms/step\n",
      "step 600/782 - loss: 0.8274 - acc_top1: 0.6851 - acc_top5: 0.9783 - 51ms/step\n",
      "step 610/782 - loss: 0.7651 - acc_top1: 0.6846 - acc_top5: 0.9782 - 51ms/step\n",
      "step 620/782 - loss: 0.8884 - acc_top1: 0.6847 - acc_top5: 0.9782 - 52ms/step\n",
      "step 630/782 - loss: 0.7882 - acc_top1: 0.6848 - acc_top5: 0.9782 - 52ms/step\n",
      "step 640/782 - loss: 1.1261 - acc_top1: 0.6847 - acc_top5: 0.9781 - 52ms/step\n",
      "step 650/782 - loss: 1.1291 - acc_top1: 0.6846 - acc_top5: 0.9781 - 52ms/step\n",
      "step 660/782 - loss: 0.8438 - acc_top1: 0.6849 - acc_top5: 0.9783 - 52ms/step\n",
      "step 670/782 - loss: 1.1369 - acc_top1: 0.6847 - acc_top5: 0.9783 - 52ms/step\n",
      "step 680/782 - loss: 0.9345 - acc_top1: 0.6844 - acc_top5: 0.9783 - 52ms/step\n",
      "step 690/782 - loss: 1.0374 - acc_top1: 0.6845 - acc_top5: 0.9783 - 52ms/step\n",
      "step 700/782 - loss: 0.8673 - acc_top1: 0.6845 - acc_top5: 0.9783 - 52ms/step\n",
      "step 710/782 - loss: 0.8636 - acc_top1: 0.6847 - acc_top5: 0.9781 - 52ms/step\n",
      "step 720/782 - loss: 0.9068 - acc_top1: 0.6849 - acc_top5: 0.9781 - 52ms/step\n",
      "step 730/782 - loss: 0.8316 - acc_top1: 0.6841 - acc_top5: 0.9781 - 52ms/step\n",
      "step 740/782 - loss: 0.9428 - acc_top1: 0.6842 - acc_top5: 0.9781 - 52ms/step\n",
      "step 750/782 - loss: 1.1437 - acc_top1: 0.6840 - acc_top5: 0.9782 - 52ms/step\n",
      "step 760/782 - loss: 1.0663 - acc_top1: 0.6842 - acc_top5: 0.9781 - 52ms/step\n",
      "step 770/782 - loss: 0.9233 - acc_top1: 0.6839 - acc_top5: 0.9780 - 52ms/step\n",
      "step 780/782 - loss: 0.9265 - acc_top1: 0.6837 - acc_top5: 0.9778 - 52ms/step\n",
      "step 782/782 - loss: 1.8127 - acc_top1: 0.6836 - acc_top5: 0.9778 - 52ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\18\n",
      "Eval begin...\n",
      "step  10/157 - loss: 0.9611 - acc_top1: 0.6000 - acc_top5: 0.9531 - 22ms/step\n",
      "step  20/157 - loss: 1.4220 - acc_top1: 0.6070 - acc_top5: 0.9578 - 22ms/step\n",
      "step  30/157 - loss: 1.5631 - acc_top1: 0.6021 - acc_top5: 0.9542 - 21ms/step\n",
      "step  40/157 - loss: 1.7257 - acc_top1: 0.6008 - acc_top5: 0.9520 - 21ms/step\n",
      "step  50/157 - loss: 1.1391 - acc_top1: 0.6009 - acc_top5: 0.9531 - 21ms/step\n",
      "step  60/157 - loss: 1.0775 - acc_top1: 0.6018 - acc_top5: 0.9523 - 21ms/step\n",
      "step  70/157 - loss: 4.1064 - acc_top1: 0.6025 - acc_top5: 0.9509 - 21ms/step\n",
      "step  80/157 - loss: 1.5780 - acc_top1: 0.6043 - acc_top5: 0.9500 - 21ms/step\n",
      "step  90/157 - loss: 0.9623 - acc_top1: 0.6010 - acc_top5: 0.9509 - 21ms/step\n",
      "step 100/157 - loss: 1.2264 - acc_top1: 0.6006 - acc_top5: 0.9506 - 21ms/step\n",
      "step 110/157 - loss: 1.4723 - acc_top1: 0.5982 - acc_top5: 0.9513 - 20ms/step\n",
      "step 120/157 - loss: 1.1570 - acc_top1: 0.5967 - acc_top5: 0.9512 - 20ms/step\n",
      "step 130/157 - loss: 1.3644 - acc_top1: 0.5974 - acc_top5: 0.9513 - 20ms/step\n",
      "step 140/157 - loss: 1.1733 - acc_top1: 0.5979 - acc_top5: 0.9511 - 20ms/step\n",
      "step 150/157 - loss: 1.8348 - acc_top1: 0.5981 - acc_top5: 0.9510 - 20ms/step\n",
      "step 157/157 - loss: 0.5863 - acc_top1: 0.5972 - acc_top5: 0.9510 - 20ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.8686 - acc_top1: 0.7000 - acc_top5: 0.9812 - 54ms/step\n",
      "step  20/782 - loss: 0.6917 - acc_top1: 0.7164 - acc_top5: 0.9797 - 53ms/step\n",
      "step  30/782 - loss: 0.8361 - acc_top1: 0.7188 - acc_top5: 0.9792 - 52ms/step\n",
      "step  40/782 - loss: 0.9542 - acc_top1: 0.7180 - acc_top5: 0.9797 - 52ms/step\n",
      "step  50/782 - loss: 0.7174 - acc_top1: 0.7225 - acc_top5: 0.9828 - 52ms/step\n",
      "step  60/782 - loss: 0.7812 - acc_top1: 0.7224 - acc_top5: 0.9828 - 52ms/step\n",
      "step  70/782 - loss: 0.9028 - acc_top1: 0.7205 - acc_top5: 0.9837 - 52ms/step\n",
      "step  80/782 - loss: 0.8856 - acc_top1: 0.7213 - acc_top5: 0.9844 - 52ms/step\n",
      "step  90/782 - loss: 0.6382 - acc_top1: 0.7208 - acc_top5: 0.9845 - 52ms/step\n",
      "step 100/782 - loss: 0.6057 - acc_top1: 0.7208 - acc_top5: 0.9836 - 52ms/step\n",
      "step 110/782 - loss: 0.6652 - acc_top1: 0.7202 - acc_top5: 0.9835 - 52ms/step\n",
      "step 120/782 - loss: 0.7504 - acc_top1: 0.7195 - acc_top5: 0.9835 - 52ms/step\n",
      "step 130/782 - loss: 0.7826 - acc_top1: 0.7210 - acc_top5: 0.9827 - 52ms/step\n",
      "step 140/782 - loss: 1.1500 - acc_top1: 0.7201 - acc_top5: 0.9825 - 52ms/step\n",
      "step 150/782 - loss: 0.8858 - acc_top1: 0.7198 - acc_top5: 0.9825 - 52ms/step\n",
      "step 160/782 - loss: 0.7631 - acc_top1: 0.7209 - acc_top5: 0.9825 - 52ms/step\n",
      "step 170/782 - loss: 0.7232 - acc_top1: 0.7203 - acc_top5: 0.9823 - 52ms/step\n",
      "step 180/782 - loss: 0.8098 - acc_top1: 0.7179 - acc_top5: 0.9819 - 52ms/step\n",
      "step 190/782 - loss: 0.8955 - acc_top1: 0.7164 - acc_top5: 0.9821 - 52ms/step\n",
      "step 200/782 - loss: 0.9410 - acc_top1: 0.7167 - acc_top5: 0.9820 - 52ms/step\n",
      "step 210/782 - loss: 0.7904 - acc_top1: 0.7158 - acc_top5: 0.9824 - 52ms/step\n",
      "step 220/782 - loss: 0.6223 - acc_top1: 0.7158 - acc_top5: 0.9822 - 52ms/step\n",
      "step 230/782 - loss: 0.6387 - acc_top1: 0.7154 - acc_top5: 0.9821 - 52ms/step\n",
      "step 240/782 - loss: 0.6845 - acc_top1: 0.7158 - acc_top5: 0.9815 - 52ms/step\n",
      "step 250/782 - loss: 0.8114 - acc_top1: 0.7147 - acc_top5: 0.9817 - 52ms/step\n",
      "step 260/782 - loss: 0.7709 - acc_top1: 0.7157 - acc_top5: 0.9817 - 52ms/step\n",
      "step 270/782 - loss: 0.6934 - acc_top1: 0.7157 - acc_top5: 0.9820 - 52ms/step\n",
      "step 280/782 - loss: 1.0316 - acc_top1: 0.7167 - acc_top5: 0.9820 - 52ms/step\n",
      "step 290/782 - loss: 0.6841 - acc_top1: 0.7153 - acc_top5: 0.9819 - 52ms/step\n",
      "step 300/782 - loss: 0.8311 - acc_top1: 0.7155 - acc_top5: 0.9818 - 52ms/step\n",
      "step 310/782 - loss: 0.8967 - acc_top1: 0.7164 - acc_top5: 0.9821 - 52ms/step\n",
      "step 320/782 - loss: 0.9898 - acc_top1: 0.7158 - acc_top5: 0.9821 - 52ms/step\n",
      "step 330/782 - loss: 0.8577 - acc_top1: 0.7152 - acc_top5: 0.9820 - 51ms/step\n",
      "step 340/782 - loss: 0.7519 - acc_top1: 0.7143 - acc_top5: 0.9822 - 51ms/step\n",
      "step 350/782 - loss: 0.9580 - acc_top1: 0.7152 - acc_top5: 0.9822 - 51ms/step\n",
      "step 360/782 - loss: 1.0256 - acc_top1: 0.7149 - acc_top5: 0.9822 - 51ms/step\n",
      "step 370/782 - loss: 0.9106 - acc_top1: 0.7147 - acc_top5: 0.9820 - 51ms/step\n",
      "step 380/782 - loss: 0.7903 - acc_top1: 0.7146 - acc_top5: 0.9819 - 51ms/step\n",
      "step 390/782 - loss: 1.0210 - acc_top1: 0.7140 - acc_top5: 0.9821 - 51ms/step\n",
      "step 400/782 - loss: 0.6522 - acc_top1: 0.7138 - acc_top5: 0.9819 - 51ms/step\n",
      "step 410/782 - loss: 0.8793 - acc_top1: 0.7133 - acc_top5: 0.9818 - 51ms/step\n",
      "step 420/782 - loss: 0.8432 - acc_top1: 0.7135 - acc_top5: 0.9814 - 51ms/step\n",
      "step 430/782 - loss: 0.5768 - acc_top1: 0.7139 - acc_top5: 0.9814 - 51ms/step\n",
      "step 440/782 - loss: 0.6711 - acc_top1: 0.7136 - acc_top5: 0.9814 - 52ms/step\n",
      "step 450/782 - loss: 0.6542 - acc_top1: 0.7146 - acc_top5: 0.9814 - 52ms/step\n",
      "step 460/782 - loss: 0.8155 - acc_top1: 0.7139 - acc_top5: 0.9812 - 52ms/step\n",
      "step 470/782 - loss: 0.8771 - acc_top1: 0.7138 - acc_top5: 0.9812 - 52ms/step\n",
      "step 480/782 - loss: 1.0991 - acc_top1: 0.7130 - acc_top5: 0.9811 - 52ms/step\n",
      "step 490/782 - loss: 0.8467 - acc_top1: 0.7131 - acc_top5: 0.9811 - 52ms/step\n",
      "step 500/782 - loss: 0.9878 - acc_top1: 0.7125 - acc_top5: 0.9808 - 52ms/step\n",
      "step 510/782 - loss: 0.9109 - acc_top1: 0.7127 - acc_top5: 0.9810 - 52ms/step\n",
      "step 520/782 - loss: 0.7650 - acc_top1: 0.7123 - acc_top5: 0.9810 - 52ms/step\n",
      "step 530/782 - loss: 0.5629 - acc_top1: 0.7127 - acc_top5: 0.9810 - 52ms/step\n",
      "step 540/782 - loss: 0.7112 - acc_top1: 0.7127 - acc_top5: 0.9810 - 52ms/step\n",
      "step 550/782 - loss: 0.6335 - acc_top1: 0.7130 - acc_top5: 0.9812 - 52ms/step\n",
      "step 560/782 - loss: 0.7696 - acc_top1: 0.7133 - acc_top5: 0.9813 - 52ms/step\n",
      "step 570/782 - loss: 0.8830 - acc_top1: 0.7130 - acc_top5: 0.9812 - 52ms/step\n",
      "step 580/782 - loss: 0.6838 - acc_top1: 0.7133 - acc_top5: 0.9812 - 52ms/step\n",
      "step 590/782 - loss: 1.1389 - acc_top1: 0.7133 - acc_top5: 0.9811 - 52ms/step\n",
      "step 600/782 - loss: 0.7510 - acc_top1: 0.7132 - acc_top5: 0.9811 - 52ms/step\n",
      "step 610/782 - loss: 0.7465 - acc_top1: 0.7130 - acc_top5: 0.9810 - 52ms/step\n",
      "step 620/782 - loss: 0.9332 - acc_top1: 0.7125 - acc_top5: 0.9808 - 53ms/step\n",
      "step 630/782 - loss: 0.8940 - acc_top1: 0.7122 - acc_top5: 0.9807 - 53ms/step\n",
      "step 640/782 - loss: 0.7040 - acc_top1: 0.7117 - acc_top5: 0.9806 - 53ms/step\n",
      "step 650/782 - loss: 0.7798 - acc_top1: 0.7116 - acc_top5: 0.9807 - 53ms/step\n",
      "step 660/782 - loss: 1.0269 - acc_top1: 0.7107 - acc_top5: 0.9806 - 53ms/step\n",
      "step 670/782 - loss: 1.1175 - acc_top1: 0.7108 - acc_top5: 0.9806 - 53ms/step\n",
      "step 680/782 - loss: 0.8598 - acc_top1: 0.7104 - acc_top5: 0.9805 - 53ms/step\n",
      "step 690/782 - loss: 0.8181 - acc_top1: 0.7104 - acc_top5: 0.9806 - 53ms/step\n",
      "step 700/782 - loss: 0.8945 - acc_top1: 0.7102 - acc_top5: 0.9804 - 53ms/step\n",
      "step 710/782 - loss: 0.6937 - acc_top1: 0.7101 - acc_top5: 0.9805 - 53ms/step\n",
      "step 720/782 - loss: 0.8472 - acc_top1: 0.7101 - acc_top5: 0.9806 - 53ms/step\n",
      "step 730/782 - loss: 0.8531 - acc_top1: 0.7103 - acc_top5: 0.9805 - 53ms/step\n",
      "step 740/782 - loss: 0.5687 - acc_top1: 0.7108 - acc_top5: 0.9806 - 53ms/step\n",
      "step 750/782 - loss: 0.9674 - acc_top1: 0.7105 - acc_top5: 0.9805 - 53ms/step\n",
      "step 760/782 - loss: 0.8426 - acc_top1: 0.7101 - acc_top5: 0.9805 - 53ms/step\n",
      "step 770/782 - loss: 0.7077 - acc_top1: 0.7096 - acc_top5: 0.9805 - 53ms/step\n",
      "step 780/782 - loss: 0.9161 - acc_top1: 0.7099 - acc_top5: 0.9804 - 53ms/step\n",
      "step 782/782 - loss: 2.8698 - acc_top1: 0.7097 - acc_top5: 0.9804 - 53ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\19\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.0474 - acc_top1: 0.5781 - acc_top5: 0.9656 - 24ms/step\n",
      "step  20/157 - loss: 2.1395 - acc_top1: 0.5828 - acc_top5: 0.9633 - 22ms/step\n",
      "step  30/157 - loss: 1.3625 - acc_top1: 0.5818 - acc_top5: 0.9589 - 21ms/step\n",
      "step  40/157 - loss: 1.8349 - acc_top1: 0.5863 - acc_top5: 0.9570 - 22ms/step\n",
      "step  50/157 - loss: 1.1729 - acc_top1: 0.5872 - acc_top5: 0.9528 - 22ms/step\n",
      "step  60/157 - loss: 4.0717 - acc_top1: 0.5859 - acc_top5: 0.9508 - 21ms/step\n",
      "step  70/157 - loss: 3.2536 - acc_top1: 0.5857 - acc_top5: 0.9493 - 21ms/step\n",
      "step  80/157 - loss: 1.4465 - acc_top1: 0.5873 - acc_top5: 0.9510 - 21ms/step\n",
      "step  90/157 - loss: 1.0112 - acc_top1: 0.5877 - acc_top5: 0.9516 - 21ms/step\n",
      "step 100/157 - loss: 1.1327 - acc_top1: 0.5873 - acc_top5: 0.9519 - 21ms/step\n",
      "step 110/157 - loss: 2.7200 - acc_top1: 0.5841 - acc_top5: 0.9517 - 21ms/step\n",
      "step 120/157 - loss: 3.8296 - acc_top1: 0.5845 - acc_top5: 0.9514 - 21ms/step\n",
      "step 130/157 - loss: 1.5592 - acc_top1: 0.5853 - acc_top5: 0.9516 - 21ms/step\n",
      "step 140/157 - loss: 2.0312 - acc_top1: 0.5843 - acc_top5: 0.9504 - 21ms/step\n",
      "step 150/157 - loss: 2.3119 - acc_top1: 0.5833 - acc_top5: 0.9504 - 21ms/step\n",
      "step 157/157 - loss: 0.7654 - acc_top1: 0.5818 - acc_top5: 0.9512 - 21ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 1.0467 - acc_top1: 0.6281 - acc_top5: 0.9547 - 53ms/step\n",
      "step  20/782 - loss: 1.1772 - acc_top1: 0.6172 - acc_top5: 0.9586 - 55ms/step\n",
      "step  30/782 - loss: 1.1141 - acc_top1: 0.6177 - acc_top5: 0.9568 - 55ms/step\n",
      "step  40/782 - loss: 1.0003 - acc_top1: 0.6273 - acc_top5: 0.9613 - 55ms/step\n",
      "step  50/782 - loss: 0.8962 - acc_top1: 0.6372 - acc_top5: 0.9653 - 55ms/step\n",
      "step  60/782 - loss: 0.8935 - acc_top1: 0.6406 - acc_top5: 0.9685 - 55ms/step\n",
      "step  70/782 - loss: 0.9110 - acc_top1: 0.6397 - acc_top5: 0.9696 - 54ms/step\n",
      "step  80/782 - loss: 0.9561 - acc_top1: 0.6406 - acc_top5: 0.9713 - 54ms/step\n",
      "step  90/782 - loss: 1.0044 - acc_top1: 0.6441 - acc_top5: 0.9727 - 54ms/step\n",
      "step 100/782 - loss: 0.8935 - acc_top1: 0.6511 - acc_top5: 0.9741 - 54ms/step\n",
      "step 110/782 - loss: 0.9052 - acc_top1: 0.6554 - acc_top5: 0.9747 - 53ms/step\n",
      "step 120/782 - loss: 1.0276 - acc_top1: 0.6577 - acc_top5: 0.9751 - 53ms/step\n",
      "step 130/782 - loss: 0.8775 - acc_top1: 0.6567 - acc_top5: 0.9764 - 53ms/step\n",
      "step 140/782 - loss: 1.0786 - acc_top1: 0.6610 - acc_top5: 0.9770 - 53ms/step\n",
      "step 150/782 - loss: 1.0151 - acc_top1: 0.6643 - acc_top5: 0.9774 - 52ms/step\n",
      "step 160/782 - loss: 0.7856 - acc_top1: 0.6670 - acc_top5: 0.9776 - 52ms/step\n",
      "step 170/782 - loss: 0.9989 - acc_top1: 0.6686 - acc_top5: 0.9772 - 52ms/step\n",
      "step 180/782 - loss: 0.6326 - acc_top1: 0.6709 - acc_top5: 0.9775 - 52ms/step\n",
      "step 190/782 - loss: 0.9422 - acc_top1: 0.6712 - acc_top5: 0.9771 - 53ms/step\n",
      "step 200/782 - loss: 0.9459 - acc_top1: 0.6733 - acc_top5: 0.9776 - 53ms/step\n",
      "step 210/782 - loss: 0.8866 - acc_top1: 0.6745 - acc_top5: 0.9775 - 52ms/step\n",
      "step 220/782 - loss: 0.8982 - acc_top1: 0.6760 - acc_top5: 0.9776 - 52ms/step\n",
      "step 230/782 - loss: 0.8116 - acc_top1: 0.6766 - acc_top5: 0.9780 - 52ms/step\n",
      "step 240/782 - loss: 0.9696 - acc_top1: 0.6776 - acc_top5: 0.9781 - 52ms/step\n",
      "step 250/782 - loss: 0.9919 - acc_top1: 0.6791 - acc_top5: 0.9786 - 52ms/step\n",
      "step 260/782 - loss: 0.8410 - acc_top1: 0.6805 - acc_top5: 0.9790 - 52ms/step\n",
      "step 270/782 - loss: 0.8947 - acc_top1: 0.6814 - acc_top5: 0.9789 - 52ms/step\n",
      "step 280/782 - loss: 0.7552 - acc_top1: 0.6822 - acc_top5: 0.9792 - 52ms/step\n",
      "step 290/782 - loss: 0.7546 - acc_top1: 0.6834 - acc_top5: 0.9796 - 52ms/step\n",
      "step 300/782 - loss: 0.7381 - acc_top1: 0.6837 - acc_top5: 0.9795 - 52ms/step\n",
      "step 310/782 - loss: 0.6530 - acc_top1: 0.6836 - acc_top5: 0.9796 - 52ms/step\n",
      "step 320/782 - loss: 0.5250 - acc_top1: 0.6847 - acc_top5: 0.9796 - 52ms/step\n",
      "step 330/782 - loss: 0.6321 - acc_top1: 0.6860 - acc_top5: 0.9798 - 52ms/step\n",
      "step 340/782 - loss: 0.8426 - acc_top1: 0.6864 - acc_top5: 0.9795 - 52ms/step\n",
      "step 350/782 - loss: 0.8235 - acc_top1: 0.6879 - acc_top5: 0.9793 - 51ms/step\n",
      "step 360/782 - loss: 0.6291 - acc_top1: 0.6884 - acc_top5: 0.9790 - 51ms/step\n",
      "step 370/782 - loss: 0.8799 - acc_top1: 0.6888 - acc_top5: 0.9791 - 51ms/step\n",
      "step 380/782 - loss: 0.7047 - acc_top1: 0.6901 - acc_top5: 0.9792 - 51ms/step\n",
      "step 390/782 - loss: 0.7731 - acc_top1: 0.6910 - acc_top5: 0.9793 - 51ms/step\n",
      "step 400/782 - loss: 0.5664 - acc_top1: 0.6909 - acc_top5: 0.9793 - 51ms/step\n",
      "step 410/782 - loss: 0.6883 - acc_top1: 0.6904 - acc_top5: 0.9794 - 51ms/step\n",
      "step 420/782 - loss: 0.8342 - acc_top1: 0.6908 - acc_top5: 0.9795 - 51ms/step\n",
      "step 430/782 - loss: 1.0163 - acc_top1: 0.6917 - acc_top5: 0.9794 - 51ms/step\n",
      "step 440/782 - loss: 0.8821 - acc_top1: 0.6922 - acc_top5: 0.9795 - 51ms/step\n",
      "step 450/782 - loss: 1.0074 - acc_top1: 0.6925 - acc_top5: 0.9795 - 51ms/step\n",
      "step 460/782 - loss: 0.7301 - acc_top1: 0.6932 - acc_top5: 0.9796 - 51ms/step\n",
      "step 470/782 - loss: 0.9860 - acc_top1: 0.6933 - acc_top5: 0.9797 - 51ms/step\n",
      "step 480/782 - loss: 0.5562 - acc_top1: 0.6941 - acc_top5: 0.9797 - 52ms/step\n",
      "step 490/782 - loss: 0.5933 - acc_top1: 0.6948 - acc_top5: 0.9798 - 52ms/step\n",
      "step 500/782 - loss: 1.0150 - acc_top1: 0.6947 - acc_top5: 0.9798 - 52ms/step\n",
      "step 510/782 - loss: 0.6903 - acc_top1: 0.6949 - acc_top5: 0.9800 - 52ms/step\n",
      "step 520/782 - loss: 0.8773 - acc_top1: 0.6953 - acc_top5: 0.9799 - 52ms/step\n",
      "step 530/782 - loss: 0.5837 - acc_top1: 0.6955 - acc_top5: 0.9800 - 52ms/step\n",
      "step 540/782 - loss: 0.7591 - acc_top1: 0.6958 - acc_top5: 0.9800 - 52ms/step\n",
      "step 550/782 - loss: 0.7668 - acc_top1: 0.6957 - acc_top5: 0.9799 - 52ms/step\n",
      "step 560/782 - loss: 0.8351 - acc_top1: 0.6955 - acc_top5: 0.9800 - 52ms/step\n",
      "step 570/782 - loss: 0.7918 - acc_top1: 0.6959 - acc_top5: 0.9802 - 52ms/step\n",
      "step 580/782 - loss: 0.8979 - acc_top1: 0.6960 - acc_top5: 0.9801 - 52ms/step\n",
      "step 590/782 - loss: 0.7618 - acc_top1: 0.6962 - acc_top5: 0.9802 - 52ms/step\n",
      "step 600/782 - loss: 0.8319 - acc_top1: 0.6967 - acc_top5: 0.9802 - 52ms/step\n",
      "step 610/782 - loss: 0.8367 - acc_top1: 0.6970 - acc_top5: 0.9804 - 52ms/step\n",
      "step 620/782 - loss: 0.8262 - acc_top1: 0.6970 - acc_top5: 0.9805 - 52ms/step\n",
      "step 630/782 - loss: 0.5032 - acc_top1: 0.6979 - acc_top5: 0.9806 - 52ms/step\n",
      "step 640/782 - loss: 0.7846 - acc_top1: 0.6980 - acc_top5: 0.9806 - 52ms/step\n",
      "step 650/782 - loss: 0.8483 - acc_top1: 0.6980 - acc_top5: 0.9806 - 52ms/step\n",
      "step 660/782 - loss: 0.7845 - acc_top1: 0.6980 - acc_top5: 0.9807 - 52ms/step\n",
      "step 670/782 - loss: 0.9284 - acc_top1: 0.6986 - acc_top5: 0.9808 - 52ms/step\n",
      "step 680/782 - loss: 0.5836 - acc_top1: 0.6987 - acc_top5: 0.9808 - 52ms/step\n",
      "step 690/782 - loss: 0.8582 - acc_top1: 0.6988 - acc_top5: 0.9808 - 52ms/step\n",
      "step 700/782 - loss: 0.7999 - acc_top1: 0.6989 - acc_top5: 0.9808 - 52ms/step\n",
      "step 710/782 - loss: 0.6002 - acc_top1: 0.6989 - acc_top5: 0.9809 - 52ms/step\n",
      "step 720/782 - loss: 0.8523 - acc_top1: 0.6988 - acc_top5: 0.9809 - 52ms/step\n",
      "step 730/782 - loss: 0.7597 - acc_top1: 0.6988 - acc_top5: 0.9808 - 52ms/step\n",
      "step 740/782 - loss: 0.7852 - acc_top1: 0.6994 - acc_top5: 0.9807 - 52ms/step\n",
      "step 750/782 - loss: 1.0203 - acc_top1: 0.6996 - acc_top5: 0.9807 - 52ms/step\n",
      "step 760/782 - loss: 1.1425 - acc_top1: 0.7001 - acc_top5: 0.9806 - 52ms/step\n",
      "step 770/782 - loss: 0.8673 - acc_top1: 0.7003 - acc_top5: 0.9807 - 52ms/step\n",
      "step 780/782 - loss: 0.8826 - acc_top1: 0.7003 - acc_top5: 0.9806 - 52ms/step\n",
      "step 782/782 - loss: 1.0449 - acc_top1: 0.7003 - acc_top5: 0.9806 - 52ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\20\n",
      "Eval begin...\n",
      "step  10/157 - loss: 0.8474 - acc_top1: 0.6078 - acc_top5: 0.9641 - 20ms/step\n",
      "step  20/157 - loss: 1.4429 - acc_top1: 0.6156 - acc_top5: 0.9633 - 19ms/step\n",
      "step  30/157 - loss: 1.3143 - acc_top1: 0.6057 - acc_top5: 0.9568 - 19ms/step\n",
      "step  40/157 - loss: 1.4939 - acc_top1: 0.6074 - acc_top5: 0.9559 - 19ms/step\n",
      "step  50/157 - loss: 1.2367 - acc_top1: 0.6081 - acc_top5: 0.9544 - 19ms/step\n",
      "step  60/157 - loss: 0.9478 - acc_top1: 0.6065 - acc_top5: 0.9534 - 19ms/step\n",
      "step  70/157 - loss: 1.1626 - acc_top1: 0.6040 - acc_top5: 0.9533 - 19ms/step\n",
      "step  80/157 - loss: 1.3013 - acc_top1: 0.6066 - acc_top5: 0.9539 - 19ms/step\n",
      "step  90/157 - loss: 0.7455 - acc_top1: 0.6087 - acc_top5: 0.9549 - 19ms/step\n",
      "step 100/157 - loss: 1.1321 - acc_top1: 0.6078 - acc_top5: 0.9558 - 19ms/step\n",
      "step 110/157 - loss: 1.5382 - acc_top1: 0.6067 - acc_top5: 0.9560 - 19ms/step\n",
      "step 120/157 - loss: 1.3139 - acc_top1: 0.6057 - acc_top5: 0.9552 - 19ms/step\n",
      "step 130/157 - loss: 1.2981 - acc_top1: 0.6070 - acc_top5: 0.9548 - 19ms/step\n",
      "step 140/157 - loss: 1.1412 - acc_top1: 0.6062 - acc_top5: 0.9542 - 19ms/step\n",
      "step 150/157 - loss: 1.0844 - acc_top1: 0.6049 - acc_top5: 0.9551 - 19ms/step\n",
      "step 157/157 - loss: 0.4395 - acc_top1: 0.6043 - acc_top5: 0.9558 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.8412 - acc_top1: 0.7266 - acc_top5: 0.9828 - 51ms/step\n",
      "step  20/782 - loss: 1.1217 - acc_top1: 0.7359 - acc_top5: 0.9836 - 50ms/step\n",
      "step  30/782 - loss: 0.9496 - acc_top1: 0.7411 - acc_top5: 0.9823 - 50ms/step\n",
      "step  40/782 - loss: 0.8288 - acc_top1: 0.7434 - acc_top5: 0.9840 - 50ms/step\n",
      "step  50/782 - loss: 0.6375 - acc_top1: 0.7466 - acc_top5: 0.9847 - 50ms/step\n",
      "step  60/782 - loss: 0.6613 - acc_top1: 0.7471 - acc_top5: 0.9849 - 50ms/step\n",
      "step  70/782 - loss: 0.9399 - acc_top1: 0.7538 - acc_top5: 0.9842 - 50ms/step\n",
      "step  80/782 - loss: 0.8563 - acc_top1: 0.7527 - acc_top5: 0.9834 - 50ms/step\n",
      "step  90/782 - loss: 1.2799 - acc_top1: 0.7528 - acc_top5: 0.9835 - 50ms/step\n",
      "step 100/782 - loss: 0.5884 - acc_top1: 0.7520 - acc_top5: 0.9831 - 50ms/step\n",
      "step 110/782 - loss: 0.6927 - acc_top1: 0.7524 - acc_top5: 0.9834 - 50ms/step\n",
      "step 120/782 - loss: 0.8055 - acc_top1: 0.7509 - acc_top5: 0.9836 - 50ms/step\n",
      "step 130/782 - loss: 0.6602 - acc_top1: 0.7534 - acc_top5: 0.9837 - 50ms/step\n",
      "step 140/782 - loss: 0.5395 - acc_top1: 0.7550 - acc_top5: 0.9843 - 50ms/step\n",
      "step 150/782 - loss: 0.8161 - acc_top1: 0.7541 - acc_top5: 0.9848 - 50ms/step\n",
      "step 160/782 - loss: 0.8148 - acc_top1: 0.7542 - acc_top5: 0.9848 - 50ms/step\n",
      "step 170/782 - loss: 0.6601 - acc_top1: 0.7536 - acc_top5: 0.9849 - 50ms/step\n",
      "step 180/782 - loss: 0.6818 - acc_top1: 0.7531 - acc_top5: 0.9853 - 50ms/step\n",
      "step 190/782 - loss: 0.7166 - acc_top1: 0.7514 - acc_top5: 0.9853 - 50ms/step\n",
      "step 200/782 - loss: 0.6103 - acc_top1: 0.7515 - acc_top5: 0.9856 - 50ms/step\n",
      "step 210/782 - loss: 0.8924 - acc_top1: 0.7505 - acc_top5: 0.9855 - 50ms/step\n",
      "step 220/782 - loss: 0.5212 - acc_top1: 0.7506 - acc_top5: 0.9857 - 50ms/step\n",
      "step 230/782 - loss: 0.7551 - acc_top1: 0.7507 - acc_top5: 0.9853 - 50ms/step\n",
      "step 240/782 - loss: 0.9024 - acc_top1: 0.7518 - acc_top5: 0.9854 - 50ms/step\n",
      "step 250/782 - loss: 0.7053 - acc_top1: 0.7516 - acc_top5: 0.9853 - 50ms/step\n",
      "step 260/782 - loss: 0.7221 - acc_top1: 0.7513 - acc_top5: 0.9855 - 50ms/step\n",
      "step 270/782 - loss: 0.4972 - acc_top1: 0.7525 - acc_top5: 0.9857 - 50ms/step\n",
      "step 280/782 - loss: 0.6980 - acc_top1: 0.7527 - acc_top5: 0.9854 - 50ms/step\n",
      "step 290/782 - loss: 1.0267 - acc_top1: 0.7511 - acc_top5: 0.9855 - 50ms/step\n",
      "step 300/782 - loss: 0.9679 - acc_top1: 0.7507 - acc_top5: 0.9857 - 50ms/step\n",
      "step 310/782 - loss: 0.7018 - acc_top1: 0.7511 - acc_top5: 0.9858 - 50ms/step\n",
      "step 320/782 - loss: 0.6773 - acc_top1: 0.7513 - acc_top5: 0.9860 - 50ms/step\n",
      "step 330/782 - loss: 0.5577 - acc_top1: 0.7507 - acc_top5: 0.9860 - 50ms/step\n",
      "step 340/782 - loss: 0.8883 - acc_top1: 0.7503 - acc_top5: 0.9857 - 50ms/step\n",
      "step 350/782 - loss: 0.6427 - acc_top1: 0.7493 - acc_top5: 0.9858 - 50ms/step\n",
      "step 360/782 - loss: 0.8513 - acc_top1: 0.7496 - acc_top5: 0.9859 - 50ms/step\n",
      "step 370/782 - loss: 0.6144 - acc_top1: 0.7500 - acc_top5: 0.9861 - 51ms/step\n",
      "step 380/782 - loss: 0.6968 - acc_top1: 0.7495 - acc_top5: 0.9861 - 51ms/step\n",
      "step 390/782 - loss: 0.8676 - acc_top1: 0.7492 - acc_top5: 0.9862 - 51ms/step\n",
      "step 400/782 - loss: 0.6950 - acc_top1: 0.7495 - acc_top5: 0.9861 - 51ms/step\n",
      "step 410/782 - loss: 0.8128 - acc_top1: 0.7489 - acc_top5: 0.9861 - 51ms/step\n",
      "step 420/782 - loss: 0.8574 - acc_top1: 0.7485 - acc_top5: 0.9859 - 51ms/step\n",
      "step 430/782 - loss: 0.8563 - acc_top1: 0.7477 - acc_top5: 0.9859 - 51ms/step\n",
      "step 440/782 - loss: 0.9294 - acc_top1: 0.7480 - acc_top5: 0.9860 - 51ms/step\n",
      "step 450/782 - loss: 0.7060 - acc_top1: 0.7473 - acc_top5: 0.9861 - 51ms/step\n",
      "step 460/782 - loss: 0.8704 - acc_top1: 0.7476 - acc_top5: 0.9860 - 51ms/step\n",
      "step 470/782 - loss: 0.7494 - acc_top1: 0.7469 - acc_top5: 0.9860 - 51ms/step\n",
      "step 480/782 - loss: 0.7281 - acc_top1: 0.7469 - acc_top5: 0.9860 - 51ms/step\n",
      "step 490/782 - loss: 0.6779 - acc_top1: 0.7467 - acc_top5: 0.9859 - 51ms/step\n",
      "step 500/782 - loss: 0.7042 - acc_top1: 0.7462 - acc_top5: 0.9859 - 51ms/step\n",
      "step 510/782 - loss: 0.8645 - acc_top1: 0.7455 - acc_top5: 0.9859 - 51ms/step\n",
      "step 520/782 - loss: 0.8835 - acc_top1: 0.7446 - acc_top5: 0.9859 - 51ms/step\n",
      "step 530/782 - loss: 0.8300 - acc_top1: 0.7446 - acc_top5: 0.9859 - 51ms/step\n",
      "step 540/782 - loss: 0.9015 - acc_top1: 0.7444 - acc_top5: 0.9859 - 51ms/step\n",
      "step 550/782 - loss: 0.8297 - acc_top1: 0.7439 - acc_top5: 0.9859 - 51ms/step\n",
      "step 560/782 - loss: 0.7226 - acc_top1: 0.7433 - acc_top5: 0.9860 - 51ms/step\n",
      "step 570/782 - loss: 0.8311 - acc_top1: 0.7430 - acc_top5: 0.9859 - 51ms/step\n",
      "step 580/782 - loss: 0.7936 - acc_top1: 0.7426 - acc_top5: 0.9857 - 51ms/step\n",
      "step 590/782 - loss: 0.6290 - acc_top1: 0.7422 - acc_top5: 0.9857 - 51ms/step\n",
      "step 600/782 - loss: 1.1915 - acc_top1: 0.7415 - acc_top5: 0.9854 - 51ms/step\n",
      "step 610/782 - loss: 0.8419 - acc_top1: 0.7412 - acc_top5: 0.9854 - 51ms/step\n",
      "step 620/782 - loss: 0.6882 - acc_top1: 0.7409 - acc_top5: 0.9854 - 51ms/step\n",
      "step 630/782 - loss: 0.9593 - acc_top1: 0.7406 - acc_top5: 0.9852 - 51ms/step\n",
      "step 640/782 - loss: 0.7405 - acc_top1: 0.7403 - acc_top5: 0.9853 - 51ms/step\n",
      "step 650/782 - loss: 0.7242 - acc_top1: 0.7407 - acc_top5: 0.9854 - 51ms/step\n",
      "step 660/782 - loss: 0.7350 - acc_top1: 0.7406 - acc_top5: 0.9853 - 51ms/step\n",
      "step 670/782 - loss: 0.7498 - acc_top1: 0.7405 - acc_top5: 0.9854 - 51ms/step\n",
      "step 680/782 - loss: 0.5821 - acc_top1: 0.7408 - acc_top5: 0.9854 - 51ms/step\n",
      "step 690/782 - loss: 0.7632 - acc_top1: 0.7408 - acc_top5: 0.9855 - 51ms/step\n",
      "step 700/782 - loss: 0.7069 - acc_top1: 0.7408 - acc_top5: 0.9855 - 51ms/step\n",
      "step 710/782 - loss: 0.4773 - acc_top1: 0.7404 - acc_top5: 0.9854 - 51ms/step\n",
      "step 720/782 - loss: 0.7591 - acc_top1: 0.7402 - acc_top5: 0.9854 - 51ms/step\n",
      "step 730/782 - loss: 0.7080 - acc_top1: 0.7402 - acc_top5: 0.9854 - 51ms/step\n",
      "step 740/782 - loss: 0.8061 - acc_top1: 0.7401 - acc_top5: 0.9854 - 51ms/step\n",
      "step 750/782 - loss: 0.8842 - acc_top1: 0.7400 - acc_top5: 0.9855 - 51ms/step\n",
      "step 760/782 - loss: 0.6712 - acc_top1: 0.7400 - acc_top5: 0.9855 - 51ms/step\n",
      "step 770/782 - loss: 0.8492 - acc_top1: 0.7397 - acc_top5: 0.9856 - 51ms/step\n",
      "step 780/782 - loss: 0.6522 - acc_top1: 0.7394 - acc_top5: 0.9856 - 51ms/step\n",
      "step 782/782 - loss: 1.3255 - acc_top1: 0.7394 - acc_top5: 0.9856 - 51ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\21\n",
      "Eval begin...\n",
      "step  10/157 - loss: 0.9633 - acc_top1: 0.6047 - acc_top5: 0.9625 - 19ms/step\n",
      "step  20/157 - loss: 1.6677 - acc_top1: 0.6000 - acc_top5: 0.9617 - 19ms/step\n",
      "step  30/157 - loss: 1.3754 - acc_top1: 0.6031 - acc_top5: 0.9589 - 19ms/step\n",
      "step  40/157 - loss: 1.6970 - acc_top1: 0.6027 - acc_top5: 0.9586 - 19ms/step\n",
      "step  50/157 - loss: 1.1761 - acc_top1: 0.6044 - acc_top5: 0.9572 - 19ms/step\n",
      "step  60/157 - loss: 1.1629 - acc_top1: 0.6055 - acc_top5: 0.9555 - 19ms/step\n",
      "step  70/157 - loss: 2.1966 - acc_top1: 0.6078 - acc_top5: 0.9549 - 19ms/step\n",
      "step  80/157 - loss: 1.2475 - acc_top1: 0.6076 - acc_top5: 0.9545 - 19ms/step\n",
      "step  90/157 - loss: 0.8475 - acc_top1: 0.6085 - acc_top5: 0.9554 - 19ms/step\n",
      "step 100/157 - loss: 1.3041 - acc_top1: 0.6088 - acc_top5: 0.9552 - 19ms/step\n",
      "step 110/157 - loss: 1.7207 - acc_top1: 0.6060 - acc_top5: 0.9555 - 19ms/step\n",
      "step 120/157 - loss: 1.2223 - acc_top1: 0.6065 - acc_top5: 0.9552 - 19ms/step\n",
      "step 130/157 - loss: 1.2976 - acc_top1: 0.6081 - acc_top5: 0.9558 - 19ms/step\n",
      "step 140/157 - loss: 1.2251 - acc_top1: 0.6070 - acc_top5: 0.9555 - 19ms/step\n",
      "step 150/157 - loss: 1.5181 - acc_top1: 0.6067 - acc_top5: 0.9547 - 19ms/step\n",
      "step 157/157 - loss: 0.9685 - acc_top1: 0.6061 - acc_top5: 0.9547 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.7386 - acc_top1: 0.7906 - acc_top5: 0.9875 - 51ms/step\n",
      "step  20/782 - loss: 0.6719 - acc_top1: 0.7742 - acc_top5: 0.9859 - 50ms/step\n",
      "step  30/782 - loss: 0.7066 - acc_top1: 0.7724 - acc_top5: 0.9870 - 50ms/step\n",
      "step  40/782 - loss: 0.5146 - acc_top1: 0.7746 - acc_top5: 0.9895 - 50ms/step\n",
      "step  50/782 - loss: 0.3696 - acc_top1: 0.7825 - acc_top5: 0.9888 - 50ms/step\n",
      "step  60/782 - loss: 0.3173 - acc_top1: 0.7831 - acc_top5: 0.9896 - 50ms/step\n",
      "step  70/782 - loss: 0.4819 - acc_top1: 0.7839 - acc_top5: 0.9904 - 50ms/step\n",
      "step  80/782 - loss: 0.5178 - acc_top1: 0.7822 - acc_top5: 0.9895 - 50ms/step\n",
      "step  90/782 - loss: 0.7828 - acc_top1: 0.7835 - acc_top5: 0.9896 - 50ms/step\n",
      "step 100/782 - loss: 0.7159 - acc_top1: 0.7825 - acc_top5: 0.9898 - 50ms/step\n",
      "step 110/782 - loss: 0.4945 - acc_top1: 0.7844 - acc_top5: 0.9903 - 50ms/step\n",
      "step 120/782 - loss: 0.5959 - acc_top1: 0.7850 - acc_top5: 0.9905 - 50ms/step\n",
      "step 130/782 - loss: 0.7485 - acc_top1: 0.7840 - acc_top5: 0.9903 - 50ms/step\n",
      "step 140/782 - loss: 0.5363 - acc_top1: 0.7836 - acc_top5: 0.9897 - 50ms/step\n",
      "step 150/782 - loss: 0.4693 - acc_top1: 0.7854 - acc_top5: 0.9897 - 50ms/step\n",
      "step 160/782 - loss: 0.6301 - acc_top1: 0.7840 - acc_top5: 0.9898 - 50ms/step\n",
      "step 170/782 - loss: 0.7148 - acc_top1: 0.7851 - acc_top5: 0.9900 - 50ms/step\n",
      "step 180/782 - loss: 0.9346 - acc_top1: 0.7853 - acc_top5: 0.9899 - 50ms/step\n",
      "step 190/782 - loss: 0.6463 - acc_top1: 0.7828 - acc_top5: 0.9897 - 50ms/step\n",
      "step 200/782 - loss: 0.4721 - acc_top1: 0.7836 - acc_top5: 0.9893 - 50ms/step\n",
      "step 210/782 - loss: 0.7154 - acc_top1: 0.7839 - acc_top5: 0.9896 - 50ms/step\n",
      "step 220/782 - loss: 0.8583 - acc_top1: 0.7832 - acc_top5: 0.9893 - 50ms/step\n",
      "step 230/782 - loss: 0.5192 - acc_top1: 0.7835 - acc_top5: 0.9895 - 50ms/step\n",
      "step 240/782 - loss: 0.7217 - acc_top1: 0.7820 - acc_top5: 0.9895 - 50ms/step\n",
      "step 250/782 - loss: 0.6140 - acc_top1: 0.7819 - acc_top5: 0.9893 - 50ms/step\n",
      "step 260/782 - loss: 0.8699 - acc_top1: 0.7806 - acc_top5: 0.9894 - 50ms/step\n",
      "step 270/782 - loss: 0.6133 - acc_top1: 0.7801 - acc_top5: 0.9892 - 50ms/step\n",
      "step 280/782 - loss: 0.5770 - acc_top1: 0.7794 - acc_top5: 0.9890 - 50ms/step\n",
      "step 290/782 - loss: 0.6296 - acc_top1: 0.7790 - acc_top5: 0.9890 - 50ms/step\n",
      "step 300/782 - loss: 0.6760 - acc_top1: 0.7788 - acc_top5: 0.9890 - 50ms/step\n",
      "step 310/782 - loss: 0.7180 - acc_top1: 0.7778 - acc_top5: 0.9888 - 50ms/step\n",
      "step 320/782 - loss: 0.6338 - acc_top1: 0.7766 - acc_top5: 0.9887 - 50ms/step\n",
      "step 330/782 - loss: 0.6637 - acc_top1: 0.7764 - acc_top5: 0.9887 - 50ms/step\n",
      "step 340/782 - loss: 0.4742 - acc_top1: 0.7761 - acc_top5: 0.9888 - 50ms/step\n",
      "step 350/782 - loss: 0.8025 - acc_top1: 0.7758 - acc_top5: 0.9889 - 50ms/step\n",
      "step 360/782 - loss: 0.6458 - acc_top1: 0.7749 - acc_top5: 0.9888 - 50ms/step\n",
      "step 370/782 - loss: 0.8552 - acc_top1: 0.7745 - acc_top5: 0.9886 - 50ms/step\n",
      "step 380/782 - loss: 0.5917 - acc_top1: 0.7736 - acc_top5: 0.9887 - 50ms/step\n",
      "step 390/782 - loss: 0.6617 - acc_top1: 0.7732 - acc_top5: 0.9888 - 50ms/step\n",
      "step 400/782 - loss: 0.7380 - acc_top1: 0.7726 - acc_top5: 0.9888 - 50ms/step\n",
      "step 410/782 - loss: 0.6844 - acc_top1: 0.7721 - acc_top5: 0.9888 - 50ms/step\n",
      "step 420/782 - loss: 0.6454 - acc_top1: 0.7719 - acc_top5: 0.9887 - 50ms/step\n",
      "step 430/782 - loss: 0.8299 - acc_top1: 0.7721 - acc_top5: 0.9886 - 50ms/step\n",
      "step 440/782 - loss: 0.6819 - acc_top1: 0.7721 - acc_top5: 0.9885 - 50ms/step\n",
      "step 450/782 - loss: 0.6909 - acc_top1: 0.7722 - acc_top5: 0.9886 - 50ms/step\n",
      "step 460/782 - loss: 0.6364 - acc_top1: 0.7717 - acc_top5: 0.9886 - 50ms/step\n",
      "step 470/782 - loss: 0.8121 - acc_top1: 0.7714 - acc_top5: 0.9884 - 50ms/step\n",
      "step 480/782 - loss: 0.8968 - acc_top1: 0.7712 - acc_top5: 0.9886 - 50ms/step\n",
      "step 490/782 - loss: 0.8040 - acc_top1: 0.7707 - acc_top5: 0.9885 - 50ms/step\n",
      "step 500/782 - loss: 0.6917 - acc_top1: 0.7699 - acc_top5: 0.9885 - 50ms/step\n",
      "step 510/782 - loss: 0.7758 - acc_top1: 0.7700 - acc_top5: 0.9886 - 50ms/step\n",
      "step 520/782 - loss: 0.3484 - acc_top1: 0.7700 - acc_top5: 0.9885 - 50ms/step\n",
      "step 530/782 - loss: 0.7110 - acc_top1: 0.7700 - acc_top5: 0.9886 - 50ms/step\n",
      "step 540/782 - loss: 0.8173 - acc_top1: 0.7697 - acc_top5: 0.9886 - 50ms/step\n",
      "step 550/782 - loss: 0.6766 - acc_top1: 0.7696 - acc_top5: 0.9885 - 50ms/step\n",
      "step 560/782 - loss: 0.6473 - acc_top1: 0.7696 - acc_top5: 0.9886 - 50ms/step\n",
      "step 570/782 - loss: 0.6056 - acc_top1: 0.7695 - acc_top5: 0.9887 - 50ms/step\n",
      "step 580/782 - loss: 0.7608 - acc_top1: 0.7695 - acc_top5: 0.9887 - 50ms/step\n",
      "step 590/782 - loss: 0.7228 - acc_top1: 0.7689 - acc_top5: 0.9886 - 50ms/step\n",
      "step 600/782 - loss: 0.6925 - acc_top1: 0.7684 - acc_top5: 0.9885 - 50ms/step\n",
      "step 610/782 - loss: 0.5012 - acc_top1: 0.7683 - acc_top5: 0.9885 - 50ms/step\n",
      "step 620/782 - loss: 0.5312 - acc_top1: 0.7681 - acc_top5: 0.9885 - 50ms/step\n",
      "step 630/782 - loss: 0.7050 - acc_top1: 0.7682 - acc_top5: 0.9885 - 50ms/step\n",
      "step 640/782 - loss: 0.8077 - acc_top1: 0.7677 - acc_top5: 0.9884 - 50ms/step\n",
      "step 650/782 - loss: 0.5561 - acc_top1: 0.7678 - acc_top5: 0.9885 - 50ms/step\n",
      "step 660/782 - loss: 0.7088 - acc_top1: 0.7677 - acc_top5: 0.9884 - 50ms/step\n",
      "step 670/782 - loss: 0.6650 - acc_top1: 0.7674 - acc_top5: 0.9885 - 50ms/step\n",
      "step 680/782 - loss: 0.9079 - acc_top1: 0.7668 - acc_top5: 0.9884 - 50ms/step\n",
      "step 690/782 - loss: 0.7393 - acc_top1: 0.7665 - acc_top5: 0.9884 - 50ms/step\n",
      "step 700/782 - loss: 0.7926 - acc_top1: 0.7662 - acc_top5: 0.9882 - 50ms/step\n",
      "step 710/782 - loss: 0.7042 - acc_top1: 0.7659 - acc_top5: 0.9881 - 50ms/step\n",
      "step 720/782 - loss: 0.4737 - acc_top1: 0.7657 - acc_top5: 0.9881 - 50ms/step\n",
      "step 730/782 - loss: 0.7519 - acc_top1: 0.7652 - acc_top5: 0.9882 - 50ms/step\n",
      "step 740/782 - loss: 0.5442 - acc_top1: 0.7650 - acc_top5: 0.9882 - 50ms/step\n",
      "step 750/782 - loss: 0.7574 - acc_top1: 0.7649 - acc_top5: 0.9883 - 50ms/step\n",
      "step 760/782 - loss: 0.4423 - acc_top1: 0.7652 - acc_top5: 0.9882 - 50ms/step\n",
      "step 770/782 - loss: 0.9710 - acc_top1: 0.7651 - acc_top5: 0.9882 - 50ms/step\n",
      "step 780/782 - loss: 0.6061 - acc_top1: 0.7647 - acc_top5: 0.9882 - 50ms/step\n",
      "step 782/782 - loss: 0.6230 - acc_top1: 0.7648 - acc_top5: 0.9882 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\22\n",
      "Eval begin...\n",
      "step  10/157 - loss: 0.7771 - acc_top1: 0.6031 - acc_top5: 0.9703 - 20ms/step\n",
      "step  20/157 - loss: 1.8445 - acc_top1: 0.6156 - acc_top5: 0.9641 - 19ms/step\n",
      "step  30/157 - loss: 1.4599 - acc_top1: 0.6130 - acc_top5: 0.9583 - 19ms/step\n",
      "step  40/157 - loss: 1.6789 - acc_top1: 0.6148 - acc_top5: 0.9574 - 19ms/step\n",
      "step  50/157 - loss: 1.4336 - acc_top1: 0.6094 - acc_top5: 0.9575 - 19ms/step\n",
      "step  60/157 - loss: 1.1891 - acc_top1: 0.6094 - acc_top5: 0.9565 - 19ms/step\n",
      "step  70/157 - loss: 1.5635 - acc_top1: 0.6118 - acc_top5: 0.9551 - 19ms/step\n",
      "step  80/157 - loss: 1.3239 - acc_top1: 0.6146 - acc_top5: 0.9547 - 19ms/step\n",
      "step  90/157 - loss: 1.0121 - acc_top1: 0.6156 - acc_top5: 0.9569 - 19ms/step\n",
      "step 100/157 - loss: 1.2618 - acc_top1: 0.6148 - acc_top5: 0.9572 - 19ms/step\n",
      "step 110/157 - loss: 1.6994 - acc_top1: 0.6122 - acc_top5: 0.9568 - 19ms/step\n",
      "step 120/157 - loss: 1.3180 - acc_top1: 0.6139 - acc_top5: 0.9565 - 19ms/step\n",
      "step 130/157 - loss: 1.5230 - acc_top1: 0.6130 - acc_top5: 0.9566 - 19ms/step\n",
      "step 140/157 - loss: 1.2537 - acc_top1: 0.6126 - acc_top5: 0.9558 - 19ms/step\n",
      "step 150/157 - loss: 1.2213 - acc_top1: 0.6108 - acc_top5: 0.9553 - 19ms/step\n",
      "step 157/157 - loss: 0.8060 - acc_top1: 0.6089 - acc_top5: 0.9558 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.6420 - acc_top1: 0.8156 - acc_top5: 0.9938 - 52ms/step\n",
      "step  20/782 - loss: 0.5345 - acc_top1: 0.8117 - acc_top5: 0.9930 - 51ms/step\n",
      "step  30/782 - loss: 0.4716 - acc_top1: 0.8099 - acc_top5: 0.9927 - 51ms/step\n",
      "step  40/782 - loss: 0.4314 - acc_top1: 0.8066 - acc_top5: 0.9922 - 50ms/step\n",
      "step  50/782 - loss: 0.5016 - acc_top1: 0.8034 - acc_top5: 0.9928 - 50ms/step\n",
      "step  60/782 - loss: 0.5789 - acc_top1: 0.8060 - acc_top5: 0.9935 - 50ms/step\n",
      "step  70/782 - loss: 0.7808 - acc_top1: 0.8105 - acc_top5: 0.9931 - 50ms/step\n",
      "step  80/782 - loss: 0.5514 - acc_top1: 0.8105 - acc_top5: 0.9926 - 50ms/step\n",
      "step  90/782 - loss: 0.4399 - acc_top1: 0.8127 - acc_top5: 0.9927 - 50ms/step\n",
      "step 100/782 - loss: 0.4081 - acc_top1: 0.8128 - acc_top5: 0.9925 - 50ms/step\n",
      "step 110/782 - loss: 0.5915 - acc_top1: 0.8098 - acc_top5: 0.9918 - 50ms/step\n",
      "step 120/782 - loss: 0.4331 - acc_top1: 0.8085 - acc_top5: 0.9918 - 50ms/step\n",
      "step 130/782 - loss: 0.5927 - acc_top1: 0.8071 - acc_top5: 0.9919 - 50ms/step\n",
      "step 140/782 - loss: 0.7619 - acc_top1: 0.8059 - acc_top5: 0.9922 - 50ms/step\n",
      "step 150/782 - loss: 0.5461 - acc_top1: 0.8065 - acc_top5: 0.9923 - 50ms/step\n",
      "step 160/782 - loss: 0.6304 - acc_top1: 0.8058 - acc_top5: 0.9921 - 50ms/step\n",
      "step 170/782 - loss: 0.4388 - acc_top1: 0.8057 - acc_top5: 0.9920 - 50ms/step\n",
      "step 180/782 - loss: 0.5715 - acc_top1: 0.8050 - acc_top5: 0.9918 - 50ms/step\n",
      "step 190/782 - loss: 0.5661 - acc_top1: 0.8041 - acc_top5: 0.9920 - 50ms/step\n",
      "step 200/782 - loss: 0.6912 - acc_top1: 0.8033 - acc_top5: 0.9917 - 50ms/step\n",
      "step 210/782 - loss: 0.5328 - acc_top1: 0.8025 - acc_top5: 0.9916 - 50ms/step\n",
      "step 220/782 - loss: 0.8268 - acc_top1: 0.8014 - acc_top5: 0.9918 - 50ms/step\n",
      "step 230/782 - loss: 0.4452 - acc_top1: 0.8006 - acc_top5: 0.9918 - 50ms/step\n",
      "step 240/782 - loss: 0.5133 - acc_top1: 0.8005 - acc_top5: 0.9919 - 50ms/step\n",
      "step 250/782 - loss: 0.4893 - acc_top1: 0.8001 - acc_top5: 0.9919 - 50ms/step\n",
      "step 260/782 - loss: 0.6317 - acc_top1: 0.7996 - acc_top5: 0.9920 - 50ms/step\n",
      "step 270/782 - loss: 0.5610 - acc_top1: 0.7983 - acc_top5: 0.9920 - 50ms/step\n",
      "step 280/782 - loss: 0.7665 - acc_top1: 0.7978 - acc_top5: 0.9916 - 50ms/step\n",
      "step 290/782 - loss: 0.4868 - acc_top1: 0.7976 - acc_top5: 0.9918 - 50ms/step\n",
      "step 300/782 - loss: 0.7263 - acc_top1: 0.7974 - acc_top5: 0.9918 - 50ms/step\n",
      "step 310/782 - loss: 0.4422 - acc_top1: 0.7978 - acc_top5: 0.9920 - 50ms/step\n",
      "step 320/782 - loss: 0.6093 - acc_top1: 0.7984 - acc_top5: 0.9919 - 50ms/step\n",
      "step 330/782 - loss: 0.5287 - acc_top1: 0.7982 - acc_top5: 0.9921 - 50ms/step\n",
      "step 340/782 - loss: 0.4066 - acc_top1: 0.7983 - acc_top5: 0.9921 - 50ms/step\n",
      "step 350/782 - loss: 0.5247 - acc_top1: 0.7976 - acc_top5: 0.9921 - 50ms/step\n",
      "step 360/782 - loss: 0.6041 - acc_top1: 0.7977 - acc_top5: 0.9918 - 50ms/step\n",
      "step 370/782 - loss: 0.4707 - acc_top1: 0.7972 - acc_top5: 0.9920 - 50ms/step\n",
      "step 380/782 - loss: 0.4066 - acc_top1: 0.7972 - acc_top5: 0.9921 - 50ms/step\n",
      "step 390/782 - loss: 0.5322 - acc_top1: 0.7962 - acc_top5: 0.9921 - 50ms/step\n",
      "step 400/782 - loss: 0.7118 - acc_top1: 0.7958 - acc_top5: 0.9921 - 50ms/step\n",
      "step 410/782 - loss: 0.6408 - acc_top1: 0.7958 - acc_top5: 0.9921 - 50ms/step\n",
      "step 420/782 - loss: 0.4024 - acc_top1: 0.7957 - acc_top5: 0.9921 - 50ms/step\n",
      "step 430/782 - loss: 0.5139 - acc_top1: 0.7953 - acc_top5: 0.9922 - 50ms/step\n",
      "step 440/782 - loss: 0.6296 - acc_top1: 0.7953 - acc_top5: 0.9922 - 50ms/step\n",
      "step 450/782 - loss: 0.4285 - acc_top1: 0.7952 - acc_top5: 0.9923 - 50ms/step\n",
      "step 460/782 - loss: 0.6070 - acc_top1: 0.7950 - acc_top5: 0.9923 - 50ms/step\n",
      "step 470/782 - loss: 0.8203 - acc_top1: 0.7951 - acc_top5: 0.9924 - 50ms/step\n",
      "step 480/782 - loss: 0.5543 - acc_top1: 0.7949 - acc_top5: 0.9924 - 50ms/step\n",
      "step 490/782 - loss: 0.7370 - acc_top1: 0.7949 - acc_top5: 0.9925 - 50ms/step\n",
      "step 500/782 - loss: 0.4726 - acc_top1: 0.7946 - acc_top5: 0.9926 - 50ms/step\n",
      "step 510/782 - loss: 0.6075 - acc_top1: 0.7948 - acc_top5: 0.9926 - 50ms/step\n",
      "step 520/782 - loss: 0.5908 - acc_top1: 0.7946 - acc_top5: 0.9927 - 50ms/step\n",
      "step 530/782 - loss: 0.7223 - acc_top1: 0.7941 - acc_top5: 0.9926 - 50ms/step\n",
      "step 540/782 - loss: 0.5854 - acc_top1: 0.7940 - acc_top5: 0.9926 - 50ms/step\n",
      "step 550/782 - loss: 0.6298 - acc_top1: 0.7934 - acc_top5: 0.9927 - 50ms/step\n",
      "step 560/782 - loss: 0.6341 - acc_top1: 0.7938 - acc_top5: 0.9927 - 50ms/step\n",
      "step 570/782 - loss: 0.6666 - acc_top1: 0.7937 - acc_top5: 0.9927 - 50ms/step\n",
      "step 580/782 - loss: 0.6260 - acc_top1: 0.7934 - acc_top5: 0.9925 - 50ms/step\n",
      "step 590/782 - loss: 1.0056 - acc_top1: 0.7932 - acc_top5: 0.9925 - 50ms/step\n",
      "step 600/782 - loss: 0.6614 - acc_top1: 0.7931 - acc_top5: 0.9925 - 50ms/step\n",
      "step 610/782 - loss: 0.6507 - acc_top1: 0.7929 - acc_top5: 0.9925 - 50ms/step\n",
      "step 620/782 - loss: 0.6826 - acc_top1: 0.7925 - acc_top5: 0.9925 - 50ms/step\n",
      "step 630/782 - loss: 0.5212 - acc_top1: 0.7919 - acc_top5: 0.9924 - 50ms/step\n",
      "step 640/782 - loss: 0.7011 - acc_top1: 0.7915 - acc_top5: 0.9923 - 50ms/step\n",
      "step 650/782 - loss: 0.5208 - acc_top1: 0.7908 - acc_top5: 0.9923 - 50ms/step\n",
      "step 660/782 - loss: 0.6587 - acc_top1: 0.7903 - acc_top5: 0.9923 - 50ms/step\n",
      "step 670/782 - loss: 0.8229 - acc_top1: 0.7899 - acc_top5: 0.9923 - 50ms/step\n",
      "step 680/782 - loss: 0.7710 - acc_top1: 0.7898 - acc_top5: 0.9923 - 50ms/step\n",
      "step 690/782 - loss: 0.6256 - acc_top1: 0.7894 - acc_top5: 0.9923 - 50ms/step\n",
      "step 700/782 - loss: 0.7099 - acc_top1: 0.7893 - acc_top5: 0.9923 - 50ms/step\n",
      "step 710/782 - loss: 0.8066 - acc_top1: 0.7886 - acc_top5: 0.9923 - 50ms/step\n",
      "step 720/782 - loss: 0.7530 - acc_top1: 0.7883 - acc_top5: 0.9923 - 50ms/step\n",
      "step 730/782 - loss: 0.6413 - acc_top1: 0.7878 - acc_top5: 0.9922 - 50ms/step\n",
      "step 740/782 - loss: 0.3475 - acc_top1: 0.7880 - acc_top5: 0.9922 - 50ms/step\n",
      "step 750/782 - loss: 0.7029 - acc_top1: 0.7879 - acc_top5: 0.9922 - 50ms/step\n",
      "step 760/782 - loss: 0.6370 - acc_top1: 0.7874 - acc_top5: 0.9921 - 50ms/step\n",
      "step 770/782 - loss: 0.5087 - acc_top1: 0.7871 - acc_top5: 0.9921 - 50ms/step\n",
      "step 780/782 - loss: 0.6801 - acc_top1: 0.7867 - acc_top5: 0.9921 - 50ms/step\n",
      "step 782/782 - loss: 1.0058 - acc_top1: 0.7867 - acc_top5: 0.9921 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\23\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.0618 - acc_top1: 0.5953 - acc_top5: 0.9625 - 20ms/step\n",
      "step  20/157 - loss: 2.1457 - acc_top1: 0.6016 - acc_top5: 0.9578 - 19ms/step\n",
      "step  30/157 - loss: 2.6376 - acc_top1: 0.6000 - acc_top5: 0.9536 - 19ms/step\n",
      "step  40/157 - loss: 1.7366 - acc_top1: 0.6004 - acc_top5: 0.9488 - 19ms/step\n",
      "step  50/157 - loss: 1.7516 - acc_top1: 0.5962 - acc_top5: 0.9478 - 19ms/step\n",
      "step  60/157 - loss: 5.4829 - acc_top1: 0.5977 - acc_top5: 0.9466 - 19ms/step\n",
      "step  70/157 - loss: 9.3337 - acc_top1: 0.5938 - acc_top5: 0.9453 - 19ms/step\n",
      "step  80/157 - loss: 2.0195 - acc_top1: 0.5943 - acc_top5: 0.9455 - 19ms/step\n",
      "step  90/157 - loss: 1.4642 - acc_top1: 0.5927 - acc_top5: 0.9474 - 19ms/step\n",
      "step 100/157 - loss: 2.2910 - acc_top1: 0.5886 - acc_top5: 0.9473 - 19ms/step\n",
      "step 110/157 - loss: 3.5944 - acc_top1: 0.5869 - acc_top5: 0.9474 - 19ms/step\n",
      "step 120/157 - loss: 1.6516 - acc_top1: 0.5878 - acc_top5: 0.9470 - 19ms/step\n",
      "step 130/157 - loss: 1.4176 - acc_top1: 0.5889 - acc_top5: 0.9460 - 19ms/step\n",
      "step 140/157 - loss: 3.7388 - acc_top1: 0.5879 - acc_top5: 0.9452 - 19ms/step\n",
      "step 150/157 - loss: 5.6989 - acc_top1: 0.5877 - acc_top5: 0.9457 - 19ms/step\n",
      "step 157/157 - loss: 0.7161 - acc_top1: 0.5852 - acc_top5: 0.9459 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.5624 - acc_top1: 0.8063 - acc_top5: 0.9906 - 51ms/step\n",
      "step  20/782 - loss: 0.5941 - acc_top1: 0.7930 - acc_top5: 0.9914 - 51ms/step\n",
      "step  30/782 - loss: 0.4591 - acc_top1: 0.8063 - acc_top5: 0.9917 - 50ms/step\n",
      "step  40/782 - loss: 0.4663 - acc_top1: 0.8129 - acc_top5: 0.9930 - 50ms/step\n",
      "step  50/782 - loss: 0.5192 - acc_top1: 0.8163 - acc_top5: 0.9934 - 50ms/step\n",
      "step  60/782 - loss: 0.6085 - acc_top1: 0.8177 - acc_top5: 0.9930 - 50ms/step\n",
      "step  70/782 - loss: 0.6446 - acc_top1: 0.8141 - acc_top5: 0.9931 - 50ms/step\n",
      "step  80/782 - loss: 0.7894 - acc_top1: 0.8127 - acc_top5: 0.9926 - 50ms/step\n",
      "step  90/782 - loss: 0.4496 - acc_top1: 0.8148 - acc_top5: 0.9927 - 50ms/step\n",
      "step 100/782 - loss: 0.5049 - acc_top1: 0.8156 - acc_top5: 0.9928 - 50ms/step\n",
      "step 110/782 - loss: 0.3855 - acc_top1: 0.8176 - acc_top5: 0.9932 - 50ms/step\n",
      "step 120/782 - loss: 0.4452 - acc_top1: 0.8180 - acc_top5: 0.9930 - 50ms/step\n",
      "step 130/782 - loss: 0.5478 - acc_top1: 0.8169 - acc_top5: 0.9931 - 50ms/step\n",
      "step 140/782 - loss: 0.5195 - acc_top1: 0.8172 - acc_top5: 0.9933 - 50ms/step\n",
      "step 150/782 - loss: 0.5131 - acc_top1: 0.8185 - acc_top5: 0.9932 - 50ms/step\n",
      "step 160/782 - loss: 0.7609 - acc_top1: 0.8187 - acc_top5: 0.9935 - 50ms/step\n",
      "step 170/782 - loss: 0.3957 - acc_top1: 0.8202 - acc_top5: 0.9937 - 50ms/step\n",
      "step 180/782 - loss: 0.4627 - acc_top1: 0.8211 - acc_top5: 0.9938 - 50ms/step\n",
      "step 190/782 - loss: 0.5052 - acc_top1: 0.8220 - acc_top5: 0.9938 - 50ms/step\n",
      "step 200/782 - loss: 0.3797 - acc_top1: 0.8217 - acc_top5: 0.9941 - 50ms/step\n",
      "step 210/782 - loss: 0.5562 - acc_top1: 0.8213 - acc_top5: 0.9941 - 50ms/step\n",
      "step 220/782 - loss: 0.8035 - acc_top1: 0.8198 - acc_top5: 0.9941 - 50ms/step\n",
      "step 230/782 - loss: 0.5919 - acc_top1: 0.8187 - acc_top5: 0.9939 - 50ms/step\n",
      "step 240/782 - loss: 0.4808 - acc_top1: 0.8187 - acc_top5: 0.9938 - 50ms/step\n",
      "step 250/782 - loss: 0.4363 - acc_top1: 0.8174 - acc_top5: 0.9938 - 50ms/step\n",
      "step 260/782 - loss: 0.5663 - acc_top1: 0.8166 - acc_top5: 0.9937 - 50ms/step\n",
      "step 270/782 - loss: 0.5059 - acc_top1: 0.8157 - acc_top5: 0.9938 - 50ms/step\n",
      "step 280/782 - loss: 0.7437 - acc_top1: 0.8160 - acc_top5: 0.9938 - 50ms/step\n",
      "step 290/782 - loss: 0.6855 - acc_top1: 0.8152 - acc_top5: 0.9938 - 50ms/step\n",
      "step 300/782 - loss: 0.4266 - acc_top1: 0.8156 - acc_top5: 0.9938 - 50ms/step\n",
      "step 310/782 - loss: 0.6515 - acc_top1: 0.8156 - acc_top5: 0.9939 - 50ms/step\n",
      "step 320/782 - loss: 0.4686 - acc_top1: 0.8159 - acc_top5: 0.9939 - 51ms/step\n",
      "step 330/782 - loss: 0.7057 - acc_top1: 0.8156 - acc_top5: 0.9939 - 51ms/step\n",
      "step 340/782 - loss: 0.3932 - acc_top1: 0.8160 - acc_top5: 0.9938 - 51ms/step\n",
      "step 350/782 - loss: 0.6576 - acc_top1: 0.8157 - acc_top5: 0.9937 - 51ms/step\n",
      "step 360/782 - loss: 0.4100 - acc_top1: 0.8154 - acc_top5: 0.9936 - 51ms/step\n",
      "step 370/782 - loss: 0.6207 - acc_top1: 0.8149 - acc_top5: 0.9937 - 51ms/step\n",
      "step 380/782 - loss: 0.6572 - acc_top1: 0.8150 - acc_top5: 0.9936 - 51ms/step\n",
      "step 390/782 - loss: 0.4065 - acc_top1: 0.8149 - acc_top5: 0.9936 - 51ms/step\n",
      "step 400/782 - loss: 0.5495 - acc_top1: 0.8145 - acc_top5: 0.9936 - 51ms/step\n",
      "step 410/782 - loss: 0.5170 - acc_top1: 0.8138 - acc_top5: 0.9936 - 51ms/step\n",
      "step 420/782 - loss: 0.5064 - acc_top1: 0.8135 - acc_top5: 0.9936 - 51ms/step\n",
      "step 430/782 - loss: 0.6264 - acc_top1: 0.8133 - acc_top5: 0.9936 - 51ms/step\n",
      "step 440/782 - loss: 0.4765 - acc_top1: 0.8130 - acc_top5: 0.9934 - 51ms/step\n",
      "step 450/782 - loss: 0.5067 - acc_top1: 0.8127 - acc_top5: 0.9933 - 51ms/step\n",
      "step 460/782 - loss: 0.3123 - acc_top1: 0.8126 - acc_top5: 0.9932 - 51ms/step\n",
      "step 470/782 - loss: 0.6017 - acc_top1: 0.8126 - acc_top5: 0.9931 - 51ms/step\n",
      "step 480/782 - loss: 0.5953 - acc_top1: 0.8123 - acc_top5: 0.9931 - 51ms/step\n",
      "step 490/782 - loss: 0.4530 - acc_top1: 0.8120 - acc_top5: 0.9931 - 52ms/step\n",
      "step 500/782 - loss: 0.5143 - acc_top1: 0.8116 - acc_top5: 0.9932 - 52ms/step\n",
      "step 510/782 - loss: 0.5410 - acc_top1: 0.8110 - acc_top5: 0.9932 - 52ms/step\n",
      "step 520/782 - loss: 0.5109 - acc_top1: 0.8105 - acc_top5: 0.9932 - 52ms/step\n",
      "step 530/782 - loss: 0.5807 - acc_top1: 0.8101 - acc_top5: 0.9932 - 52ms/step\n",
      "step 540/782 - loss: 0.7843 - acc_top1: 0.8096 - acc_top5: 0.9932 - 52ms/step\n",
      "step 550/782 - loss: 0.7755 - acc_top1: 0.8092 - acc_top5: 0.9933 - 52ms/step\n",
      "step 560/782 - loss: 0.5260 - acc_top1: 0.8090 - acc_top5: 0.9933 - 52ms/step\n",
      "step 570/782 - loss: 0.8796 - acc_top1: 0.8084 - acc_top5: 0.9933 - 52ms/step\n",
      "step 580/782 - loss: 0.4580 - acc_top1: 0.8083 - acc_top5: 0.9933 - 52ms/step\n",
      "step 590/782 - loss: 0.7274 - acc_top1: 0.8077 - acc_top5: 0.9931 - 52ms/step\n",
      "step 600/782 - loss: 0.6128 - acc_top1: 0.8073 - acc_top5: 0.9930 - 52ms/step\n",
      "step 610/782 - loss: 0.4974 - acc_top1: 0.8070 - acc_top5: 0.9931 - 52ms/step\n",
      "step 620/782 - loss: 0.4566 - acc_top1: 0.8073 - acc_top5: 0.9931 - 52ms/step\n",
      "step 630/782 - loss: 0.5587 - acc_top1: 0.8074 - acc_top5: 0.9931 - 52ms/step\n",
      "step 640/782 - loss: 0.4628 - acc_top1: 0.8070 - acc_top5: 0.9930 - 52ms/step\n",
      "step 650/782 - loss: 0.5767 - acc_top1: 0.8066 - acc_top5: 0.9929 - 52ms/step\n",
      "step 660/782 - loss: 0.7128 - acc_top1: 0.8061 - acc_top5: 0.9929 - 52ms/step\n",
      "step 670/782 - loss: 0.4475 - acc_top1: 0.8059 - acc_top5: 0.9928 - 52ms/step\n",
      "step 680/782 - loss: 0.5166 - acc_top1: 0.8057 - acc_top5: 0.9928 - 53ms/step\n",
      "step 690/782 - loss: 0.4072 - acc_top1: 0.8055 - acc_top5: 0.9928 - 53ms/step\n",
      "step 700/782 - loss: 0.7856 - acc_top1: 0.8054 - acc_top5: 0.9929 - 53ms/step\n",
      "step 710/782 - loss: 0.6183 - acc_top1: 0.8053 - acc_top5: 0.9929 - 53ms/step\n",
      "step 720/782 - loss: 0.8106 - acc_top1: 0.8049 - acc_top5: 0.9929 - 53ms/step\n",
      "step 730/782 - loss: 0.5864 - acc_top1: 0.8047 - acc_top5: 0.9929 - 53ms/step\n",
      "step 740/782 - loss: 0.4184 - acc_top1: 0.8046 - acc_top5: 0.9928 - 53ms/step\n",
      "step 750/782 - loss: 0.4477 - acc_top1: 0.8045 - acc_top5: 0.9928 - 53ms/step\n",
      "step 760/782 - loss: 0.6295 - acc_top1: 0.8047 - acc_top5: 0.9928 - 53ms/step\n",
      "step 770/782 - loss: 0.7916 - acc_top1: 0.8046 - acc_top5: 0.9928 - 53ms/step\n",
      "step 780/782 - loss: 0.5170 - acc_top1: 0.8044 - acc_top5: 0.9927 - 53ms/step\n",
      "step 782/782 - loss: 2.2140 - acc_top1: 0.8043 - acc_top5: 0.9927 - 53ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\24\n",
      "Eval begin...\n",
      "step  10/157 - loss: 0.7087 - acc_top1: 0.6281 - acc_top5: 0.9688 - 21ms/step\n",
      "step  20/157 - loss: 1.9724 - acc_top1: 0.6133 - acc_top5: 0.9656 - 22ms/step\n",
      "step  30/157 - loss: 1.3763 - acc_top1: 0.6130 - acc_top5: 0.9594 - 21ms/step\n",
      "step  40/157 - loss: 1.6965 - acc_top1: 0.6156 - acc_top5: 0.9594 - 21ms/step\n",
      "step  50/157 - loss: 1.1958 - acc_top1: 0.6166 - acc_top5: 0.9584 - 21ms/step\n",
      "step  60/157 - loss: 1.1631 - acc_top1: 0.6161 - acc_top5: 0.9563 - 21ms/step\n",
      "step  70/157 - loss: 2.0421 - acc_top1: 0.6147 - acc_top5: 0.9540 - 21ms/step\n",
      "step  80/157 - loss: 1.2680 - acc_top1: 0.6135 - acc_top5: 0.9533 - 21ms/step\n",
      "step  90/157 - loss: 1.1931 - acc_top1: 0.6139 - acc_top5: 0.9542 - 21ms/step\n",
      "step 100/157 - loss: 1.4418 - acc_top1: 0.6141 - acc_top5: 0.9537 - 21ms/step\n",
      "step 110/157 - loss: 1.9361 - acc_top1: 0.6118 - acc_top5: 0.9528 - 21ms/step\n",
      "step 120/157 - loss: 1.5708 - acc_top1: 0.6121 - acc_top5: 0.9523 - 21ms/step\n",
      "step 130/157 - loss: 1.4031 - acc_top1: 0.6136 - acc_top5: 0.9523 - 21ms/step\n",
      "step 140/157 - loss: 1.4800 - acc_top1: 0.6124 - acc_top5: 0.9516 - 21ms/step\n",
      "step 150/157 - loss: 1.7659 - acc_top1: 0.6131 - acc_top5: 0.9522 - 21ms/step\n",
      "step 157/157 - loss: 0.8541 - acc_top1: 0.6127 - acc_top5: 0.9518 - 21ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.5551 - acc_top1: 0.8125 - acc_top5: 0.9984 - 55ms/step\n",
      "step  20/782 - loss: 0.4236 - acc_top1: 0.8164 - acc_top5: 0.9977 - 55ms/step\n",
      "step  30/782 - loss: 0.3878 - acc_top1: 0.8182 - acc_top5: 0.9974 - 55ms/step\n",
      "step  40/782 - loss: 0.4754 - acc_top1: 0.8195 - acc_top5: 0.9969 - 56ms/step\n",
      "step  50/782 - loss: 0.5021 - acc_top1: 0.8206 - acc_top5: 0.9972 - 57ms/step\n",
      "step  60/782 - loss: 0.3079 - acc_top1: 0.8286 - acc_top5: 0.9966 - 57ms/step\n",
      "step  70/782 - loss: 0.3996 - acc_top1: 0.8313 - acc_top5: 0.9967 - 57ms/step\n",
      "step  80/782 - loss: 0.3590 - acc_top1: 0.8313 - acc_top5: 0.9961 - 56ms/step\n",
      "step  90/782 - loss: 0.4477 - acc_top1: 0.8337 - acc_top5: 0.9958 - 56ms/step\n",
      "step 100/782 - loss: 0.5378 - acc_top1: 0.8378 - acc_top5: 0.9961 - 56ms/step\n",
      "step 110/782 - loss: 0.5818 - acc_top1: 0.8372 - acc_top5: 0.9957 - 56ms/step\n",
      "step 120/782 - loss: 0.3950 - acc_top1: 0.8375 - acc_top5: 0.9957 - 55ms/step\n",
      "step 130/782 - loss: 0.5744 - acc_top1: 0.8364 - acc_top5: 0.9957 - 55ms/step\n",
      "step 140/782 - loss: 0.6186 - acc_top1: 0.8349 - acc_top5: 0.9958 - 55ms/step\n",
      "step 150/782 - loss: 0.5367 - acc_top1: 0.8348 - acc_top5: 0.9956 - 55ms/step\n",
      "step 160/782 - loss: 0.2690 - acc_top1: 0.8339 - acc_top5: 0.9956 - 55ms/step\n",
      "step 170/782 - loss: 0.5176 - acc_top1: 0.8348 - acc_top5: 0.9958 - 55ms/step\n",
      "step 180/782 - loss: 0.5142 - acc_top1: 0.8352 - acc_top5: 0.9959 - 55ms/step\n",
      "step 190/782 - loss: 0.3601 - acc_top1: 0.8346 - acc_top5: 0.9959 - 55ms/step\n",
      "step 200/782 - loss: 0.3763 - acc_top1: 0.8347 - acc_top5: 0.9958 - 55ms/step\n",
      "step 210/782 - loss: 0.2648 - acc_top1: 0.8350 - acc_top5: 0.9959 - 55ms/step\n",
      "step 220/782 - loss: 0.3298 - acc_top1: 0.8349 - acc_top5: 0.9957 - 55ms/step\n",
      "step 230/782 - loss: 0.3843 - acc_top1: 0.8338 - acc_top5: 0.9958 - 55ms/step\n",
      "step 240/782 - loss: 0.5882 - acc_top1: 0.8337 - acc_top5: 0.9958 - 55ms/step\n",
      "step 250/782 - loss: 0.3791 - acc_top1: 0.8327 - acc_top5: 0.9958 - 55ms/step\n",
      "step 260/782 - loss: 0.3739 - acc_top1: 0.8323 - acc_top5: 0.9960 - 55ms/step\n",
      "step 270/782 - loss: 0.5074 - acc_top1: 0.8316 - acc_top5: 0.9958 - 55ms/step\n",
      "step 280/782 - loss: 0.3287 - acc_top1: 0.8320 - acc_top5: 0.9958 - 55ms/step\n",
      "step 290/782 - loss: 0.3943 - acc_top1: 0.8317 - acc_top5: 0.9957 - 55ms/step\n",
      "step 300/782 - loss: 0.4502 - acc_top1: 0.8310 - acc_top5: 0.9956 - 55ms/step\n",
      "step 310/782 - loss: 0.6852 - acc_top1: 0.8309 - acc_top5: 0.9955 - 55ms/step\n",
      "step 320/782 - loss: 0.4976 - acc_top1: 0.8318 - acc_top5: 0.9956 - 55ms/step\n",
      "step 330/782 - loss: 0.8762 - acc_top1: 0.8315 - acc_top5: 0.9953 - 55ms/step\n",
      "step 340/782 - loss: 0.5792 - acc_top1: 0.8313 - acc_top5: 0.9951 - 55ms/step\n",
      "step 350/782 - loss: 0.5160 - acc_top1: 0.8315 - acc_top5: 0.9952 - 55ms/step\n",
      "step 360/782 - loss: 0.4794 - acc_top1: 0.8316 - acc_top5: 0.9952 - 55ms/step\n",
      "step 370/782 - loss: 0.4750 - acc_top1: 0.8314 - acc_top5: 0.9952 - 55ms/step\n",
      "step 380/782 - loss: 0.3287 - acc_top1: 0.8311 - acc_top5: 0.9952 - 55ms/step\n",
      "step 390/782 - loss: 0.6112 - acc_top1: 0.8315 - acc_top5: 0.9951 - 55ms/step\n",
      "step 400/782 - loss: 0.2896 - acc_top1: 0.8320 - acc_top5: 0.9951 - 55ms/step\n",
      "step 410/782 - loss: 0.5149 - acc_top1: 0.8320 - acc_top5: 0.9949 - 55ms/step\n",
      "step 420/782 - loss: 0.4356 - acc_top1: 0.8318 - acc_top5: 0.9948 - 55ms/step\n",
      "step 430/782 - loss: 0.5490 - acc_top1: 0.8323 - acc_top5: 0.9949 - 55ms/step\n",
      "step 440/782 - loss: 0.3669 - acc_top1: 0.8325 - acc_top5: 0.9949 - 55ms/step\n",
      "step 450/782 - loss: 0.6576 - acc_top1: 0.8326 - acc_top5: 0.9949 - 55ms/step\n",
      "step 460/782 - loss: 0.3350 - acc_top1: 0.8326 - acc_top5: 0.9950 - 55ms/step\n",
      "step 470/782 - loss: 0.4723 - acc_top1: 0.8326 - acc_top5: 0.9949 - 55ms/step\n",
      "step 480/782 - loss: 0.2862 - acc_top1: 0.8327 - acc_top5: 0.9949 - 55ms/step\n",
      "step 490/782 - loss: 0.7630 - acc_top1: 0.8329 - acc_top5: 0.9948 - 55ms/step\n",
      "step 500/782 - loss: 0.5575 - acc_top1: 0.8326 - acc_top5: 0.9948 - 55ms/step\n",
      "step 510/782 - loss: 0.4640 - acc_top1: 0.8321 - acc_top5: 0.9948 - 54ms/step\n",
      "step 520/782 - loss: 0.4506 - acc_top1: 0.8317 - acc_top5: 0.9948 - 54ms/step\n",
      "step 530/782 - loss: 0.5576 - acc_top1: 0.8311 - acc_top5: 0.9948 - 54ms/step\n",
      "step 540/782 - loss: 0.4412 - acc_top1: 0.8306 - acc_top5: 0.9948 - 54ms/step\n",
      "step 550/782 - loss: 0.4467 - acc_top1: 0.8297 - acc_top5: 0.9946 - 54ms/step\n",
      "step 560/782 - loss: 0.4371 - acc_top1: 0.8295 - acc_top5: 0.9946 - 54ms/step\n",
      "step 570/782 - loss: 0.3966 - acc_top1: 0.8290 - acc_top5: 0.9945 - 54ms/step\n",
      "step 580/782 - loss: 0.4332 - acc_top1: 0.8287 - acc_top5: 0.9945 - 54ms/step\n",
      "step 590/782 - loss: 0.4667 - acc_top1: 0.8282 - acc_top5: 0.9944 - 54ms/step\n",
      "step 600/782 - loss: 0.4147 - acc_top1: 0.8278 - acc_top5: 0.9945 - 54ms/step\n",
      "step 610/782 - loss: 0.4052 - acc_top1: 0.8276 - acc_top5: 0.9946 - 54ms/step\n",
      "step 620/782 - loss: 0.3365 - acc_top1: 0.8275 - acc_top5: 0.9946 - 54ms/step\n",
      "step 630/782 - loss: 0.6637 - acc_top1: 0.8269 - acc_top5: 0.9946 - 54ms/step\n",
      "step 640/782 - loss: 0.3905 - acc_top1: 0.8270 - acc_top5: 0.9946 - 54ms/step\n",
      "step 650/782 - loss: 0.5062 - acc_top1: 0.8268 - acc_top5: 0.9946 - 54ms/step\n",
      "step 660/782 - loss: 0.6851 - acc_top1: 0.8262 - acc_top5: 0.9946 - 54ms/step\n",
      "step 670/782 - loss: 0.3982 - acc_top1: 0.8263 - acc_top5: 0.9946 - 54ms/step\n",
      "step 680/782 - loss: 0.4360 - acc_top1: 0.8266 - acc_top5: 0.9946 - 54ms/step\n",
      "step 690/782 - loss: 0.4685 - acc_top1: 0.8266 - acc_top5: 0.9946 - 54ms/step\n",
      "step 700/782 - loss: 0.5579 - acc_top1: 0.8266 - acc_top5: 0.9946 - 54ms/step\n",
      "step 710/782 - loss: 0.4827 - acc_top1: 0.8263 - acc_top5: 0.9947 - 54ms/step\n",
      "step 720/782 - loss: 0.4808 - acc_top1: 0.8259 - acc_top5: 0.9946 - 54ms/step\n",
      "step 730/782 - loss: 0.5783 - acc_top1: 0.8259 - acc_top5: 0.9946 - 54ms/step\n",
      "step 740/782 - loss: 0.6120 - acc_top1: 0.8262 - acc_top5: 0.9946 - 54ms/step\n",
      "step 750/782 - loss: 0.4267 - acc_top1: 0.8259 - acc_top5: 0.9946 - 54ms/step\n",
      "step 760/782 - loss: 0.7298 - acc_top1: 0.8260 - acc_top5: 0.9946 - 54ms/step\n",
      "step 770/782 - loss: 0.5069 - acc_top1: 0.8256 - acc_top5: 0.9946 - 54ms/step\n",
      "step 780/782 - loss: 0.8755 - acc_top1: 0.8252 - acc_top5: 0.9945 - 54ms/step\n",
      "step 782/782 - loss: 0.9836 - acc_top1: 0.8252 - acc_top5: 0.9945 - 54ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\25\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.0109 - acc_top1: 0.6094 - acc_top5: 0.9563 - 18ms/step\n",
      "step  20/157 - loss: 2.1783 - acc_top1: 0.6031 - acc_top5: 0.9609 - 19ms/step\n",
      "step  30/157 - loss: 1.4569 - acc_top1: 0.6115 - acc_top5: 0.9557 - 19ms/step\n",
      "step  40/157 - loss: 1.9058 - acc_top1: 0.6172 - acc_top5: 0.9555 - 19ms/step\n",
      "step  50/157 - loss: 1.2052 - acc_top1: 0.6228 - acc_top5: 0.9547 - 19ms/step\n",
      "step  60/157 - loss: 1.3358 - acc_top1: 0.6208 - acc_top5: 0.9503 - 19ms/step\n",
      "step  70/157 - loss: 1.2900 - acc_top1: 0.6192 - acc_top5: 0.9509 - 19ms/step\n",
      "step  80/157 - loss: 1.5203 - acc_top1: 0.6164 - acc_top5: 0.9506 - 19ms/step\n",
      "step  90/157 - loss: 0.9270 - acc_top1: 0.6146 - acc_top5: 0.9510 - 19ms/step\n",
      "step 100/157 - loss: 1.4192 - acc_top1: 0.6125 - acc_top5: 0.9517 - 19ms/step\n",
      "step 110/157 - loss: 1.8473 - acc_top1: 0.6107 - acc_top5: 0.9510 - 19ms/step\n",
      "step 120/157 - loss: 1.6889 - acc_top1: 0.6091 - acc_top5: 0.9507 - 19ms/step\n",
      "step 130/157 - loss: 1.4790 - acc_top1: 0.6091 - acc_top5: 0.9513 - 19ms/step\n",
      "step 140/157 - loss: 1.4765 - acc_top1: 0.6075 - acc_top5: 0.9506 - 19ms/step\n",
      "step 150/157 - loss: 1.1010 - acc_top1: 0.6062 - acc_top5: 0.9505 - 19ms/step\n",
      "step 157/157 - loss: 1.0296 - acc_top1: 0.6052 - acc_top5: 0.9509 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.4435 - acc_top1: 0.8250 - acc_top5: 0.9984 - 51ms/step\n",
      "step  20/782 - loss: 0.6335 - acc_top1: 0.8195 - acc_top5: 0.9969 - 51ms/step\n",
      "step  30/782 - loss: 0.3623 - acc_top1: 0.8292 - acc_top5: 0.9958 - 51ms/step\n",
      "step  40/782 - loss: 0.4350 - acc_top1: 0.8340 - acc_top5: 0.9961 - 52ms/step\n",
      "step  50/782 - loss: 0.3417 - acc_top1: 0.8394 - acc_top5: 0.9966 - 53ms/step\n",
      "step  60/782 - loss: 0.3520 - acc_top1: 0.8411 - acc_top5: 0.9971 - 53ms/step\n",
      "step  70/782 - loss: 0.3792 - acc_top1: 0.8431 - acc_top5: 0.9973 - 52ms/step\n",
      "step  80/782 - loss: 0.3999 - acc_top1: 0.8438 - acc_top5: 0.9973 - 52ms/step\n",
      "step  90/782 - loss: 0.5042 - acc_top1: 0.8484 - acc_top5: 0.9974 - 52ms/step\n",
      "step 100/782 - loss: 0.2306 - acc_top1: 0.8528 - acc_top5: 0.9973 - 52ms/step\n",
      "step 110/782 - loss: 0.5819 - acc_top1: 0.8536 - acc_top5: 0.9972 - 52ms/step\n",
      "step 120/782 - loss: 0.3013 - acc_top1: 0.8565 - acc_top5: 0.9970 - 52ms/step\n",
      "step 130/782 - loss: 0.2786 - acc_top1: 0.8560 - acc_top5: 0.9968 - 52ms/step\n",
      "step 140/782 - loss: 0.3076 - acc_top1: 0.8568 - acc_top5: 0.9968 - 51ms/step\n",
      "step 150/782 - loss: 0.4870 - acc_top1: 0.8553 - acc_top5: 0.9967 - 52ms/step\n",
      "step 160/782 - loss: 0.4654 - acc_top1: 0.8562 - acc_top5: 0.9966 - 52ms/step\n",
      "step 170/782 - loss: 0.4089 - acc_top1: 0.8571 - acc_top5: 0.9965 - 51ms/step\n",
      "step 180/782 - loss: 0.3192 - acc_top1: 0.8578 - acc_top5: 0.9965 - 51ms/step\n",
      "step 190/782 - loss: 0.5508 - acc_top1: 0.8580 - acc_top5: 0.9963 - 51ms/step\n",
      "step 200/782 - loss: 0.4100 - acc_top1: 0.8579 - acc_top5: 0.9965 - 51ms/step\n",
      "step 210/782 - loss: 0.2809 - acc_top1: 0.8590 - acc_top5: 0.9965 - 51ms/step\n",
      "step 220/782 - loss: 0.4641 - acc_top1: 0.8584 - acc_top5: 0.9964 - 51ms/step\n",
      "step 230/782 - loss: 0.4322 - acc_top1: 0.8582 - acc_top5: 0.9961 - 51ms/step\n",
      "step 240/782 - loss: 0.4025 - acc_top1: 0.8586 - acc_top5: 0.9962 - 51ms/step\n",
      "step 250/782 - loss: 0.3224 - acc_top1: 0.8579 - acc_top5: 0.9961 - 51ms/step\n",
      "step 260/782 - loss: 0.2787 - acc_top1: 0.8583 - acc_top5: 0.9961 - 51ms/step\n",
      "step 270/782 - loss: 0.4234 - acc_top1: 0.8583 - acc_top5: 0.9960 - 51ms/step\n",
      "step 280/782 - loss: 0.6024 - acc_top1: 0.8581 - acc_top5: 0.9960 - 51ms/step\n",
      "step 290/782 - loss: 0.4556 - acc_top1: 0.8573 - acc_top5: 0.9960 - 51ms/step\n",
      "step 300/782 - loss: 0.4148 - acc_top1: 0.8566 - acc_top5: 0.9960 - 51ms/step\n",
      "step 310/782 - loss: 0.6639 - acc_top1: 0.8560 - acc_top5: 0.9960 - 51ms/step\n",
      "step 320/782 - loss: 0.4702 - acc_top1: 0.8553 - acc_top5: 0.9958 - 51ms/step\n",
      "step 330/782 - loss: 0.4830 - acc_top1: 0.8551 - acc_top5: 0.9958 - 51ms/step\n",
      "step 340/782 - loss: 0.3644 - acc_top1: 0.8558 - acc_top5: 0.9959 - 51ms/step\n",
      "step 350/782 - loss: 0.4574 - acc_top1: 0.8562 - acc_top5: 0.9958 - 51ms/step\n",
      "step 360/782 - loss: 0.4106 - acc_top1: 0.8569 - acc_top5: 0.9958 - 51ms/step\n",
      "step 370/782 - loss: 0.3071 - acc_top1: 0.8561 - acc_top5: 0.9959 - 51ms/step\n",
      "step 380/782 - loss: 0.3250 - acc_top1: 0.8558 - acc_top5: 0.9957 - 51ms/step\n",
      "step 390/782 - loss: 0.2484 - acc_top1: 0.8554 - acc_top5: 0.9958 - 51ms/step\n",
      "step 400/782 - loss: 0.4745 - acc_top1: 0.8548 - acc_top5: 0.9956 - 51ms/step\n",
      "step 410/782 - loss: 0.5055 - acc_top1: 0.8545 - acc_top5: 0.9957 - 52ms/step\n",
      "step 420/782 - loss: 0.5937 - acc_top1: 0.8538 - acc_top5: 0.9955 - 51ms/step\n",
      "step 430/782 - loss: 0.4983 - acc_top1: 0.8536 - acc_top5: 0.9955 - 51ms/step\n",
      "step 440/782 - loss: 0.8668 - acc_top1: 0.8523 - acc_top5: 0.9952 - 52ms/step\n",
      "step 450/782 - loss: 0.5218 - acc_top1: 0.8515 - acc_top5: 0.9951 - 52ms/step\n",
      "step 460/782 - loss: 0.4224 - acc_top1: 0.8513 - acc_top5: 0.9950 - 52ms/step\n",
      "step 470/782 - loss: 0.6791 - acc_top1: 0.8507 - acc_top5: 0.9951 - 52ms/step\n",
      "step 480/782 - loss: 0.6159 - acc_top1: 0.8503 - acc_top5: 0.9951 - 52ms/step\n",
      "step 490/782 - loss: 0.4202 - acc_top1: 0.8493 - acc_top5: 0.9952 - 52ms/step\n",
      "step 500/782 - loss: 0.4930 - acc_top1: 0.8493 - acc_top5: 0.9952 - 52ms/step\n",
      "step 510/782 - loss: 0.5135 - acc_top1: 0.8488 - acc_top5: 0.9951 - 52ms/step\n",
      "step 520/782 - loss: 0.5559 - acc_top1: 0.8483 - acc_top5: 0.9952 - 52ms/step\n",
      "step 530/782 - loss: 0.4515 - acc_top1: 0.8481 - acc_top5: 0.9953 - 52ms/step\n",
      "step 540/782 - loss: 0.5341 - acc_top1: 0.8476 - acc_top5: 0.9953 - 52ms/step\n",
      "step 550/782 - loss: 0.3668 - acc_top1: 0.8475 - acc_top5: 0.9953 - 52ms/step\n",
      "step 560/782 - loss: 0.5285 - acc_top1: 0.8470 - acc_top5: 0.9952 - 52ms/step\n",
      "step 570/782 - loss: 0.8568 - acc_top1: 0.8461 - acc_top5: 0.9951 - 52ms/step\n",
      "step 580/782 - loss: 0.4073 - acc_top1: 0.8455 - acc_top5: 0.9951 - 52ms/step\n",
      "step 590/782 - loss: 0.4395 - acc_top1: 0.8451 - acc_top5: 0.9952 - 52ms/step\n",
      "step 600/782 - loss: 0.5229 - acc_top1: 0.8447 - acc_top5: 0.9951 - 52ms/step\n",
      "step 610/782 - loss: 0.6074 - acc_top1: 0.8442 - acc_top5: 0.9951 - 52ms/step\n",
      "step 620/782 - loss: 0.7308 - acc_top1: 0.8437 - acc_top5: 0.9951 - 52ms/step\n",
      "step 630/782 - loss: 0.4239 - acc_top1: 0.8431 - acc_top5: 0.9950 - 52ms/step\n",
      "step 640/782 - loss: 0.4146 - acc_top1: 0.8434 - acc_top5: 0.9950 - 52ms/step\n",
      "step 650/782 - loss: 0.4167 - acc_top1: 0.8431 - acc_top5: 0.9950 - 52ms/step\n",
      "step 660/782 - loss: 0.3980 - acc_top1: 0.8425 - acc_top5: 0.9951 - 52ms/step\n",
      "step 670/782 - loss: 0.5367 - acc_top1: 0.8423 - acc_top5: 0.9951 - 52ms/step\n",
      "step 680/782 - loss: 0.4017 - acc_top1: 0.8418 - acc_top5: 0.9951 - 52ms/step\n",
      "step 690/782 - loss: 0.3847 - acc_top1: 0.8418 - acc_top5: 0.9951 - 52ms/step\n",
      "step 700/782 - loss: 0.5280 - acc_top1: 0.8412 - acc_top5: 0.9949 - 52ms/step\n",
      "step 710/782 - loss: 0.5158 - acc_top1: 0.8410 - acc_top5: 0.9949 - 52ms/step\n",
      "step 720/782 - loss: 0.4275 - acc_top1: 0.8407 - acc_top5: 0.9948 - 52ms/step\n",
      "step 730/782 - loss: 0.3149 - acc_top1: 0.8405 - acc_top5: 0.9949 - 52ms/step\n",
      "step 740/782 - loss: 0.5101 - acc_top1: 0.8400 - acc_top5: 0.9948 - 52ms/step\n",
      "step 750/782 - loss: 0.4982 - acc_top1: 0.8399 - acc_top5: 0.9948 - 52ms/step\n",
      "step 760/782 - loss: 0.5941 - acc_top1: 0.8399 - acc_top5: 0.9948 - 52ms/step\n",
      "step 770/782 - loss: 0.3297 - acc_top1: 0.8397 - acc_top5: 0.9948 - 52ms/step\n",
      "step 780/782 - loss: 0.6146 - acc_top1: 0.8396 - acc_top5: 0.9948 - 52ms/step\n",
      "step 782/782 - loss: 1.0929 - acc_top1: 0.8396 - acc_top5: 0.9948 - 52ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\26\n",
      "Eval begin...\n",
      "step  10/157 - loss: 0.9987 - acc_top1: 0.6062 - acc_top5: 0.9625 - 20ms/step\n",
      "step  20/157 - loss: 1.9877 - acc_top1: 0.6156 - acc_top5: 0.9594 - 20ms/step\n",
      "step  30/157 - loss: 1.2806 - acc_top1: 0.6219 - acc_top5: 0.9552 - 20ms/step\n",
      "step  40/157 - loss: 1.8616 - acc_top1: 0.6195 - acc_top5: 0.9539 - 20ms/step\n",
      "step  50/157 - loss: 1.6083 - acc_top1: 0.6191 - acc_top5: 0.9516 - 20ms/step\n",
      "step  60/157 - loss: 1.0517 - acc_top1: 0.6201 - acc_top5: 0.9495 - 20ms/step\n",
      "step  70/157 - loss: 1.9925 - acc_top1: 0.6183 - acc_top5: 0.9478 - 20ms/step\n",
      "step  80/157 - loss: 1.4238 - acc_top1: 0.6197 - acc_top5: 0.9486 - 20ms/step\n",
      "step  90/157 - loss: 1.0612 - acc_top1: 0.6165 - acc_top5: 0.9498 - 19ms/step\n",
      "step 100/157 - loss: 1.4591 - acc_top1: 0.6158 - acc_top5: 0.9506 - 19ms/step\n",
      "step 110/157 - loss: 1.9921 - acc_top1: 0.6114 - acc_top5: 0.9500 - 19ms/step\n",
      "step 120/157 - loss: 1.7459 - acc_top1: 0.6102 - acc_top5: 0.9495 - 19ms/step\n",
      "step 130/157 - loss: 1.7015 - acc_top1: 0.6091 - acc_top5: 0.9502 - 19ms/step\n",
      "step 140/157 - loss: 1.3638 - acc_top1: 0.6094 - acc_top5: 0.9501 - 19ms/step\n",
      "step 150/157 - loss: 1.4374 - acc_top1: 0.6091 - acc_top5: 0.9501 - 19ms/step\n",
      "step 157/157 - loss: 0.5958 - acc_top1: 0.6081 - acc_top5: 0.9500 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.5087 - acc_top1: 0.8609 - acc_top5: 0.9984 - 51ms/step\n",
      "step  20/782 - loss: 0.3662 - acc_top1: 0.8594 - acc_top5: 0.9969 - 51ms/step\n",
      "step  30/782 - loss: 0.3569 - acc_top1: 0.8604 - acc_top5: 0.9974 - 50ms/step\n",
      "step  40/782 - loss: 0.4542 - acc_top1: 0.8633 - acc_top5: 0.9969 - 50ms/step\n",
      "step  50/782 - loss: 0.5030 - acc_top1: 0.8678 - acc_top5: 0.9972 - 50ms/step\n",
      "step  60/782 - loss: 0.3493 - acc_top1: 0.8688 - acc_top5: 0.9974 - 50ms/step\n",
      "step  70/782 - loss: 0.4781 - acc_top1: 0.8723 - acc_top5: 0.9969 - 50ms/step\n",
      "step  80/782 - loss: 0.3294 - acc_top1: 0.8725 - acc_top5: 0.9971 - 50ms/step\n",
      "step  90/782 - loss: 0.2035 - acc_top1: 0.8745 - acc_top5: 0.9970 - 50ms/step\n",
      "step 100/782 - loss: 0.3702 - acc_top1: 0.8752 - acc_top5: 0.9969 - 50ms/step\n",
      "step 110/782 - loss: 0.2214 - acc_top1: 0.8756 - acc_top5: 0.9966 - 50ms/step\n",
      "step 120/782 - loss: 0.4474 - acc_top1: 0.8754 - acc_top5: 0.9966 - 50ms/step\n",
      "step 130/782 - loss: 0.3695 - acc_top1: 0.8760 - acc_top5: 0.9966 - 50ms/step\n",
      "step 140/782 - loss: 0.3306 - acc_top1: 0.8779 - acc_top5: 0.9968 - 50ms/step\n",
      "step 150/782 - loss: 0.3384 - acc_top1: 0.8786 - acc_top5: 0.9965 - 50ms/step\n",
      "step 160/782 - loss: 0.2362 - acc_top1: 0.8789 - acc_top5: 0.9967 - 50ms/step\n",
      "step 170/782 - loss: 0.2618 - acc_top1: 0.8784 - acc_top5: 0.9967 - 51ms/step\n",
      "step 180/782 - loss: 0.6417 - acc_top1: 0.8774 - acc_top5: 0.9964 - 51ms/step\n",
      "step 190/782 - loss: 0.2535 - acc_top1: 0.8771 - acc_top5: 0.9964 - 51ms/step\n",
      "step 200/782 - loss: 0.2798 - acc_top1: 0.8750 - acc_top5: 0.9963 - 51ms/step\n",
      "step 210/782 - loss: 0.3754 - acc_top1: 0.8751 - acc_top5: 0.9963 - 51ms/step\n",
      "step 220/782 - loss: 0.3251 - acc_top1: 0.8745 - acc_top5: 0.9962 - 51ms/step\n",
      "step 230/782 - loss: 0.4469 - acc_top1: 0.8747 - acc_top5: 0.9963 - 51ms/step\n",
      "step 240/782 - loss: 0.2488 - acc_top1: 0.8736 - acc_top5: 0.9962 - 51ms/step\n",
      "step 250/782 - loss: 0.2320 - acc_top1: 0.8739 - acc_top5: 0.9963 - 51ms/step\n",
      "step 260/782 - loss: 0.3776 - acc_top1: 0.8736 - acc_top5: 0.9963 - 51ms/step\n",
      "step 270/782 - loss: 0.4569 - acc_top1: 0.8739 - acc_top5: 0.9964 - 51ms/step\n",
      "step 280/782 - loss: 0.4700 - acc_top1: 0.8739 - acc_top5: 0.9963 - 51ms/step\n",
      "step 290/782 - loss: 0.2365 - acc_top1: 0.8740 - acc_top5: 0.9963 - 51ms/step\n",
      "step 300/782 - loss: 0.4474 - acc_top1: 0.8733 - acc_top5: 0.9963 - 51ms/step\n",
      "step 310/782 - loss: 0.3198 - acc_top1: 0.8734 - acc_top5: 0.9963 - 51ms/step\n",
      "step 320/782 - loss: 0.3725 - acc_top1: 0.8733 - acc_top5: 0.9962 - 51ms/step\n",
      "step 330/782 - loss: 0.5595 - acc_top1: 0.8732 - acc_top5: 0.9962 - 51ms/step\n",
      "step 340/782 - loss: 0.4039 - acc_top1: 0.8723 - acc_top5: 0.9961 - 51ms/step\n",
      "step 350/782 - loss: 0.2901 - acc_top1: 0.8727 - acc_top5: 0.9961 - 51ms/step\n",
      "step 360/782 - loss: 0.2412 - acc_top1: 0.8718 - acc_top5: 0.9960 - 51ms/step\n",
      "step 370/782 - loss: 0.3530 - acc_top1: 0.8715 - acc_top5: 0.9960 - 51ms/step\n",
      "step 380/782 - loss: 0.6196 - acc_top1: 0.8713 - acc_top5: 0.9960 - 51ms/step\n",
      "step 390/782 - loss: 0.3236 - acc_top1: 0.8709 - acc_top5: 0.9960 - 51ms/step\n",
      "step 400/782 - loss: 0.4021 - acc_top1: 0.8707 - acc_top5: 0.9959 - 51ms/step\n",
      "step 410/782 - loss: 0.3760 - acc_top1: 0.8705 - acc_top5: 0.9959 - 51ms/step\n",
      "step 420/782 - loss: 0.4626 - acc_top1: 0.8699 - acc_top5: 0.9960 - 51ms/step\n",
      "step 430/782 - loss: 0.3993 - acc_top1: 0.8696 - acc_top5: 0.9961 - 51ms/step\n",
      "step 440/782 - loss: 0.3730 - acc_top1: 0.8686 - acc_top5: 0.9960 - 51ms/step\n",
      "step 450/782 - loss: 0.3731 - acc_top1: 0.8684 - acc_top5: 0.9959 - 51ms/step\n",
      "step 460/782 - loss: 0.2047 - acc_top1: 0.8682 - acc_top5: 0.9959 - 51ms/step\n",
      "step 470/782 - loss: 0.4504 - acc_top1: 0.8679 - acc_top5: 0.9959 - 51ms/step\n",
      "step 480/782 - loss: 0.2326 - acc_top1: 0.8676 - acc_top5: 0.9960 - 51ms/step\n",
      "step 490/782 - loss: 0.3506 - acc_top1: 0.8678 - acc_top5: 0.9960 - 51ms/step\n",
      "step 500/782 - loss: 0.3739 - acc_top1: 0.8678 - acc_top5: 0.9960 - 51ms/step\n",
      "step 510/782 - loss: 0.4858 - acc_top1: 0.8677 - acc_top5: 0.9961 - 51ms/step\n",
      "step 520/782 - loss: 0.3408 - acc_top1: 0.8673 - acc_top5: 0.9962 - 51ms/step\n",
      "step 530/782 - loss: 0.3498 - acc_top1: 0.8670 - acc_top5: 0.9962 - 51ms/step\n",
      "step 540/782 - loss: 0.5373 - acc_top1: 0.8663 - acc_top5: 0.9962 - 51ms/step\n",
      "step 550/782 - loss: 0.4130 - acc_top1: 0.8660 - acc_top5: 0.9962 - 51ms/step\n",
      "step 560/782 - loss: 0.2355 - acc_top1: 0.8660 - acc_top5: 0.9962 - 51ms/step\n",
      "step 570/782 - loss: 0.3459 - acc_top1: 0.8664 - acc_top5: 0.9962 - 51ms/step\n",
      "step 580/782 - loss: 0.3053 - acc_top1: 0.8665 - acc_top5: 0.9961 - 51ms/step\n",
      "step 590/782 - loss: 0.4428 - acc_top1: 0.8660 - acc_top5: 0.9962 - 51ms/step\n",
      "step 600/782 - loss: 0.3568 - acc_top1: 0.8658 - acc_top5: 0.9962 - 51ms/step\n",
      "step 610/782 - loss: 0.6427 - acc_top1: 0.8658 - acc_top5: 0.9962 - 51ms/step\n",
      "step 620/782 - loss: 0.3700 - acc_top1: 0.8660 - acc_top5: 0.9962 - 51ms/step\n",
      "step 630/782 - loss: 0.4623 - acc_top1: 0.8655 - acc_top5: 0.9962 - 51ms/step\n",
      "step 640/782 - loss: 0.4903 - acc_top1: 0.8655 - acc_top5: 0.9962 - 51ms/step\n",
      "step 650/782 - loss: 0.2940 - acc_top1: 0.8654 - acc_top5: 0.9962 - 51ms/step\n",
      "step 660/782 - loss: 0.4860 - acc_top1: 0.8649 - acc_top5: 0.9962 - 51ms/step\n",
      "step 670/782 - loss: 0.3667 - acc_top1: 0.8651 - acc_top5: 0.9962 - 51ms/step\n",
      "step 680/782 - loss: 0.4367 - acc_top1: 0.8651 - acc_top5: 0.9961 - 51ms/step\n",
      "step 690/782 - loss: 0.5816 - acc_top1: 0.8649 - acc_top5: 0.9961 - 51ms/step\n",
      "step 700/782 - loss: 0.2819 - acc_top1: 0.8647 - acc_top5: 0.9962 - 51ms/step\n",
      "step 710/782 - loss: 0.8505 - acc_top1: 0.8640 - acc_top5: 0.9961 - 51ms/step\n",
      "step 720/782 - loss: 0.5370 - acc_top1: 0.8636 - acc_top5: 0.9962 - 51ms/step\n",
      "step 730/782 - loss: 0.4258 - acc_top1: 0.8632 - acc_top5: 0.9961 - 51ms/step\n",
      "step 740/782 - loss: 0.4658 - acc_top1: 0.8635 - acc_top5: 0.9962 - 51ms/step\n",
      "step 750/782 - loss: 0.4247 - acc_top1: 0.8631 - acc_top5: 0.9962 - 51ms/step\n",
      "step 760/782 - loss: 0.2589 - acc_top1: 0.8628 - acc_top5: 0.9962 - 51ms/step\n",
      "step 770/782 - loss: 0.5211 - acc_top1: 0.8629 - acc_top5: 0.9962 - 51ms/step\n",
      "step 780/782 - loss: 0.5140 - acc_top1: 0.8625 - acc_top5: 0.9962 - 51ms/step\n",
      "step 782/782 - loss: 0.9144 - acc_top1: 0.8624 - acc_top5: 0.9962 - 51ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\27\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.1800 - acc_top1: 0.6203 - acc_top5: 0.9625 - 20ms/step\n",
      "step  20/157 - loss: 2.0280 - acc_top1: 0.6336 - acc_top5: 0.9641 - 19ms/step\n",
      "step  30/157 - loss: 1.5703 - acc_top1: 0.6266 - acc_top5: 0.9635 - 19ms/step\n",
      "step  40/157 - loss: 2.0105 - acc_top1: 0.6262 - acc_top5: 0.9590 - 19ms/step\n",
      "step  50/157 - loss: 1.1164 - acc_top1: 0.6269 - acc_top5: 0.9584 - 19ms/step\n",
      "step  60/157 - loss: 1.3532 - acc_top1: 0.6263 - acc_top5: 0.9557 - 19ms/step\n",
      "step  70/157 - loss: 2.7484 - acc_top1: 0.6254 - acc_top5: 0.9549 - 19ms/step\n",
      "step  80/157 - loss: 1.2625 - acc_top1: 0.6242 - acc_top5: 0.9553 - 19ms/step\n",
      "step  90/157 - loss: 1.1532 - acc_top1: 0.6226 - acc_top5: 0.9547 - 19ms/step\n",
      "step 100/157 - loss: 1.6118 - acc_top1: 0.6217 - acc_top5: 0.9542 - 19ms/step\n",
      "step 110/157 - loss: 1.9894 - acc_top1: 0.6197 - acc_top5: 0.9548 - 19ms/step\n",
      "step 120/157 - loss: 1.6215 - acc_top1: 0.6193 - acc_top5: 0.9547 - 19ms/step\n",
      "step 130/157 - loss: 1.7914 - acc_top1: 0.6189 - acc_top5: 0.9542 - 19ms/step\n",
      "step 140/157 - loss: 1.9416 - acc_top1: 0.6161 - acc_top5: 0.9537 - 19ms/step\n",
      "step 150/157 - loss: 5.2012 - acc_top1: 0.6149 - acc_top5: 0.9540 - 19ms/step\n",
      "step 157/157 - loss: 0.6311 - acc_top1: 0.6136 - acc_top5: 0.9540 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.6146 - acc_top1: 0.8891 - acc_top5: 0.9938 - 53ms/step\n",
      "step  20/782 - loss: 0.6287 - acc_top1: 0.8820 - acc_top5: 0.9961 - 51ms/step\n",
      "step  30/782 - loss: 0.2167 - acc_top1: 0.8823 - acc_top5: 0.9969 - 51ms/step\n",
      "step  40/782 - loss: 0.3644 - acc_top1: 0.8793 - acc_top5: 0.9965 - 51ms/step\n",
      "step  50/782 - loss: 0.2949 - acc_top1: 0.8831 - acc_top5: 0.9966 - 51ms/step\n",
      "step  60/782 - loss: 0.2016 - acc_top1: 0.8833 - acc_top5: 0.9964 - 51ms/step\n",
      "step  70/782 - loss: 0.2667 - acc_top1: 0.8846 - acc_top5: 0.9960 - 51ms/step\n",
      "step  80/782 - loss: 0.3386 - acc_top1: 0.8852 - acc_top5: 0.9959 - 50ms/step\n",
      "step  90/782 - loss: 0.2864 - acc_top1: 0.8880 - acc_top5: 0.9964 - 50ms/step\n",
      "step 100/782 - loss: 0.3181 - acc_top1: 0.8888 - acc_top5: 0.9964 - 50ms/step\n",
      "step 110/782 - loss: 0.5490 - acc_top1: 0.8895 - acc_top5: 0.9966 - 50ms/step\n",
      "step 120/782 - loss: 0.2734 - acc_top1: 0.8883 - acc_top5: 0.9965 - 50ms/step\n",
      "step 130/782 - loss: 0.2284 - acc_top1: 0.8882 - acc_top5: 0.9964 - 50ms/step\n",
      "step 140/782 - loss: 0.5683 - acc_top1: 0.8877 - acc_top5: 0.9962 - 50ms/step\n",
      "step 150/782 - loss: 0.2423 - acc_top1: 0.8880 - acc_top5: 0.9964 - 51ms/step\n",
      "step 160/782 - loss: 0.3855 - acc_top1: 0.8888 - acc_top5: 0.9965 - 51ms/step\n",
      "step 170/782 - loss: 0.2724 - acc_top1: 0.8898 - acc_top5: 0.9966 - 51ms/step\n",
      "step 180/782 - loss: 0.3151 - acc_top1: 0.8891 - acc_top5: 0.9967 - 51ms/step\n",
      "step 190/782 - loss: 0.3424 - acc_top1: 0.8885 - acc_top5: 0.9965 - 51ms/step\n",
      "step 200/782 - loss: 0.6154 - acc_top1: 0.8878 - acc_top5: 0.9965 - 51ms/step\n",
      "step 210/782 - loss: 0.4362 - acc_top1: 0.8872 - acc_top5: 0.9964 - 51ms/step\n",
      "step 220/782 - loss: 0.4279 - acc_top1: 0.8864 - acc_top5: 0.9965 - 51ms/step\n",
      "step 230/782 - loss: 0.3744 - acc_top1: 0.8853 - acc_top5: 0.9964 - 51ms/step\n",
      "step 240/782 - loss: 0.3125 - acc_top1: 0.8854 - acc_top5: 0.9965 - 51ms/step\n",
      "step 250/782 - loss: 0.3511 - acc_top1: 0.8857 - acc_top5: 0.9964 - 51ms/step\n",
      "step 260/782 - loss: 0.3079 - acc_top1: 0.8859 - acc_top5: 0.9963 - 51ms/step\n",
      "step 270/782 - loss: 0.3171 - acc_top1: 0.8863 - acc_top5: 0.9964 - 51ms/step\n",
      "step 280/782 - loss: 0.2805 - acc_top1: 0.8864 - acc_top5: 0.9963 - 51ms/step\n",
      "step 290/782 - loss: 0.2025 - acc_top1: 0.8867 - acc_top5: 0.9963 - 51ms/step\n",
      "step 300/782 - loss: 0.2510 - acc_top1: 0.8867 - acc_top5: 0.9965 - 51ms/step\n",
      "step 310/782 - loss: 0.2138 - acc_top1: 0.8867 - acc_top5: 0.9965 - 51ms/step\n",
      "step 320/782 - loss: 0.4323 - acc_top1: 0.8868 - acc_top5: 0.9965 - 51ms/step\n",
      "step 330/782 - loss: 0.3481 - acc_top1: 0.8868 - acc_top5: 0.9964 - 51ms/step\n",
      "step 340/782 - loss: 0.5186 - acc_top1: 0.8859 - acc_top5: 0.9965 - 51ms/step\n",
      "step 350/782 - loss: 0.2734 - acc_top1: 0.8858 - acc_top5: 0.9965 - 51ms/step\n",
      "step 360/782 - loss: 0.3889 - acc_top1: 0.8854 - acc_top5: 0.9966 - 51ms/step\n",
      "step 370/782 - loss: 0.4991 - acc_top1: 0.8855 - acc_top5: 0.9965 - 51ms/step\n",
      "step 380/782 - loss: 0.3086 - acc_top1: 0.8856 - acc_top5: 0.9966 - 51ms/step\n",
      "step 390/782 - loss: 0.3130 - acc_top1: 0.8856 - acc_top5: 0.9966 - 51ms/step\n",
      "step 400/782 - loss: 0.4472 - acc_top1: 0.8857 - acc_top5: 0.9965 - 51ms/step\n",
      "step 410/782 - loss: 0.3675 - acc_top1: 0.8850 - acc_top5: 0.9965 - 51ms/step\n",
      "step 420/782 - loss: 0.3684 - acc_top1: 0.8846 - acc_top5: 0.9966 - 51ms/step\n",
      "step 430/782 - loss: 0.2594 - acc_top1: 0.8842 - acc_top5: 0.9965 - 51ms/step\n",
      "step 440/782 - loss: 0.4518 - acc_top1: 0.8836 - acc_top5: 0.9964 - 51ms/step\n",
      "step 450/782 - loss: 0.3957 - acc_top1: 0.8839 - acc_top5: 0.9964 - 51ms/step\n",
      "step 460/782 - loss: 0.3324 - acc_top1: 0.8840 - acc_top5: 0.9964 - 51ms/step\n",
      "step 470/782 - loss: 0.4054 - acc_top1: 0.8837 - acc_top5: 0.9963 - 51ms/step\n",
      "step 480/782 - loss: 0.4390 - acc_top1: 0.8829 - acc_top5: 0.9961 - 51ms/step\n",
      "step 490/782 - loss: 0.3026 - acc_top1: 0.8828 - acc_top5: 0.9961 - 51ms/step\n",
      "step 500/782 - loss: 0.5396 - acc_top1: 0.8824 - acc_top5: 0.9962 - 51ms/step\n",
      "step 510/782 - loss: 0.3230 - acc_top1: 0.8824 - acc_top5: 0.9962 - 51ms/step\n",
      "step 520/782 - loss: 0.1853 - acc_top1: 0.8823 - acc_top5: 0.9962 - 51ms/step\n",
      "step 530/782 - loss: 0.4128 - acc_top1: 0.8824 - acc_top5: 0.9962 - 51ms/step\n",
      "step 540/782 - loss: 0.5513 - acc_top1: 0.8819 - acc_top5: 0.9962 - 51ms/step\n",
      "step 550/782 - loss: 0.4297 - acc_top1: 0.8813 - acc_top5: 0.9963 - 51ms/step\n",
      "step 560/782 - loss: 0.4324 - acc_top1: 0.8812 - acc_top5: 0.9963 - 51ms/step\n",
      "step 570/782 - loss: 0.6130 - acc_top1: 0.8811 - acc_top5: 0.9964 - 51ms/step\n",
      "step 580/782 - loss: 0.3260 - acc_top1: 0.8805 - acc_top5: 0.9964 - 51ms/step\n",
      "step 590/782 - loss: 0.3735 - acc_top1: 0.8802 - acc_top5: 0.9963 - 51ms/step\n",
      "step 600/782 - loss: 0.5892 - acc_top1: 0.8802 - acc_top5: 0.9963 - 51ms/step\n",
      "step 610/782 - loss: 0.7175 - acc_top1: 0.8797 - acc_top5: 0.9962 - 51ms/step\n",
      "step 620/782 - loss: 0.3229 - acc_top1: 0.8792 - acc_top5: 0.9962 - 51ms/step\n",
      "step 630/782 - loss: 0.3420 - acc_top1: 0.8790 - acc_top5: 0.9962 - 51ms/step\n",
      "step 640/782 - loss: 0.3179 - acc_top1: 0.8786 - acc_top5: 0.9962 - 51ms/step\n",
      "step 650/782 - loss: 0.6586 - acc_top1: 0.8786 - acc_top5: 0.9962 - 51ms/step\n",
      "step 660/782 - loss: 0.3324 - acc_top1: 0.8778 - acc_top5: 0.9962 - 51ms/step\n",
      "step 670/782 - loss: 0.3545 - acc_top1: 0.8774 - acc_top5: 0.9963 - 51ms/step\n",
      "step 680/782 - loss: 0.4556 - acc_top1: 0.8772 - acc_top5: 0.9962 - 51ms/step\n",
      "step 690/782 - loss: 0.2589 - acc_top1: 0.8765 - acc_top5: 0.9962 - 50ms/step\n",
      "step 700/782 - loss: 0.5494 - acc_top1: 0.8760 - acc_top5: 0.9963 - 50ms/step\n",
      "step 710/782 - loss: 0.3749 - acc_top1: 0.8759 - acc_top5: 0.9963 - 50ms/step\n",
      "step 720/782 - loss: 0.4675 - acc_top1: 0.8758 - acc_top5: 0.9962 - 50ms/step\n",
      "step 730/782 - loss: 0.4040 - acc_top1: 0.8759 - acc_top5: 0.9962 - 50ms/step\n",
      "step 740/782 - loss: 0.5001 - acc_top1: 0.8757 - acc_top5: 0.9962 - 50ms/step\n",
      "step 750/782 - loss: 0.3740 - acc_top1: 0.8755 - acc_top5: 0.9962 - 50ms/step\n",
      "step 760/782 - loss: 0.4226 - acc_top1: 0.8750 - acc_top5: 0.9962 - 50ms/step\n",
      "step 770/782 - loss: 0.4356 - acc_top1: 0.8749 - acc_top5: 0.9962 - 50ms/step\n",
      "step 780/782 - loss: 0.3464 - acc_top1: 0.8749 - acc_top5: 0.9962 - 50ms/step\n",
      "step 782/782 - loss: 1.4629 - acc_top1: 0.8749 - acc_top5: 0.9961 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\28\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.0449 - acc_top1: 0.6234 - acc_top5: 0.9672 - 20ms/step\n",
      "step  20/157 - loss: 2.1899 - acc_top1: 0.6273 - acc_top5: 0.9641 - 19ms/step\n",
      "step  30/157 - loss: 1.4897 - acc_top1: 0.6229 - acc_top5: 0.9609 - 19ms/step\n",
      "step  40/157 - loss: 2.3070 - acc_top1: 0.6262 - acc_top5: 0.9602 - 19ms/step\n",
      "step  50/157 - loss: 1.2916 - acc_top1: 0.6238 - acc_top5: 0.9578 - 19ms/step\n",
      "step  60/157 - loss: 1.2195 - acc_top1: 0.6227 - acc_top5: 0.9539 - 19ms/step\n",
      "step  70/157 - loss: 1.3416 - acc_top1: 0.6232 - acc_top5: 0.9531 - 19ms/step\n",
      "step  80/157 - loss: 1.3623 - acc_top1: 0.6211 - acc_top5: 0.9529 - 19ms/step\n",
      "step  90/157 - loss: 1.2678 - acc_top1: 0.6227 - acc_top5: 0.9542 - 19ms/step\n",
      "step 100/157 - loss: 1.7103 - acc_top1: 0.6192 - acc_top5: 0.9533 - 19ms/step\n",
      "step 110/157 - loss: 2.0919 - acc_top1: 0.6162 - acc_top5: 0.9518 - 19ms/step\n",
      "step 120/157 - loss: 1.7723 - acc_top1: 0.6164 - acc_top5: 0.9517 - 19ms/step\n",
      "step 130/157 - loss: 1.7279 - acc_top1: 0.6165 - acc_top5: 0.9532 - 19ms/step\n",
      "step 140/157 - loss: 1.4355 - acc_top1: 0.6158 - acc_top5: 0.9529 - 19ms/step\n",
      "step 150/157 - loss: 1.2560 - acc_top1: 0.6160 - acc_top5: 0.9527 - 19ms/step\n",
      "step 157/157 - loss: 0.6597 - acc_top1: 0.6156 - acc_top5: 0.9530 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.4123 - acc_top1: 0.8969 - acc_top5: 1.0000 - 51ms/step\n",
      "step  20/782 - loss: 0.2888 - acc_top1: 0.8703 - acc_top5: 0.9984 - 53ms/step\n",
      "step  30/782 - loss: 0.3926 - acc_top1: 0.8776 - acc_top5: 0.9964 - 53ms/step\n",
      "step  40/782 - loss: 0.3756 - acc_top1: 0.8844 - acc_top5: 0.9961 - 52ms/step\n",
      "step  50/782 - loss: 0.5253 - acc_top1: 0.8809 - acc_top5: 0.9953 - 52ms/step\n",
      "step  60/782 - loss: 0.3345 - acc_top1: 0.8794 - acc_top5: 0.9956 - 52ms/step\n",
      "step  70/782 - loss: 0.1408 - acc_top1: 0.8853 - acc_top5: 0.9958 - 51ms/step\n",
      "step  80/782 - loss: 0.2418 - acc_top1: 0.8879 - acc_top5: 0.9963 - 51ms/step\n",
      "step  90/782 - loss: 0.2885 - acc_top1: 0.8884 - acc_top5: 0.9965 - 51ms/step\n",
      "step 100/782 - loss: 0.2647 - acc_top1: 0.8878 - acc_top5: 0.9966 - 51ms/step\n",
      "step 110/782 - loss: 0.2190 - acc_top1: 0.8884 - acc_top5: 0.9969 - 51ms/step\n",
      "step 120/782 - loss: 0.2282 - acc_top1: 0.8893 - acc_top5: 0.9970 - 51ms/step\n",
      "step 130/782 - loss: 0.3570 - acc_top1: 0.8895 - acc_top5: 0.9968 - 51ms/step\n",
      "step 140/782 - loss: 0.3195 - acc_top1: 0.8891 - acc_top5: 0.9969 - 51ms/step\n",
      "step 150/782 - loss: 0.3162 - acc_top1: 0.8905 - acc_top5: 0.9968 - 51ms/step\n",
      "step 160/782 - loss: 0.2832 - acc_top1: 0.8902 - acc_top5: 0.9968 - 51ms/step\n",
      "step 170/782 - loss: 0.2579 - acc_top1: 0.8911 - acc_top5: 0.9967 - 51ms/step\n",
      "step 180/782 - loss: 0.1880 - acc_top1: 0.8912 - acc_top5: 0.9968 - 51ms/step\n",
      "step 190/782 - loss: 0.2628 - acc_top1: 0.8905 - acc_top5: 0.9966 - 51ms/step\n",
      "step 200/782 - loss: 0.3824 - acc_top1: 0.8902 - acc_top5: 0.9967 - 50ms/step\n",
      "step 210/782 - loss: 0.4529 - acc_top1: 0.8896 - acc_top5: 0.9967 - 50ms/step\n",
      "step 220/782 - loss: 0.3201 - acc_top1: 0.8893 - acc_top5: 0.9967 - 50ms/step\n",
      "step 230/782 - loss: 0.3334 - acc_top1: 0.8892 - acc_top5: 0.9967 - 50ms/step\n",
      "step 240/782 - loss: 0.1688 - acc_top1: 0.8895 - acc_top5: 0.9968 - 50ms/step\n",
      "step 250/782 - loss: 0.3863 - acc_top1: 0.8899 - acc_top5: 0.9968 - 50ms/step\n",
      "step 260/782 - loss: 0.3621 - acc_top1: 0.8898 - acc_top5: 0.9968 - 50ms/step\n",
      "step 270/782 - loss: 0.3766 - acc_top1: 0.8891 - acc_top5: 0.9968 - 50ms/step\n",
      "step 280/782 - loss: 0.2926 - acc_top1: 0.8890 - acc_top5: 0.9967 - 50ms/step\n",
      "step 290/782 - loss: 0.3288 - acc_top1: 0.8884 - acc_top5: 0.9967 - 50ms/step\n",
      "step 300/782 - loss: 0.4354 - acc_top1: 0.8879 - acc_top5: 0.9967 - 50ms/step\n",
      "step 310/782 - loss: 0.3244 - acc_top1: 0.8883 - acc_top5: 0.9968 - 50ms/step\n",
      "step 320/782 - loss: 0.4253 - acc_top1: 0.8876 - acc_top5: 0.9968 - 50ms/step\n",
      "step 330/782 - loss: 0.3346 - acc_top1: 0.8875 - acc_top5: 0.9968 - 50ms/step\n",
      "step 340/782 - loss: 0.2897 - acc_top1: 0.8881 - acc_top5: 0.9968 - 50ms/step\n",
      "step 350/782 - loss: 0.3444 - acc_top1: 0.8882 - acc_top5: 0.9967 - 50ms/step\n",
      "step 360/782 - loss: 0.2870 - acc_top1: 0.8886 - acc_top5: 0.9967 - 50ms/step\n",
      "step 370/782 - loss: 0.4567 - acc_top1: 0.8883 - acc_top5: 0.9967 - 50ms/step\n",
      "step 380/782 - loss: 0.2705 - acc_top1: 0.8882 - acc_top5: 0.9964 - 50ms/step\n",
      "step 390/782 - loss: 0.5561 - acc_top1: 0.8874 - acc_top5: 0.9964 - 50ms/step\n",
      "step 400/782 - loss: 0.4721 - acc_top1: 0.8871 - acc_top5: 0.9963 - 50ms/step\n",
      "step 410/782 - loss: 0.3580 - acc_top1: 0.8872 - acc_top5: 0.9963 - 50ms/step\n",
      "step 420/782 - loss: 0.2663 - acc_top1: 0.8875 - acc_top5: 0.9964 - 50ms/step\n",
      "step 430/782 - loss: 0.2775 - acc_top1: 0.8876 - acc_top5: 0.9964 - 50ms/step\n",
      "step 440/782 - loss: 0.5801 - acc_top1: 0.8873 - acc_top5: 0.9964 - 50ms/step\n",
      "step 450/782 - loss: 0.3098 - acc_top1: 0.8871 - acc_top5: 0.9965 - 50ms/step\n",
      "step 460/782 - loss: 0.4332 - acc_top1: 0.8871 - acc_top5: 0.9964 - 50ms/step\n",
      "step 470/782 - loss: 0.3179 - acc_top1: 0.8871 - acc_top5: 0.9964 - 50ms/step\n",
      "step 480/782 - loss: 0.3054 - acc_top1: 0.8870 - acc_top5: 0.9964 - 50ms/step\n",
      "step 490/782 - loss: 0.5292 - acc_top1: 0.8865 - acc_top5: 0.9963 - 50ms/step\n",
      "step 500/782 - loss: 0.3788 - acc_top1: 0.8862 - acc_top5: 0.9963 - 50ms/step\n",
      "step 510/782 - loss: 0.2055 - acc_top1: 0.8858 - acc_top5: 0.9964 - 50ms/step\n",
      "step 520/782 - loss: 0.4124 - acc_top1: 0.8859 - acc_top5: 0.9965 - 50ms/step\n",
      "step 530/782 - loss: 0.2958 - acc_top1: 0.8861 - acc_top5: 0.9965 - 50ms/step\n",
      "step 540/782 - loss: 0.2281 - acc_top1: 0.8863 - acc_top5: 0.9965 - 50ms/step\n",
      "step 550/782 - loss: 0.3038 - acc_top1: 0.8861 - acc_top5: 0.9965 - 50ms/step\n",
      "step 560/782 - loss: 0.2030 - acc_top1: 0.8859 - acc_top5: 0.9965 - 50ms/step\n",
      "step 570/782 - loss: 0.3388 - acc_top1: 0.8862 - acc_top5: 0.9965 - 50ms/step\n",
      "step 580/782 - loss: 0.2684 - acc_top1: 0.8857 - acc_top5: 0.9966 - 50ms/step\n",
      "step 590/782 - loss: 0.3195 - acc_top1: 0.8855 - acc_top5: 0.9966 - 50ms/step\n",
      "step 600/782 - loss: 0.1979 - acc_top1: 0.8854 - acc_top5: 0.9966 - 50ms/step\n",
      "step 610/782 - loss: 0.4097 - acc_top1: 0.8853 - acc_top5: 0.9966 - 50ms/step\n",
      "step 620/782 - loss: 0.2691 - acc_top1: 0.8854 - acc_top5: 0.9966 - 50ms/step\n",
      "step 630/782 - loss: 0.4303 - acc_top1: 0.8853 - acc_top5: 0.9967 - 50ms/step\n",
      "step 640/782 - loss: 0.2841 - acc_top1: 0.8855 - acc_top5: 0.9966 - 50ms/step\n",
      "step 650/782 - loss: 0.3338 - acc_top1: 0.8852 - acc_top5: 0.9966 - 50ms/step\n",
      "step 660/782 - loss: 0.5270 - acc_top1: 0.8851 - acc_top5: 0.9966 - 50ms/step\n",
      "step 670/782 - loss: 0.2797 - acc_top1: 0.8852 - acc_top5: 0.9967 - 50ms/step\n",
      "step 680/782 - loss: 0.2315 - acc_top1: 0.8850 - acc_top5: 0.9967 - 50ms/step\n",
      "step 690/782 - loss: 0.3246 - acc_top1: 0.8848 - acc_top5: 0.9966 - 50ms/step\n",
      "step 700/782 - loss: 0.2052 - acc_top1: 0.8849 - acc_top5: 0.9966 - 50ms/step\n",
      "step 710/782 - loss: 0.2948 - acc_top1: 0.8849 - acc_top5: 0.9966 - 50ms/step\n",
      "step 720/782 - loss: 0.4867 - acc_top1: 0.8845 - acc_top5: 0.9966 - 50ms/step\n",
      "step 730/782 - loss: 0.3094 - acc_top1: 0.8843 - acc_top5: 0.9966 - 50ms/step\n",
      "step 740/782 - loss: 0.3819 - acc_top1: 0.8845 - acc_top5: 0.9966 - 50ms/step\n",
      "step 750/782 - loss: 0.3355 - acc_top1: 0.8847 - acc_top5: 0.9966 - 50ms/step\n",
      "step 760/782 - loss: 0.2913 - acc_top1: 0.8847 - acc_top5: 0.9966 - 50ms/step\n",
      "step 770/782 - loss: 0.1492 - acc_top1: 0.8847 - acc_top5: 0.9966 - 50ms/step\n",
      "step 780/782 - loss: 0.4734 - acc_top1: 0.8846 - acc_top5: 0.9966 - 50ms/step\n",
      "step 782/782 - loss: 0.5127 - acc_top1: 0.8846 - acc_top5: 0.9966 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\29\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.1092 - acc_top1: 0.6266 - acc_top5: 0.9656 - 20ms/step\n",
      "step  20/157 - loss: 1.9772 - acc_top1: 0.6266 - acc_top5: 0.9625 - 19ms/step\n",
      "step  30/157 - loss: 1.7536 - acc_top1: 0.6109 - acc_top5: 0.9557 - 19ms/step\n",
      "step  40/157 - loss: 2.3189 - acc_top1: 0.6203 - acc_top5: 0.9535 - 19ms/step\n",
      "step  50/157 - loss: 1.5244 - acc_top1: 0.6191 - acc_top5: 0.9528 - 19ms/step\n",
      "step  60/157 - loss: 1.7697 - acc_top1: 0.6180 - acc_top5: 0.9523 - 19ms/step\n",
      "step  70/157 - loss: 5.6339 - acc_top1: 0.6190 - acc_top5: 0.9509 - 19ms/step\n",
      "step  80/157 - loss: 1.7350 - acc_top1: 0.6182 - acc_top5: 0.9502 - 19ms/step\n",
      "step  90/157 - loss: 1.2361 - acc_top1: 0.6200 - acc_top5: 0.9510 - 19ms/step\n",
      "step 100/157 - loss: 1.9250 - acc_top1: 0.6186 - acc_top5: 0.9513 - 19ms/step\n",
      "step 110/157 - loss: 2.2381 - acc_top1: 0.6161 - acc_top5: 0.9506 - 19ms/step\n",
      "step 120/157 - loss: 2.0219 - acc_top1: 0.6135 - acc_top5: 0.9500 - 19ms/step\n",
      "step 130/157 - loss: 2.2892 - acc_top1: 0.6130 - acc_top5: 0.9500 - 19ms/step\n",
      "step 140/157 - loss: 3.9174 - acc_top1: 0.6135 - acc_top5: 0.9498 - 19ms/step\n",
      "step 150/157 - loss: 7.0925 - acc_top1: 0.6141 - acc_top5: 0.9491 - 19ms/step\n",
      "step 157/157 - loss: 1.3406 - acc_top1: 0.6125 - acc_top5: 0.9489 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.1859 - acc_top1: 0.9203 - acc_top5: 0.9953 - 53ms/step\n",
      "step  20/782 - loss: 0.1913 - acc_top1: 0.9047 - acc_top5: 0.9977 - 55ms/step\n",
      "step  30/782 - loss: 0.2754 - acc_top1: 0.9047 - acc_top5: 0.9974 - 54ms/step\n",
      "step  40/782 - loss: 0.3733 - acc_top1: 0.9082 - acc_top5: 0.9980 - 53ms/step\n",
      "step  50/782 - loss: 0.1751 - acc_top1: 0.9153 - acc_top5: 0.9984 - 52ms/step\n",
      "step  60/782 - loss: 0.2379 - acc_top1: 0.9133 - acc_top5: 0.9984 - 52ms/step\n",
      "step  70/782 - loss: 0.2662 - acc_top1: 0.9138 - acc_top5: 0.9982 - 52ms/step\n",
      "step  80/782 - loss: 0.1710 - acc_top1: 0.9148 - acc_top5: 0.9982 - 52ms/step\n",
      "step  90/782 - loss: 0.3375 - acc_top1: 0.9113 - acc_top5: 0.9984 - 51ms/step\n",
      "step 100/782 - loss: 0.2434 - acc_top1: 0.9102 - acc_top5: 0.9983 - 51ms/step\n",
      "step 110/782 - loss: 0.3970 - acc_top1: 0.9104 - acc_top5: 0.9983 - 51ms/step\n",
      "step 120/782 - loss: 0.2996 - acc_top1: 0.9100 - acc_top5: 0.9983 - 51ms/step\n",
      "step 130/782 - loss: 0.1138 - acc_top1: 0.9107 - acc_top5: 0.9983 - 51ms/step\n",
      "step 140/782 - loss: 0.1446 - acc_top1: 0.9117 - acc_top5: 0.9983 - 51ms/step\n",
      "step 150/782 - loss: 0.2636 - acc_top1: 0.9113 - acc_top5: 0.9984 - 51ms/step\n",
      "step 160/782 - loss: 0.1485 - acc_top1: 0.9121 - acc_top5: 0.9984 - 51ms/step\n",
      "step 170/782 - loss: 0.2444 - acc_top1: 0.9116 - acc_top5: 0.9985 - 51ms/step\n",
      "step 180/782 - loss: 0.3196 - acc_top1: 0.9118 - acc_top5: 0.9984 - 51ms/step\n",
      "step 190/782 - loss: 0.3744 - acc_top1: 0.9119 - acc_top5: 0.9984 - 51ms/step\n",
      "step 200/782 - loss: 0.2144 - acc_top1: 0.9116 - acc_top5: 0.9983 - 51ms/step\n",
      "step 210/782 - loss: 0.5352 - acc_top1: 0.9109 - acc_top5: 0.9983 - 51ms/step\n",
      "step 220/782 - loss: 0.2278 - acc_top1: 0.9112 - acc_top5: 0.9984 - 51ms/step\n",
      "step 230/782 - loss: 0.1840 - acc_top1: 0.9119 - acc_top5: 0.9984 - 51ms/step\n",
      "step 240/782 - loss: 0.3754 - acc_top1: 0.9115 - acc_top5: 0.9982 - 51ms/step\n",
      "step 250/782 - loss: 0.2731 - acc_top1: 0.9119 - acc_top5: 0.9981 - 50ms/step\n",
      "step 260/782 - loss: 0.2838 - acc_top1: 0.9120 - acc_top5: 0.9981 - 50ms/step\n",
      "step 270/782 - loss: 0.1730 - acc_top1: 0.9117 - acc_top5: 0.9981 - 50ms/step\n",
      "step 280/782 - loss: 0.2308 - acc_top1: 0.9111 - acc_top5: 0.9982 - 50ms/step\n",
      "step 290/782 - loss: 0.2736 - acc_top1: 0.9109 - acc_top5: 0.9982 - 50ms/step\n",
      "step 300/782 - loss: 0.1483 - acc_top1: 0.9106 - acc_top5: 0.9981 - 50ms/step\n",
      "step 310/782 - loss: 0.2196 - acc_top1: 0.9109 - acc_top5: 0.9981 - 50ms/step\n",
      "step 320/782 - loss: 0.4356 - acc_top1: 0.9103 - acc_top5: 0.9979 - 50ms/step\n",
      "step 330/782 - loss: 0.3868 - acc_top1: 0.9098 - acc_top5: 0.9979 - 50ms/step\n",
      "step 340/782 - loss: 0.3565 - acc_top1: 0.9095 - acc_top5: 0.9979 - 50ms/step\n",
      "step 350/782 - loss: 0.3625 - acc_top1: 0.9087 - acc_top5: 0.9978 - 50ms/step\n",
      "step 360/782 - loss: 0.3702 - acc_top1: 0.9081 - acc_top5: 0.9978 - 50ms/step\n",
      "step 370/782 - loss: 0.2317 - acc_top1: 0.9077 - acc_top5: 0.9978 - 50ms/step\n",
      "step 380/782 - loss: 0.3025 - acc_top1: 0.9078 - acc_top5: 0.9978 - 50ms/step\n",
      "step 390/782 - loss: 0.1054 - acc_top1: 0.9082 - acc_top5: 0.9978 - 50ms/step\n",
      "step 400/782 - loss: 0.1644 - acc_top1: 0.9077 - acc_top5: 0.9977 - 50ms/step\n",
      "step 410/782 - loss: 0.3688 - acc_top1: 0.9074 - acc_top5: 0.9977 - 50ms/step\n",
      "step 420/782 - loss: 0.3288 - acc_top1: 0.9071 - acc_top5: 0.9976 - 50ms/step\n",
      "step 430/782 - loss: 0.2536 - acc_top1: 0.9065 - acc_top5: 0.9976 - 50ms/step\n",
      "step 440/782 - loss: 0.3500 - acc_top1: 0.9067 - acc_top5: 0.9975 - 50ms/step\n",
      "step 450/782 - loss: 0.4141 - acc_top1: 0.9068 - acc_top5: 0.9975 - 50ms/step\n",
      "step 460/782 - loss: 0.4280 - acc_top1: 0.9065 - acc_top5: 0.9975 - 50ms/step\n",
      "step 470/782 - loss: 0.2538 - acc_top1: 0.9062 - acc_top5: 0.9976 - 50ms/step\n",
      "step 480/782 - loss: 0.3135 - acc_top1: 0.9057 - acc_top5: 0.9975 - 50ms/step\n",
      "step 490/782 - loss: 0.2394 - acc_top1: 0.9055 - acc_top5: 0.9975 - 50ms/step\n",
      "step 500/782 - loss: 0.2434 - acc_top1: 0.9054 - acc_top5: 0.9975 - 50ms/step\n",
      "step 510/782 - loss: 0.3240 - acc_top1: 0.9047 - acc_top5: 0.9976 - 50ms/step\n",
      "step 520/782 - loss: 0.3821 - acc_top1: 0.9044 - acc_top5: 0.9976 - 50ms/step\n",
      "step 530/782 - loss: 0.2350 - acc_top1: 0.9043 - acc_top5: 0.9976 - 50ms/step\n",
      "step 540/782 - loss: 0.2487 - acc_top1: 0.9043 - acc_top5: 0.9976 - 50ms/step\n",
      "step 550/782 - loss: 0.2700 - acc_top1: 0.9039 - acc_top5: 0.9976 - 50ms/step\n",
      "step 560/782 - loss: 0.2473 - acc_top1: 0.9038 - acc_top5: 0.9975 - 50ms/step\n",
      "step 570/782 - loss: 0.2298 - acc_top1: 0.9039 - acc_top5: 0.9975 - 50ms/step\n",
      "step 580/782 - loss: 0.2338 - acc_top1: 0.9037 - acc_top5: 0.9976 - 50ms/step\n",
      "step 590/782 - loss: 0.3809 - acc_top1: 0.9034 - acc_top5: 0.9975 - 50ms/step\n",
      "step 600/782 - loss: 0.2169 - acc_top1: 0.9032 - acc_top5: 0.9975 - 50ms/step\n",
      "step 610/782 - loss: 0.4486 - acc_top1: 0.9031 - acc_top5: 0.9976 - 50ms/step\n",
      "step 620/782 - loss: 0.4112 - acc_top1: 0.9028 - acc_top5: 0.9976 - 50ms/step\n",
      "step 630/782 - loss: 0.1882 - acc_top1: 0.9027 - acc_top5: 0.9976 - 50ms/step\n",
      "step 640/782 - loss: 0.2587 - acc_top1: 0.9021 - acc_top5: 0.9975 - 50ms/step\n",
      "step 650/782 - loss: 0.2302 - acc_top1: 0.9020 - acc_top5: 0.9975 - 50ms/step\n",
      "step 660/782 - loss: 0.2234 - acc_top1: 0.9020 - acc_top5: 0.9975 - 50ms/step\n",
      "step 670/782 - loss: 0.1873 - acc_top1: 0.9019 - acc_top5: 0.9975 - 50ms/step\n",
      "step 680/782 - loss: 0.2343 - acc_top1: 0.9017 - acc_top5: 0.9974 - 50ms/step\n",
      "step 690/782 - loss: 0.2746 - acc_top1: 0.9015 - acc_top5: 0.9974 - 50ms/step\n",
      "step 700/782 - loss: 0.2848 - acc_top1: 0.9014 - acc_top5: 0.9974 - 50ms/step\n",
      "step 710/782 - loss: 0.2152 - acc_top1: 0.9013 - acc_top5: 0.9974 - 50ms/step\n",
      "step 720/782 - loss: 0.3121 - acc_top1: 0.9013 - acc_top5: 0.9973 - 50ms/step\n",
      "step 730/782 - loss: 0.0794 - acc_top1: 0.9017 - acc_top5: 0.9973 - 50ms/step\n",
      "step 740/782 - loss: 0.3687 - acc_top1: 0.9014 - acc_top5: 0.9973 - 50ms/step\n",
      "step 750/782 - loss: 0.2702 - acc_top1: 0.9014 - acc_top5: 0.9972 - 50ms/step\n",
      "step 760/782 - loss: 0.2301 - acc_top1: 0.9014 - acc_top5: 0.9972 - 50ms/step\n",
      "step 770/782 - loss: 0.5482 - acc_top1: 0.9009 - acc_top5: 0.9971 - 50ms/step\n",
      "step 780/782 - loss: 0.3139 - acc_top1: 0.9007 - acc_top5: 0.9971 - 50ms/step\n",
      "step 782/782 - loss: 1.0054 - acc_top1: 0.9008 - acc_top5: 0.9971 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\30\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.2576 - acc_top1: 0.6125 - acc_top5: 0.9563 - 20ms/step\n",
      "step  20/157 - loss: 2.8887 - acc_top1: 0.6156 - acc_top5: 0.9555 - 19ms/step\n",
      "step  30/157 - loss: 2.1144 - acc_top1: 0.6156 - acc_top5: 0.9510 - 19ms/step\n",
      "step  40/157 - loss: 2.5248 - acc_top1: 0.6129 - acc_top5: 0.9469 - 19ms/step\n",
      "step  50/157 - loss: 1.1404 - acc_top1: 0.6166 - acc_top5: 0.9444 - 19ms/step\n",
      "step  60/157 - loss: 1.6908 - acc_top1: 0.6159 - acc_top5: 0.9437 - 19ms/step\n",
      "step  70/157 - loss: 5.7460 - acc_top1: 0.6179 - acc_top5: 0.9429 - 19ms/step\n",
      "step  80/157 - loss: 1.6683 - acc_top1: 0.6156 - acc_top5: 0.9430 - 19ms/step\n",
      "step  90/157 - loss: 1.2097 - acc_top1: 0.6142 - acc_top5: 0.9436 - 19ms/step\n",
      "step 100/157 - loss: 1.6900 - acc_top1: 0.6116 - acc_top5: 0.9444 - 19ms/step\n",
      "step 110/157 - loss: 2.3740 - acc_top1: 0.6091 - acc_top5: 0.9445 - 19ms/step\n",
      "step 120/157 - loss: 2.0742 - acc_top1: 0.6072 - acc_top5: 0.9440 - 19ms/step\n",
      "step 130/157 - loss: 1.9910 - acc_top1: 0.6061 - acc_top5: 0.9454 - 19ms/step\n",
      "step 140/157 - loss: 4.1278 - acc_top1: 0.6040 - acc_top5: 0.9455 - 19ms/step\n",
      "step 150/157 - loss: 7.1348 - acc_top1: 0.6039 - acc_top5: 0.9454 - 19ms/step\n",
      "step 157/157 - loss: 0.7878 - acc_top1: 0.6035 - acc_top5: 0.9453 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.2312 - acc_top1: 0.8938 - acc_top5: 0.9922 - 51ms/step\n",
      "step  20/782 - loss: 0.2595 - acc_top1: 0.8922 - acc_top5: 0.9938 - 51ms/step\n",
      "step  30/782 - loss: 0.2267 - acc_top1: 0.8943 - acc_top5: 0.9943 - 51ms/step\n",
      "step  40/782 - loss: 0.2952 - acc_top1: 0.8906 - acc_top5: 0.9949 - 50ms/step\n",
      "step  50/782 - loss: 0.2802 - acc_top1: 0.8947 - acc_top5: 0.9956 - 50ms/step\n",
      "step  60/782 - loss: 0.3756 - acc_top1: 0.8966 - acc_top5: 0.9958 - 51ms/step\n",
      "step  70/782 - loss: 0.2661 - acc_top1: 0.8996 - acc_top5: 0.9964 - 51ms/step\n",
      "step  80/782 - loss: 0.0959 - acc_top1: 0.9039 - acc_top5: 0.9965 - 51ms/step\n",
      "step  90/782 - loss: 0.2243 - acc_top1: 0.9049 - acc_top5: 0.9964 - 51ms/step\n",
      "step 100/782 - loss: 0.1985 - acc_top1: 0.9059 - acc_top5: 0.9966 - 51ms/step\n",
      "step 110/782 - loss: 0.2706 - acc_top1: 0.9068 - acc_top5: 0.9967 - 51ms/step\n",
      "step 120/782 - loss: 0.4122 - acc_top1: 0.9072 - acc_top5: 0.9967 - 51ms/step\n",
      "step 130/782 - loss: 0.2796 - acc_top1: 0.9056 - acc_top5: 0.9966 - 51ms/step\n",
      "step 140/782 - loss: 0.2900 - acc_top1: 0.9070 - acc_top5: 0.9967 - 51ms/step\n",
      "step 150/782 - loss: 0.2614 - acc_top1: 0.9073 - acc_top5: 0.9968 - 51ms/step\n",
      "step 160/782 - loss: 0.2494 - acc_top1: 0.9077 - acc_top5: 0.9969 - 51ms/step\n",
      "step 170/782 - loss: 0.3083 - acc_top1: 0.9082 - acc_top5: 0.9967 - 51ms/step\n",
      "step 180/782 - loss: 0.2652 - acc_top1: 0.9085 - acc_top5: 0.9967 - 51ms/step\n",
      "step 190/782 - loss: 0.3130 - acc_top1: 0.9072 - acc_top5: 0.9967 - 51ms/step\n",
      "step 200/782 - loss: 0.3512 - acc_top1: 0.9079 - acc_top5: 0.9968 - 51ms/step\n",
      "step 210/782 - loss: 0.1537 - acc_top1: 0.9079 - acc_top5: 0.9965 - 51ms/step\n",
      "step 220/782 - loss: 0.2632 - acc_top1: 0.9077 - acc_top5: 0.9965 - 51ms/step\n",
      "step 230/782 - loss: 0.3351 - acc_top1: 0.9072 - acc_top5: 0.9965 - 51ms/step\n",
      "step 240/782 - loss: 0.2739 - acc_top1: 0.9082 - acc_top5: 0.9965 - 50ms/step\n",
      "step 250/782 - loss: 0.2339 - acc_top1: 0.9075 - acc_top5: 0.9966 - 50ms/step\n",
      "step 260/782 - loss: 0.2887 - acc_top1: 0.9073 - acc_top5: 0.9966 - 50ms/step\n",
      "step 270/782 - loss: 0.3588 - acc_top1: 0.9077 - acc_top5: 0.9965 - 50ms/step\n",
      "step 280/782 - loss: 0.1711 - acc_top1: 0.9080 - acc_top5: 0.9965 - 50ms/step\n",
      "step 290/782 - loss: 0.1790 - acc_top1: 0.9078 - acc_top5: 0.9964 - 50ms/step\n",
      "step 300/782 - loss: 0.2157 - acc_top1: 0.9083 - acc_top5: 0.9964 - 50ms/step\n",
      "step 310/782 - loss: 0.2007 - acc_top1: 0.9082 - acc_top5: 0.9964 - 50ms/step\n",
      "step 320/782 - loss: 0.2827 - acc_top1: 0.9084 - acc_top5: 0.9964 - 50ms/step\n",
      "step 330/782 - loss: 0.2290 - acc_top1: 0.9089 - acc_top5: 0.9965 - 50ms/step\n",
      "step 340/782 - loss: 0.1647 - acc_top1: 0.9087 - acc_top5: 0.9966 - 50ms/step\n",
      "step 350/782 - loss: 0.4277 - acc_top1: 0.9083 - acc_top5: 0.9966 - 50ms/step\n",
      "step 360/782 - loss: 0.2923 - acc_top1: 0.9084 - acc_top5: 0.9966 - 50ms/step\n",
      "step 370/782 - loss: 0.1847 - acc_top1: 0.9081 - acc_top5: 0.9967 - 50ms/step\n",
      "step 380/782 - loss: 0.2261 - acc_top1: 0.9074 - acc_top5: 0.9968 - 51ms/step\n",
      "step 390/782 - loss: 0.3158 - acc_top1: 0.9077 - acc_top5: 0.9968 - 51ms/step\n",
      "step 400/782 - loss: 0.3355 - acc_top1: 0.9077 - acc_top5: 0.9968 - 51ms/step\n",
      "step 410/782 - loss: 0.2368 - acc_top1: 0.9075 - acc_top5: 0.9969 - 51ms/step\n",
      "step 420/782 - loss: 0.3912 - acc_top1: 0.9075 - acc_top5: 0.9969 - 51ms/step\n",
      "step 430/782 - loss: 0.3410 - acc_top1: 0.9074 - acc_top5: 0.9970 - 51ms/step\n",
      "step 440/782 - loss: 0.4469 - acc_top1: 0.9074 - acc_top5: 0.9971 - 50ms/step\n",
      "step 450/782 - loss: 0.3305 - acc_top1: 0.9072 - acc_top5: 0.9971 - 50ms/step\n",
      "step 460/782 - loss: 0.1751 - acc_top1: 0.9073 - acc_top5: 0.9971 - 50ms/step\n",
      "step 470/782 - loss: 0.3602 - acc_top1: 0.9069 - acc_top5: 0.9971 - 50ms/step\n",
      "step 480/782 - loss: 0.1954 - acc_top1: 0.9072 - acc_top5: 0.9971 - 50ms/step\n",
      "step 490/782 - loss: 0.2388 - acc_top1: 0.9072 - acc_top5: 0.9971 - 50ms/step\n",
      "step 500/782 - loss: 0.2674 - acc_top1: 0.9071 - acc_top5: 0.9970 - 50ms/step\n",
      "step 510/782 - loss: 0.3196 - acc_top1: 0.9070 - acc_top5: 0.9970 - 50ms/step\n",
      "step 520/782 - loss: 0.1785 - acc_top1: 0.9072 - acc_top5: 0.9971 - 50ms/step\n",
      "step 530/782 - loss: 0.3645 - acc_top1: 0.9070 - acc_top5: 0.9971 - 50ms/step\n",
      "step 540/782 - loss: 0.2935 - acc_top1: 0.9069 - acc_top5: 0.9971 - 50ms/step\n",
      "step 550/782 - loss: 0.1043 - acc_top1: 0.9072 - acc_top5: 0.9971 - 50ms/step\n",
      "step 560/782 - loss: 0.3083 - acc_top1: 0.9068 - acc_top5: 0.9971 - 50ms/step\n",
      "step 570/782 - loss: 0.2532 - acc_top1: 0.9071 - acc_top5: 0.9971 - 50ms/step\n",
      "step 580/782 - loss: 0.3668 - acc_top1: 0.9074 - acc_top5: 0.9971 - 50ms/step\n",
      "step 590/782 - loss: 0.2017 - acc_top1: 0.9076 - acc_top5: 0.9971 - 50ms/step\n",
      "step 600/782 - loss: 0.3569 - acc_top1: 0.9074 - acc_top5: 0.9971 - 50ms/step\n",
      "step 610/782 - loss: 0.2028 - acc_top1: 0.9075 - acc_top5: 0.9971 - 50ms/step\n",
      "step 620/782 - loss: 0.4254 - acc_top1: 0.9073 - acc_top5: 0.9971 - 50ms/step\n",
      "step 630/782 - loss: 0.3573 - acc_top1: 0.9075 - acc_top5: 0.9971 - 50ms/step\n",
      "step 640/782 - loss: 0.2644 - acc_top1: 0.9074 - acc_top5: 0.9972 - 50ms/step\n",
      "step 650/782 - loss: 0.2577 - acc_top1: 0.9072 - acc_top5: 0.9972 - 50ms/step\n",
      "step 660/782 - loss: 0.2837 - acc_top1: 0.9074 - acc_top5: 0.9972 - 50ms/step\n",
      "step 670/782 - loss: 0.3784 - acc_top1: 0.9075 - acc_top5: 0.9972 - 50ms/step\n",
      "step 680/782 - loss: 0.3560 - acc_top1: 0.9071 - acc_top5: 0.9972 - 50ms/step\n",
      "step 690/782 - loss: 0.2163 - acc_top1: 0.9071 - acc_top5: 0.9971 - 50ms/step\n",
      "step 700/782 - loss: 0.4495 - acc_top1: 0.9070 - acc_top5: 0.9971 - 50ms/step\n",
      "step 710/782 - loss: 0.2121 - acc_top1: 0.9068 - acc_top5: 0.9971 - 50ms/step\n",
      "step 720/782 - loss: 0.1812 - acc_top1: 0.9068 - acc_top5: 0.9971 - 50ms/step\n",
      "step 730/782 - loss: 0.2404 - acc_top1: 0.9067 - acc_top5: 0.9972 - 50ms/step\n",
      "step 740/782 - loss: 0.2175 - acc_top1: 0.9066 - acc_top5: 0.9971 - 50ms/step\n",
      "step 750/782 - loss: 0.3185 - acc_top1: 0.9065 - acc_top5: 0.9971 - 50ms/step\n",
      "step 760/782 - loss: 0.1974 - acc_top1: 0.9064 - acc_top5: 0.9972 - 50ms/step\n",
      "step 770/782 - loss: 0.2237 - acc_top1: 0.9064 - acc_top5: 0.9971 - 50ms/step\n",
      "step 780/782 - loss: 0.1573 - acc_top1: 0.9065 - acc_top5: 0.9972 - 50ms/step\n",
      "step 782/782 - loss: 0.3478 - acc_top1: 0.9064 - acc_top5: 0.9972 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\31\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.3245 - acc_top1: 0.6203 - acc_top5: 0.9625 - 19ms/step\n",
      "step  20/157 - loss: 2.4502 - acc_top1: 0.6148 - acc_top5: 0.9602 - 19ms/step\n",
      "step  30/157 - loss: 1.9316 - acc_top1: 0.6182 - acc_top5: 0.9542 - 19ms/step\n",
      "step  40/157 - loss: 2.3361 - acc_top1: 0.6164 - acc_top5: 0.9547 - 19ms/step\n",
      "step  50/157 - loss: 1.5048 - acc_top1: 0.6138 - acc_top5: 0.9522 - 19ms/step\n",
      "step  60/157 - loss: 1.8835 - acc_top1: 0.6130 - acc_top5: 0.9495 - 19ms/step\n",
      "step  70/157 - loss: 4.8662 - acc_top1: 0.6163 - acc_top5: 0.9496 - 19ms/step\n",
      "step  80/157 - loss: 1.3773 - acc_top1: 0.6186 - acc_top5: 0.9508 - 19ms/step\n",
      "step  90/157 - loss: 1.2938 - acc_top1: 0.6186 - acc_top5: 0.9517 - 19ms/step\n",
      "step 100/157 - loss: 1.6139 - acc_top1: 0.6192 - acc_top5: 0.9514 - 19ms/step\n",
      "step 110/157 - loss: 2.5873 - acc_top1: 0.6168 - acc_top5: 0.9504 - 19ms/step\n",
      "step 120/157 - loss: 2.1772 - acc_top1: 0.6159 - acc_top5: 0.9499 - 19ms/step\n",
      "step 130/157 - loss: 1.9676 - acc_top1: 0.6156 - acc_top5: 0.9507 - 19ms/step\n",
      "step 140/157 - loss: 3.8695 - acc_top1: 0.6144 - acc_top5: 0.9509 - 19ms/step\n",
      "step 150/157 - loss: 7.6683 - acc_top1: 0.6140 - acc_top5: 0.9509 - 19ms/step\n",
      "step 157/157 - loss: 0.8785 - acc_top1: 0.6132 - acc_top5: 0.9512 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.3375 - acc_top1: 0.9094 - acc_top5: 0.9984 - 52ms/step\n",
      "step  20/782 - loss: 0.1722 - acc_top1: 0.9219 - acc_top5: 0.9984 - 52ms/step\n",
      "step  30/782 - loss: 0.1817 - acc_top1: 0.9245 - acc_top5: 0.9984 - 51ms/step\n",
      "step  40/782 - loss: 0.2455 - acc_top1: 0.9266 - acc_top5: 0.9984 - 51ms/step\n",
      "step  50/782 - loss: 0.2147 - acc_top1: 0.9259 - acc_top5: 0.9984 - 51ms/step\n",
      "step  60/782 - loss: 0.1695 - acc_top1: 0.9253 - acc_top5: 0.9982 - 51ms/step\n",
      "step  70/782 - loss: 0.3093 - acc_top1: 0.9272 - acc_top5: 0.9980 - 51ms/step\n",
      "step  80/782 - loss: 0.2225 - acc_top1: 0.9271 - acc_top5: 0.9980 - 51ms/step\n",
      "step  90/782 - loss: 0.1158 - acc_top1: 0.9274 - acc_top5: 0.9976 - 51ms/step\n",
      "step 100/782 - loss: 0.3394 - acc_top1: 0.9269 - acc_top5: 0.9975 - 51ms/step\n",
      "step 110/782 - loss: 0.2033 - acc_top1: 0.9257 - acc_top5: 0.9977 - 51ms/step\n",
      "step 120/782 - loss: 0.2787 - acc_top1: 0.9224 - acc_top5: 0.9977 - 51ms/step\n",
      "step 130/782 - loss: 0.1678 - acc_top1: 0.9192 - acc_top5: 0.9976 - 51ms/step\n",
      "step 140/782 - loss: 0.3402 - acc_top1: 0.9175 - acc_top5: 0.9975 - 51ms/step\n",
      "step 150/782 - loss: 0.3081 - acc_top1: 0.9160 - acc_top5: 0.9975 - 51ms/step\n",
      "step 160/782 - loss: 0.2388 - acc_top1: 0.9141 - acc_top5: 0.9976 - 51ms/step\n",
      "step 170/782 - loss: 0.2845 - acc_top1: 0.9129 - acc_top5: 0.9976 - 51ms/step\n",
      "step 180/782 - loss: 0.3904 - acc_top1: 0.9121 - acc_top5: 0.9977 - 51ms/step\n",
      "step 190/782 - loss: 0.3512 - acc_top1: 0.9114 - acc_top5: 0.9977 - 51ms/step\n",
      "step 200/782 - loss: 0.4259 - acc_top1: 0.9110 - acc_top5: 0.9977 - 51ms/step\n",
      "step 210/782 - loss: 0.2768 - acc_top1: 0.9109 - acc_top5: 0.9976 - 51ms/step\n",
      "step 220/782 - loss: 0.1712 - acc_top1: 0.9110 - acc_top5: 0.9977 - 51ms/step\n",
      "step 230/782 - loss: 0.1297 - acc_top1: 0.9110 - acc_top5: 0.9978 - 51ms/step\n",
      "step 240/782 - loss: 0.4037 - acc_top1: 0.9100 - acc_top5: 0.9978 - 51ms/step\n",
      "step 250/782 - loss: 0.2946 - acc_top1: 0.9097 - acc_top5: 0.9979 - 51ms/step\n",
      "step 260/782 - loss: 0.2375 - acc_top1: 0.9101 - acc_top5: 0.9978 - 51ms/step\n",
      "step 270/782 - loss: 0.1129 - acc_top1: 0.9094 - acc_top5: 0.9978 - 51ms/step\n",
      "step 280/782 - loss: 0.3085 - acc_top1: 0.9102 - acc_top5: 0.9979 - 51ms/step\n",
      "step 290/782 - loss: 0.4954 - acc_top1: 0.9110 - acc_top5: 0.9978 - 51ms/step\n",
      "step 300/782 - loss: 0.2831 - acc_top1: 0.9108 - acc_top5: 0.9978 - 51ms/step\n",
      "step 310/782 - loss: 0.5077 - acc_top1: 0.9104 - acc_top5: 0.9978 - 51ms/step\n",
      "step 320/782 - loss: 0.3564 - acc_top1: 0.9103 - acc_top5: 0.9979 - 51ms/step\n",
      "step 330/782 - loss: 0.2592 - acc_top1: 0.9106 - acc_top5: 0.9979 - 51ms/step\n",
      "step 340/782 - loss: 0.2436 - acc_top1: 0.9103 - acc_top5: 0.9979 - 51ms/step\n",
      "step 350/782 - loss: 0.2560 - acc_top1: 0.9099 - acc_top5: 0.9979 - 51ms/step\n",
      "step 360/782 - loss: 0.4524 - acc_top1: 0.9095 - acc_top5: 0.9979 - 51ms/step\n",
      "step 370/782 - loss: 0.2288 - acc_top1: 0.9098 - acc_top5: 0.9978 - 51ms/step\n",
      "step 380/782 - loss: 0.2970 - acc_top1: 0.9096 - acc_top5: 0.9978 - 51ms/step\n",
      "step 390/782 - loss: 0.2727 - acc_top1: 0.9093 - acc_top5: 0.9978 - 51ms/step\n",
      "step 400/782 - loss: 0.3888 - acc_top1: 0.9090 - acc_top5: 0.9978 - 51ms/step\n",
      "step 410/782 - loss: 0.2238 - acc_top1: 0.9090 - acc_top5: 0.9978 - 51ms/step\n",
      "step 420/782 - loss: 0.3867 - acc_top1: 0.9093 - acc_top5: 0.9978 - 51ms/step\n",
      "step 430/782 - loss: 0.2058 - acc_top1: 0.9096 - acc_top5: 0.9978 - 51ms/step\n",
      "step 440/782 - loss: 0.1852 - acc_top1: 0.9098 - acc_top5: 0.9979 - 51ms/step\n",
      "step 450/782 - loss: 0.1529 - acc_top1: 0.9094 - acc_top5: 0.9979 - 51ms/step\n",
      "step 460/782 - loss: 0.1153 - acc_top1: 0.9093 - acc_top5: 0.9978 - 51ms/step\n",
      "step 470/782 - loss: 0.3516 - acc_top1: 0.9089 - acc_top5: 0.9978 - 51ms/step\n",
      "step 480/782 - loss: 0.2827 - acc_top1: 0.9087 - acc_top5: 0.9977 - 51ms/step\n",
      "step 490/782 - loss: 0.1533 - acc_top1: 0.9084 - acc_top5: 0.9977 - 51ms/step\n",
      "step 500/782 - loss: 0.0711 - acc_top1: 0.9084 - acc_top5: 0.9977 - 51ms/step\n",
      "step 510/782 - loss: 0.3341 - acc_top1: 0.9081 - acc_top5: 0.9976 - 51ms/step\n",
      "step 520/782 - loss: 0.2422 - acc_top1: 0.9080 - acc_top5: 0.9976 - 51ms/step\n",
      "step 530/782 - loss: 0.2224 - acc_top1: 0.9081 - acc_top5: 0.9976 - 51ms/step\n",
      "step 540/782 - loss: 0.2764 - acc_top1: 0.9081 - acc_top5: 0.9976 - 51ms/step\n",
      "step 550/782 - loss: 0.3717 - acc_top1: 0.9078 - acc_top5: 0.9976 - 51ms/step\n",
      "step 560/782 - loss: 0.4076 - acc_top1: 0.9078 - acc_top5: 0.9975 - 51ms/step\n",
      "step 570/782 - loss: 0.2006 - acc_top1: 0.9078 - acc_top5: 0.9976 - 51ms/step\n",
      "step 580/782 - loss: 0.2096 - acc_top1: 0.9079 - acc_top5: 0.9975 - 51ms/step\n",
      "step 590/782 - loss: 0.2205 - acc_top1: 0.9078 - acc_top5: 0.9975 - 51ms/step\n",
      "step 600/782 - loss: 0.2395 - acc_top1: 0.9080 - acc_top5: 0.9975 - 51ms/step\n",
      "step 610/782 - loss: 0.2121 - acc_top1: 0.9085 - acc_top5: 0.9975 - 51ms/step\n",
      "step 620/782 - loss: 0.2444 - acc_top1: 0.9087 - acc_top5: 0.9976 - 51ms/step\n",
      "step 630/782 - loss: 0.2148 - acc_top1: 0.9087 - acc_top5: 0.9975 - 51ms/step\n",
      "step 640/782 - loss: 0.1829 - acc_top1: 0.9086 - acc_top5: 0.9975 - 51ms/step\n",
      "step 650/782 - loss: 0.3043 - acc_top1: 0.9088 - acc_top5: 0.9975 - 51ms/step\n",
      "step 660/782 - loss: 0.2873 - acc_top1: 0.9087 - acc_top5: 0.9976 - 51ms/step\n",
      "step 670/782 - loss: 0.3267 - acc_top1: 0.9087 - acc_top5: 0.9976 - 51ms/step\n",
      "step 680/782 - loss: 0.3760 - acc_top1: 0.9089 - acc_top5: 0.9976 - 51ms/step\n",
      "step 690/782 - loss: 0.2093 - acc_top1: 0.9088 - acc_top5: 0.9976 - 51ms/step\n",
      "step 700/782 - loss: 0.2110 - acc_top1: 0.9085 - acc_top5: 0.9976 - 51ms/step\n",
      "step 710/782 - loss: 0.2866 - acc_top1: 0.9081 - acc_top5: 0.9976 - 51ms/step\n",
      "step 720/782 - loss: 0.3240 - acc_top1: 0.9081 - acc_top5: 0.9976 - 51ms/step\n",
      "step 730/782 - loss: 0.2472 - acc_top1: 0.9079 - acc_top5: 0.9976 - 51ms/step\n",
      "step 740/782 - loss: 0.2776 - acc_top1: 0.9079 - acc_top5: 0.9976 - 51ms/step\n",
      "step 750/782 - loss: 0.3249 - acc_top1: 0.9078 - acc_top5: 0.9976 - 51ms/step\n",
      "step 760/782 - loss: 0.2297 - acc_top1: 0.9078 - acc_top5: 0.9975 - 51ms/step\n",
      "step 770/782 - loss: 0.1626 - acc_top1: 0.9080 - acc_top5: 0.9976 - 51ms/step\n",
      "step 780/782 - loss: 0.2008 - acc_top1: 0.9079 - acc_top5: 0.9976 - 51ms/step\n",
      "step 782/782 - loss: 0.3018 - acc_top1: 0.9079 - acc_top5: 0.9976 - 51ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\32\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.1794 - acc_top1: 0.6156 - acc_top5: 0.9578 - 19ms/step\n",
      "step  20/157 - loss: 2.5280 - acc_top1: 0.6195 - acc_top5: 0.9578 - 19ms/step\n",
      "step  30/157 - loss: 1.9087 - acc_top1: 0.6177 - acc_top5: 0.9510 - 19ms/step\n",
      "step  40/157 - loss: 2.3024 - acc_top1: 0.6234 - acc_top5: 0.9523 - 19ms/step\n",
      "step  50/157 - loss: 1.4911 - acc_top1: 0.6241 - acc_top5: 0.9519 - 19ms/step\n",
      "step  60/157 - loss: 2.3685 - acc_top1: 0.6240 - acc_top5: 0.9503 - 19ms/step\n",
      "step  70/157 - loss: 3.1803 - acc_top1: 0.6241 - acc_top5: 0.9493 - 19ms/step\n",
      "step  80/157 - loss: 1.2938 - acc_top1: 0.6234 - acc_top5: 0.9488 - 19ms/step\n",
      "step  90/157 - loss: 1.5139 - acc_top1: 0.6222 - acc_top5: 0.9490 - 19ms/step\n",
      "step 100/157 - loss: 2.3819 - acc_top1: 0.6203 - acc_top5: 0.9489 - 19ms/step\n",
      "step 110/157 - loss: 2.8501 - acc_top1: 0.6162 - acc_top5: 0.9486 - 19ms/step\n",
      "step 120/157 - loss: 2.0305 - acc_top1: 0.6155 - acc_top5: 0.9484 - 19ms/step\n",
      "step 130/157 - loss: 2.0595 - acc_top1: 0.6163 - acc_top5: 0.9496 - 19ms/step\n",
      "step 140/157 - loss: 2.7423 - acc_top1: 0.6158 - acc_top5: 0.9497 - 19ms/step\n",
      "step 150/157 - loss: 5.6827 - acc_top1: 0.6148 - acc_top5: 0.9501 - 19ms/step\n",
      "step 157/157 - loss: 1.3142 - acc_top1: 0.6140 - acc_top5: 0.9501 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.3701 - acc_top1: 0.9328 - acc_top5: 1.0000 - 52ms/step\n",
      "step  20/782 - loss: 0.3457 - acc_top1: 0.9313 - acc_top5: 0.9977 - 51ms/step\n",
      "step  30/782 - loss: 0.0680 - acc_top1: 0.9375 - acc_top5: 0.9964 - 51ms/step\n",
      "step  40/782 - loss: 0.1443 - acc_top1: 0.9379 - acc_top5: 0.9969 - 51ms/step\n",
      "step  50/782 - loss: 0.0827 - acc_top1: 0.9366 - acc_top5: 0.9966 - 50ms/step\n",
      "step  60/782 - loss: 0.1993 - acc_top1: 0.9367 - acc_top5: 0.9966 - 50ms/step\n",
      "step  70/782 - loss: 0.2572 - acc_top1: 0.9350 - acc_top5: 0.9971 - 50ms/step\n",
      "step  80/782 - loss: 0.1901 - acc_top1: 0.9336 - acc_top5: 0.9975 - 50ms/step\n",
      "step  90/782 - loss: 0.4165 - acc_top1: 0.9307 - acc_top5: 0.9974 - 50ms/step\n",
      "step 100/782 - loss: 0.0978 - acc_top1: 0.9313 - acc_top5: 0.9975 - 50ms/step\n",
      "step 110/782 - loss: 0.2065 - acc_top1: 0.9307 - acc_top5: 0.9977 - 50ms/step\n",
      "step 120/782 - loss: 0.1510 - acc_top1: 0.9316 - acc_top5: 0.9977 - 50ms/step\n",
      "step 130/782 - loss: 0.1372 - acc_top1: 0.9315 - acc_top5: 0.9975 - 50ms/step\n",
      "step 140/782 - loss: 0.1975 - acc_top1: 0.9308 - acc_top5: 0.9974 - 50ms/step\n",
      "step 150/782 - loss: 0.3117 - acc_top1: 0.9317 - acc_top5: 0.9976 - 50ms/step\n",
      "step 160/782 - loss: 0.0907 - acc_top1: 0.9318 - acc_top5: 0.9978 - 50ms/step\n",
      "step 170/782 - loss: 0.2412 - acc_top1: 0.9325 - acc_top5: 0.9977 - 51ms/step\n",
      "step 180/782 - loss: 0.1645 - acc_top1: 0.9332 - acc_top5: 0.9977 - 51ms/step\n",
      "step 190/782 - loss: 0.2282 - acc_top1: 0.9339 - acc_top5: 0.9976 - 51ms/step\n",
      "step 200/782 - loss: 0.3591 - acc_top1: 0.9334 - acc_top5: 0.9977 - 51ms/step\n",
      "step 210/782 - loss: 0.1932 - acc_top1: 0.9336 - acc_top5: 0.9978 - 51ms/step\n",
      "step 220/782 - loss: 0.1102 - acc_top1: 0.9332 - acc_top5: 0.9978 - 51ms/step\n",
      "step 230/782 - loss: 0.2851 - acc_top1: 0.9329 - acc_top5: 0.9978 - 51ms/step\n",
      "step 240/782 - loss: 0.1168 - acc_top1: 0.9311 - acc_top5: 0.9977 - 51ms/step\n",
      "step 250/782 - loss: 0.1994 - acc_top1: 0.9303 - acc_top5: 0.9978 - 51ms/step\n",
      "step 260/782 - loss: 0.1655 - acc_top1: 0.9304 - acc_top5: 0.9978 - 51ms/step\n",
      "step 270/782 - loss: 0.3258 - acc_top1: 0.9295 - acc_top5: 0.9977 - 51ms/step\n",
      "step 280/782 - loss: 0.1433 - acc_top1: 0.9291 - acc_top5: 0.9976 - 51ms/step\n",
      "step 290/782 - loss: 0.5565 - acc_top1: 0.9282 - acc_top5: 0.9974 - 51ms/step\n",
      "step 300/782 - loss: 0.1430 - acc_top1: 0.9278 - acc_top5: 0.9973 - 51ms/step\n",
      "step 310/782 - loss: 0.1755 - acc_top1: 0.9273 - acc_top5: 0.9974 - 51ms/step\n",
      "step 320/782 - loss: 0.2505 - acc_top1: 0.9274 - acc_top5: 0.9975 - 51ms/step\n",
      "step 330/782 - loss: 0.3785 - acc_top1: 0.9270 - acc_top5: 0.9975 - 51ms/step\n",
      "step 340/782 - loss: 0.1351 - acc_top1: 0.9268 - acc_top5: 0.9976 - 51ms/step\n",
      "step 350/782 - loss: 0.3299 - acc_top1: 0.9265 - acc_top5: 0.9976 - 50ms/step\n",
      "step 360/782 - loss: 0.1894 - acc_top1: 0.9261 - acc_top5: 0.9976 - 50ms/step\n",
      "step 370/782 - loss: 0.1337 - acc_top1: 0.9258 - acc_top5: 0.9976 - 50ms/step\n",
      "step 380/782 - loss: 0.0701 - acc_top1: 0.9261 - acc_top5: 0.9976 - 50ms/step\n",
      "step 390/782 - loss: 0.2802 - acc_top1: 0.9262 - acc_top5: 0.9977 - 50ms/step\n",
      "step 400/782 - loss: 0.1393 - acc_top1: 0.9260 - acc_top5: 0.9977 - 50ms/step\n",
      "step 410/782 - loss: 0.2295 - acc_top1: 0.9256 - acc_top5: 0.9978 - 50ms/step\n",
      "step 420/782 - loss: 0.3043 - acc_top1: 0.9258 - acc_top5: 0.9978 - 50ms/step\n",
      "step 430/782 - loss: 0.2097 - acc_top1: 0.9254 - acc_top5: 0.9978 - 50ms/step\n",
      "step 440/782 - loss: 0.3254 - acc_top1: 0.9250 - acc_top5: 0.9978 - 50ms/step\n",
      "step 450/782 - loss: 0.2309 - acc_top1: 0.9251 - acc_top5: 0.9977 - 50ms/step\n",
      "step 460/782 - loss: 0.3892 - acc_top1: 0.9251 - acc_top5: 0.9978 - 50ms/step\n",
      "step 470/782 - loss: 0.2874 - acc_top1: 0.9254 - acc_top5: 0.9977 - 50ms/step\n",
      "step 480/782 - loss: 0.2256 - acc_top1: 0.9255 - acc_top5: 0.9977 - 50ms/step\n",
      "step 490/782 - loss: 0.3454 - acc_top1: 0.9253 - acc_top5: 0.9978 - 50ms/step\n",
      "step 500/782 - loss: 0.1738 - acc_top1: 0.9256 - acc_top5: 0.9978 - 50ms/step\n",
      "step 510/782 - loss: 0.2728 - acc_top1: 0.9250 - acc_top5: 0.9977 - 50ms/step\n",
      "step 520/782 - loss: 0.3758 - acc_top1: 0.9248 - acc_top5: 0.9977 - 50ms/step\n",
      "step 530/782 - loss: 0.3305 - acc_top1: 0.9249 - acc_top5: 0.9977 - 50ms/step\n",
      "step 540/782 - loss: 0.4318 - acc_top1: 0.9247 - acc_top5: 0.9977 - 50ms/step\n",
      "step 550/782 - loss: 0.2242 - acc_top1: 0.9246 - acc_top5: 0.9977 - 50ms/step\n",
      "step 560/782 - loss: 0.1739 - acc_top1: 0.9245 - acc_top5: 0.9977 - 50ms/step\n",
      "step 570/782 - loss: 0.1221 - acc_top1: 0.9245 - acc_top5: 0.9977 - 50ms/step\n",
      "step 580/782 - loss: 0.2266 - acc_top1: 0.9243 - acc_top5: 0.9977 - 50ms/step\n",
      "step 590/782 - loss: 0.2122 - acc_top1: 0.9243 - acc_top5: 0.9976 - 50ms/step\n",
      "step 600/782 - loss: 0.3423 - acc_top1: 0.9238 - acc_top5: 0.9976 - 50ms/step\n",
      "step 610/782 - loss: 0.1634 - acc_top1: 0.9235 - acc_top5: 0.9976 - 50ms/step\n",
      "step 620/782 - loss: 0.2499 - acc_top1: 0.9234 - acc_top5: 0.9977 - 50ms/step\n",
      "step 630/782 - loss: 0.3419 - acc_top1: 0.9235 - acc_top5: 0.9977 - 50ms/step\n",
      "step 640/782 - loss: 0.2791 - acc_top1: 0.9232 - acc_top5: 0.9977 - 50ms/step\n",
      "step 650/782 - loss: 0.1950 - acc_top1: 0.9227 - acc_top5: 0.9976 - 50ms/step\n",
      "step 660/782 - loss: 0.2062 - acc_top1: 0.9228 - acc_top5: 0.9976 - 50ms/step\n",
      "step 670/782 - loss: 0.2684 - acc_top1: 0.9226 - acc_top5: 0.9976 - 50ms/step\n",
      "step 680/782 - loss: 0.3604 - acc_top1: 0.9223 - acc_top5: 0.9977 - 50ms/step\n",
      "step 690/782 - loss: 0.3025 - acc_top1: 0.9222 - acc_top5: 0.9976 - 50ms/step\n",
      "step 700/782 - loss: 0.3137 - acc_top1: 0.9221 - acc_top5: 0.9976 - 50ms/step\n",
      "step 710/782 - loss: 0.1880 - acc_top1: 0.9221 - acc_top5: 0.9976 - 50ms/step\n",
      "step 720/782 - loss: 0.3443 - acc_top1: 0.9218 - acc_top5: 0.9976 - 50ms/step\n",
      "step 730/782 - loss: 0.1728 - acc_top1: 0.9217 - acc_top5: 0.9977 - 50ms/step\n",
      "step 740/782 - loss: 0.2424 - acc_top1: 0.9217 - acc_top5: 0.9977 - 50ms/step\n",
      "step 750/782 - loss: 0.3210 - acc_top1: 0.9218 - acc_top5: 0.9976 - 51ms/step\n",
      "step 760/782 - loss: 0.3633 - acc_top1: 0.9216 - acc_top5: 0.9977 - 51ms/step\n",
      "step 770/782 - loss: 0.3157 - acc_top1: 0.9209 - acc_top5: 0.9976 - 51ms/step\n",
      "step 780/782 - loss: 0.1914 - acc_top1: 0.9206 - acc_top5: 0.9976 - 51ms/step\n",
      "step 782/782 - loss: 0.9553 - acc_top1: 0.9206 - acc_top5: 0.9976 - 51ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\33\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.0551 - acc_top1: 0.6062 - acc_top5: 0.9688 - 21ms/step\n",
      "step  20/157 - loss: 2.7122 - acc_top1: 0.6211 - acc_top5: 0.9594 - 20ms/step\n",
      "step  30/157 - loss: 1.8358 - acc_top1: 0.6250 - acc_top5: 0.9552 - 20ms/step\n",
      "step  40/157 - loss: 2.3251 - acc_top1: 0.6258 - acc_top5: 0.9535 - 20ms/step\n",
      "step  50/157 - loss: 1.3876 - acc_top1: 0.6200 - acc_top5: 0.9525 - 20ms/step\n",
      "step  60/157 - loss: 1.4862 - acc_top1: 0.6198 - acc_top5: 0.9477 - 20ms/step\n",
      "step  70/157 - loss: 1.9560 - acc_top1: 0.6208 - acc_top5: 0.9482 - 20ms/step\n",
      "step  80/157 - loss: 1.4465 - acc_top1: 0.6178 - acc_top5: 0.9486 - 20ms/step\n",
      "step  90/157 - loss: 1.2764 - acc_top1: 0.6168 - acc_top5: 0.9500 - 20ms/step\n",
      "step 100/157 - loss: 1.9461 - acc_top1: 0.6173 - acc_top5: 0.9498 - 20ms/step\n",
      "step 110/157 - loss: 2.4193 - acc_top1: 0.6155 - acc_top5: 0.9499 - 20ms/step\n",
      "step 120/157 - loss: 2.0912 - acc_top1: 0.6158 - acc_top5: 0.9492 - 20ms/step\n",
      "step 130/157 - loss: 1.9498 - acc_top1: 0.6179 - acc_top5: 0.9504 - 20ms/step\n",
      "step 140/157 - loss: 1.6892 - acc_top1: 0.6174 - acc_top5: 0.9491 - 20ms/step\n",
      "step 150/157 - loss: 1.6785 - acc_top1: 0.6182 - acc_top5: 0.9492 - 20ms/step\n",
      "step 157/157 - loss: 1.9144 - acc_top1: 0.6167 - acc_top5: 0.9494 - 20ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.1737 - acc_top1: 0.9125 - acc_top5: 0.9938 - 53ms/step\n",
      "step  20/782 - loss: 0.2930 - acc_top1: 0.9242 - acc_top5: 0.9969 - 52ms/step\n",
      "step  30/782 - loss: 0.0867 - acc_top1: 0.9286 - acc_top5: 0.9974 - 52ms/step\n",
      "step  40/782 - loss: 0.3539 - acc_top1: 0.9262 - acc_top5: 0.9977 - 52ms/step\n",
      "step  50/782 - loss: 0.2001 - acc_top1: 0.9266 - acc_top5: 0.9981 - 52ms/step\n",
      "step  60/782 - loss: 0.1549 - acc_top1: 0.9271 - acc_top5: 0.9979 - 52ms/step\n",
      "step  70/782 - loss: 0.1772 - acc_top1: 0.9281 - acc_top5: 0.9978 - 52ms/step\n",
      "step  80/782 - loss: 0.1088 - acc_top1: 0.9297 - acc_top5: 0.9980 - 52ms/step\n",
      "step  90/782 - loss: 0.1330 - acc_top1: 0.9293 - acc_top5: 0.9979 - 52ms/step\n",
      "step 100/782 - loss: 0.1586 - acc_top1: 0.9300 - acc_top5: 0.9981 - 53ms/step\n",
      "step 110/782 - loss: 0.0980 - acc_top1: 0.9297 - acc_top5: 0.9982 - 53ms/step\n",
      "step 120/782 - loss: 0.1067 - acc_top1: 0.9316 - acc_top5: 0.9980 - 53ms/step\n",
      "step 130/782 - loss: 0.0851 - acc_top1: 0.9320 - acc_top5: 0.9978 - 53ms/step\n",
      "step 140/782 - loss: 0.1242 - acc_top1: 0.9321 - acc_top5: 0.9980 - 52ms/step\n",
      "step 150/782 - loss: 0.1147 - acc_top1: 0.9332 - acc_top5: 0.9979 - 52ms/step\n",
      "step 160/782 - loss: 0.1681 - acc_top1: 0.9338 - acc_top5: 0.9978 - 52ms/step\n",
      "step 170/782 - loss: 0.1021 - acc_top1: 0.9343 - acc_top5: 0.9976 - 52ms/step\n",
      "step 180/782 - loss: 0.2736 - acc_top1: 0.9342 - acc_top5: 0.9977 - 52ms/step\n",
      "step 190/782 - loss: 0.0837 - acc_top1: 0.9343 - acc_top5: 0.9979 - 52ms/step\n",
      "step 200/782 - loss: 0.1831 - acc_top1: 0.9331 - acc_top5: 0.9977 - 52ms/step\n",
      "step 210/782 - loss: 0.2213 - acc_top1: 0.9333 - acc_top5: 0.9978 - 53ms/step\n",
      "step 220/782 - loss: 0.1140 - acc_top1: 0.9340 - acc_top5: 0.9978 - 53ms/step\n",
      "step 230/782 - loss: 0.1890 - acc_top1: 0.9344 - acc_top5: 0.9978 - 53ms/step\n",
      "step 240/782 - loss: 0.1255 - acc_top1: 0.9342 - acc_top5: 0.9979 - 52ms/step\n",
      "step 250/782 - loss: 0.1234 - acc_top1: 0.9345 - acc_top5: 0.9979 - 52ms/step\n",
      "step 260/782 - loss: 0.2641 - acc_top1: 0.9346 - acc_top5: 0.9978 - 52ms/step\n",
      "step 270/782 - loss: 0.1999 - acc_top1: 0.9345 - acc_top5: 0.9979 - 52ms/step\n",
      "step 280/782 - loss: 0.1309 - acc_top1: 0.9345 - acc_top5: 0.9979 - 52ms/step\n",
      "step 290/782 - loss: 0.2156 - acc_top1: 0.9341 - acc_top5: 0.9978 - 52ms/step\n",
      "step 300/782 - loss: 0.3359 - acc_top1: 0.9335 - acc_top5: 0.9978 - 52ms/step\n",
      "step 310/782 - loss: 0.2313 - acc_top1: 0.9333 - acc_top5: 0.9979 - 52ms/step\n",
      "step 320/782 - loss: 0.2576 - acc_top1: 0.9323 - acc_top5: 0.9979 - 52ms/step\n",
      "step 330/782 - loss: 0.2858 - acc_top1: 0.9317 - acc_top5: 0.9978 - 52ms/step\n",
      "step 340/782 - loss: 0.2892 - acc_top1: 0.9314 - acc_top5: 0.9978 - 52ms/step\n",
      "step 350/782 - loss: 0.2555 - acc_top1: 0.9315 - acc_top5: 0.9977 - 52ms/step\n",
      "step 360/782 - loss: 0.1711 - acc_top1: 0.9312 - acc_top5: 0.9978 - 52ms/step\n",
      "step 370/782 - loss: 0.4454 - acc_top1: 0.9307 - acc_top5: 0.9978 - 52ms/step\n",
      "step 380/782 - loss: 0.1397 - acc_top1: 0.9308 - acc_top5: 0.9978 - 52ms/step\n",
      "step 390/782 - loss: 0.0974 - acc_top1: 0.9310 - acc_top5: 0.9978 - 52ms/step\n",
      "step 400/782 - loss: 0.2232 - acc_top1: 0.9312 - acc_top5: 0.9979 - 52ms/step\n",
      "step 410/782 - loss: 0.2147 - acc_top1: 0.9310 - acc_top5: 0.9978 - 52ms/step\n",
      "step 420/782 - loss: 0.1982 - acc_top1: 0.9311 - acc_top5: 0.9977 - 52ms/step\n",
      "step 430/782 - loss: 0.1840 - acc_top1: 0.9312 - acc_top5: 0.9977 - 52ms/step\n",
      "step 440/782 - loss: 0.1490 - acc_top1: 0.9311 - acc_top5: 0.9977 - 52ms/step\n",
      "step 450/782 - loss: 0.1611 - acc_top1: 0.9309 - acc_top5: 0.9977 - 52ms/step\n",
      "step 460/782 - loss: 0.2941 - acc_top1: 0.9310 - acc_top5: 0.9978 - 52ms/step\n",
      "step 470/782 - loss: 0.2320 - acc_top1: 0.9309 - acc_top5: 0.9977 - 52ms/step\n",
      "step 480/782 - loss: 0.3661 - acc_top1: 0.9303 - acc_top5: 0.9977 - 52ms/step\n",
      "step 490/782 - loss: 0.2015 - acc_top1: 0.9302 - acc_top5: 0.9977 - 52ms/step\n",
      "step 500/782 - loss: 0.3706 - acc_top1: 0.9298 - acc_top5: 0.9977 - 52ms/step\n",
      "step 510/782 - loss: 0.2003 - acc_top1: 0.9298 - acc_top5: 0.9978 - 52ms/step\n",
      "step 520/782 - loss: 0.1564 - acc_top1: 0.9296 - acc_top5: 0.9978 - 52ms/step\n",
      "step 530/782 - loss: 0.2439 - acc_top1: 0.9294 - acc_top5: 0.9978 - 52ms/step\n",
      "step 540/782 - loss: 0.2465 - acc_top1: 0.9296 - acc_top5: 0.9978 - 52ms/step\n",
      "step 550/782 - loss: 0.1983 - acc_top1: 0.9297 - acc_top5: 0.9978 - 52ms/step\n",
      "step 560/782 - loss: 0.1468 - acc_top1: 0.9296 - acc_top5: 0.9979 - 52ms/step\n",
      "step 570/782 - loss: 0.1736 - acc_top1: 0.9296 - acc_top5: 0.9979 - 52ms/step\n",
      "step 580/782 - loss: 0.1280 - acc_top1: 0.9297 - acc_top5: 0.9979 - 52ms/step\n",
      "step 590/782 - loss: 0.1131 - acc_top1: 0.9298 - acc_top5: 0.9979 - 52ms/step\n",
      "step 600/782 - loss: 0.0984 - acc_top1: 0.9296 - acc_top5: 0.9979 - 52ms/step\n",
      "step 610/782 - loss: 0.0942 - acc_top1: 0.9297 - acc_top5: 0.9980 - 52ms/step\n",
      "step 620/782 - loss: 0.3208 - acc_top1: 0.9293 - acc_top5: 0.9980 - 52ms/step\n",
      "step 630/782 - loss: 0.2381 - acc_top1: 0.9291 - acc_top5: 0.9980 - 52ms/step\n",
      "step 640/782 - loss: 0.1379 - acc_top1: 0.9289 - acc_top5: 0.9980 - 52ms/step\n",
      "step 650/782 - loss: 0.1732 - acc_top1: 0.9288 - acc_top5: 0.9980 - 52ms/step\n",
      "step 660/782 - loss: 0.3038 - acc_top1: 0.9284 - acc_top5: 0.9980 - 52ms/step\n",
      "step 670/782 - loss: 0.3367 - acc_top1: 0.9281 - acc_top5: 0.9980 - 52ms/step\n",
      "step 680/782 - loss: 0.4295 - acc_top1: 0.9279 - acc_top5: 0.9980 - 52ms/step\n",
      "step 690/782 - loss: 0.2547 - acc_top1: 0.9279 - acc_top5: 0.9980 - 52ms/step\n",
      "step 700/782 - loss: 0.3033 - acc_top1: 0.9279 - acc_top5: 0.9980 - 52ms/step\n",
      "step 710/782 - loss: 0.1736 - acc_top1: 0.9279 - acc_top5: 0.9980 - 52ms/step\n",
      "step 720/782 - loss: 0.1943 - acc_top1: 0.9278 - acc_top5: 0.9980 - 52ms/step\n",
      "step 730/782 - loss: 0.0943 - acc_top1: 0.9278 - acc_top5: 0.9980 - 52ms/step\n",
      "step 740/782 - loss: 0.1156 - acc_top1: 0.9277 - acc_top5: 0.9980 - 52ms/step\n",
      "step 750/782 - loss: 0.2100 - acc_top1: 0.9276 - acc_top5: 0.9980 - 52ms/step\n",
      "step 760/782 - loss: 0.2651 - acc_top1: 0.9279 - acc_top5: 0.9981 - 52ms/step\n",
      "step 770/782 - loss: 0.3357 - acc_top1: 0.9278 - acc_top5: 0.9981 - 52ms/step\n",
      "step 780/782 - loss: 0.3259 - acc_top1: 0.9277 - acc_top5: 0.9981 - 52ms/step\n",
      "step 782/782 - loss: 1.0782 - acc_top1: 0.9277 - acc_top5: 0.9981 - 52ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\34\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.2499 - acc_top1: 0.6453 - acc_top5: 0.9563 - 19ms/step\n",
      "step  20/157 - loss: 2.5452 - acc_top1: 0.6305 - acc_top5: 0.9547 - 19ms/step\n",
      "step  30/157 - loss: 1.8774 - acc_top1: 0.6255 - acc_top5: 0.9516 - 19ms/step\n",
      "step  40/157 - loss: 2.3961 - acc_top1: 0.6285 - acc_top5: 0.9512 - 19ms/step\n",
      "step  50/157 - loss: 1.6046 - acc_top1: 0.6253 - acc_top5: 0.9484 - 19ms/step\n",
      "step  60/157 - loss: 1.5527 - acc_top1: 0.6279 - acc_top5: 0.9477 - 19ms/step\n",
      "step  70/157 - loss: 3.5319 - acc_top1: 0.6259 - acc_top5: 0.9475 - 19ms/step\n",
      "step  80/157 - loss: 1.8896 - acc_top1: 0.6227 - acc_top5: 0.9453 - 19ms/step\n",
      "step  90/157 - loss: 1.3568 - acc_top1: 0.6241 - acc_top5: 0.9469 - 19ms/step\n",
      "step 100/157 - loss: 1.9425 - acc_top1: 0.6242 - acc_top5: 0.9475 - 19ms/step\n",
      "step 110/157 - loss: 2.9861 - acc_top1: 0.6227 - acc_top5: 0.9469 - 19ms/step\n",
      "step 120/157 - loss: 2.2754 - acc_top1: 0.6216 - acc_top5: 0.9460 - 19ms/step\n",
      "step 130/157 - loss: 1.9275 - acc_top1: 0.6212 - acc_top5: 0.9470 - 19ms/step\n",
      "step 140/157 - loss: 3.1132 - acc_top1: 0.6211 - acc_top5: 0.9475 - 19ms/step\n",
      "step 150/157 - loss: 5.8491 - acc_top1: 0.6199 - acc_top5: 0.9474 - 19ms/step\n",
      "step 157/157 - loss: 1.7104 - acc_top1: 0.6188 - acc_top5: 0.9475 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.4911 - acc_top1: 0.8984 - acc_top5: 0.9984 - 52ms/step\n",
      "step  20/782 - loss: 0.4414 - acc_top1: 0.8906 - acc_top5: 0.9969 - 52ms/step\n",
      "step  30/782 - loss: 0.1517 - acc_top1: 0.8938 - acc_top5: 0.9974 - 51ms/step\n",
      "step  40/782 - loss: 0.2131 - acc_top1: 0.9027 - acc_top5: 0.9977 - 51ms/step\n",
      "step  50/782 - loss: 0.2007 - acc_top1: 0.9059 - acc_top5: 0.9978 - 51ms/step\n",
      "step  60/782 - loss: 0.2045 - acc_top1: 0.9109 - acc_top5: 0.9977 - 51ms/step\n",
      "step  70/782 - loss: 0.2812 - acc_top1: 0.9138 - acc_top5: 0.9975 - 51ms/step\n",
      "step  80/782 - loss: 0.2795 - acc_top1: 0.9152 - acc_top5: 0.9975 - 51ms/step\n",
      "step  90/782 - loss: 0.1722 - acc_top1: 0.9161 - acc_top5: 0.9972 - 51ms/step\n",
      "step 100/782 - loss: 0.1274 - acc_top1: 0.9183 - acc_top5: 0.9973 - 51ms/step\n",
      "step 110/782 - loss: 0.1332 - acc_top1: 0.9193 - acc_top5: 0.9976 - 51ms/step\n",
      "step 120/782 - loss: 0.2411 - acc_top1: 0.9219 - acc_top5: 0.9977 - 51ms/step\n",
      "step 130/782 - loss: 0.2040 - acc_top1: 0.9231 - acc_top5: 0.9975 - 51ms/step\n",
      "step 140/782 - loss: 0.1230 - acc_top1: 0.9232 - acc_top5: 0.9975 - 51ms/step\n",
      "step 150/782 - loss: 0.1899 - acc_top1: 0.9248 - acc_top5: 0.9977 - 51ms/step\n",
      "step 160/782 - loss: 0.1195 - acc_top1: 0.9262 - acc_top5: 0.9979 - 51ms/step\n",
      "step 170/782 - loss: 0.1677 - acc_top1: 0.9278 - acc_top5: 0.9980 - 51ms/step\n",
      "step 180/782 - loss: 0.0891 - acc_top1: 0.9292 - acc_top5: 0.9981 - 51ms/step\n",
      "step 190/782 - loss: 0.1370 - acc_top1: 0.9302 - acc_top5: 0.9979 - 51ms/step\n",
      "step 200/782 - loss: 0.3582 - acc_top1: 0.9308 - acc_top5: 0.9980 - 51ms/step\n",
      "step 210/782 - loss: 0.1609 - acc_top1: 0.9313 - acc_top5: 0.9980 - 51ms/step\n",
      "step 220/782 - loss: 0.1631 - acc_top1: 0.9321 - acc_top5: 0.9981 - 51ms/step\n",
      "step 230/782 - loss: 0.1571 - acc_top1: 0.9329 - acc_top5: 0.9982 - 51ms/step\n",
      "step 240/782 - loss: 0.1134 - acc_top1: 0.9333 - acc_top5: 0.9980 - 51ms/step\n",
      "step 250/782 - loss: 0.1305 - acc_top1: 0.9340 - acc_top5: 0.9980 - 50ms/step\n",
      "step 260/782 - loss: 0.1727 - acc_top1: 0.9341 - acc_top5: 0.9980 - 50ms/step\n",
      "step 270/782 - loss: 0.1984 - acc_top1: 0.9342 - acc_top5: 0.9980 - 50ms/step\n",
      "step 280/782 - loss: 0.1483 - acc_top1: 0.9336 - acc_top5: 0.9981 - 50ms/step\n",
      "step 290/782 - loss: 0.1289 - acc_top1: 0.9339 - acc_top5: 0.9982 - 50ms/step\n",
      "step 300/782 - loss: 0.2104 - acc_top1: 0.9334 - acc_top5: 0.9982 - 50ms/step\n",
      "step 310/782 - loss: 0.1535 - acc_top1: 0.9338 - acc_top5: 0.9982 - 50ms/step\n",
      "step 320/782 - loss: 0.3309 - acc_top1: 0.9335 - acc_top5: 0.9982 - 50ms/step\n",
      "step 330/782 - loss: 0.2154 - acc_top1: 0.9336 - acc_top5: 0.9982 - 51ms/step\n",
      "step 340/782 - loss: 0.2168 - acc_top1: 0.9341 - acc_top5: 0.9982 - 51ms/step\n",
      "step 350/782 - loss: 0.4910 - acc_top1: 0.9342 - acc_top5: 0.9982 - 50ms/step\n",
      "step 360/782 - loss: 0.1094 - acc_top1: 0.9345 - acc_top5: 0.9982 - 50ms/step\n",
      "step 370/782 - loss: 0.1110 - acc_top1: 0.9345 - acc_top5: 0.9982 - 50ms/step\n",
      "step 380/782 - loss: 0.0944 - acc_top1: 0.9347 - acc_top5: 0.9982 - 50ms/step\n",
      "step 390/782 - loss: 0.1113 - acc_top1: 0.9348 - acc_top5: 0.9982 - 50ms/step\n",
      "step 400/782 - loss: 0.2011 - acc_top1: 0.9347 - acc_top5: 0.9982 - 50ms/step\n",
      "step 410/782 - loss: 0.1383 - acc_top1: 0.9352 - acc_top5: 0.9982 - 51ms/step\n",
      "step 420/782 - loss: 0.1571 - acc_top1: 0.9355 - acc_top5: 0.9982 - 51ms/step\n",
      "step 430/782 - loss: 0.4648 - acc_top1: 0.9350 - acc_top5: 0.9981 - 51ms/step\n",
      "step 440/782 - loss: 0.1658 - acc_top1: 0.9349 - acc_top5: 0.9982 - 51ms/step\n",
      "step 450/782 - loss: 0.2294 - acc_top1: 0.9350 - acc_top5: 0.9981 - 51ms/step\n",
      "step 460/782 - loss: 0.2552 - acc_top1: 0.9350 - acc_top5: 0.9981 - 51ms/step\n",
      "step 470/782 - loss: 0.2631 - acc_top1: 0.9349 - acc_top5: 0.9981 - 51ms/step\n",
      "step 480/782 - loss: 0.2092 - acc_top1: 0.9352 - acc_top5: 0.9980 - 51ms/step\n",
      "step 490/782 - loss: 0.2555 - acc_top1: 0.9349 - acc_top5: 0.9980 - 51ms/step\n",
      "step 500/782 - loss: 0.1704 - acc_top1: 0.9350 - acc_top5: 0.9979 - 51ms/step\n",
      "step 510/782 - loss: 0.2186 - acc_top1: 0.9349 - acc_top5: 0.9979 - 51ms/step\n",
      "step 520/782 - loss: 0.2608 - acc_top1: 0.9349 - acc_top5: 0.9979 - 51ms/step\n",
      "step 530/782 - loss: 0.2210 - acc_top1: 0.9348 - acc_top5: 0.9979 - 51ms/step\n",
      "step 540/782 - loss: 0.1232 - acc_top1: 0.9348 - acc_top5: 0.9979 - 51ms/step\n",
      "step 550/782 - loss: 0.2381 - acc_top1: 0.9350 - acc_top5: 0.9980 - 51ms/step\n",
      "step 560/782 - loss: 0.0790 - acc_top1: 0.9350 - acc_top5: 0.9979 - 51ms/step\n",
      "step 570/782 - loss: 0.3498 - acc_top1: 0.9350 - acc_top5: 0.9979 - 51ms/step\n",
      "step 580/782 - loss: 0.1923 - acc_top1: 0.9350 - acc_top5: 0.9980 - 51ms/step\n",
      "step 590/782 - loss: 0.1862 - acc_top1: 0.9351 - acc_top5: 0.9980 - 51ms/step\n",
      "step 600/782 - loss: 0.2470 - acc_top1: 0.9349 - acc_top5: 0.9980 - 51ms/step\n",
      "step 610/782 - loss: 0.2350 - acc_top1: 0.9350 - acc_top5: 0.9980 - 51ms/step\n",
      "step 620/782 - loss: 0.1866 - acc_top1: 0.9349 - acc_top5: 0.9980 - 51ms/step\n",
      "step 630/782 - loss: 0.1374 - acc_top1: 0.9348 - acc_top5: 0.9980 - 51ms/step\n",
      "step 640/782 - loss: 0.1834 - acc_top1: 0.9346 - acc_top5: 0.9980 - 51ms/step\n",
      "step 650/782 - loss: 0.3169 - acc_top1: 0.9343 - acc_top5: 0.9980 - 51ms/step\n",
      "step 660/782 - loss: 0.0929 - acc_top1: 0.9341 - acc_top5: 0.9980 - 51ms/step\n",
      "step 670/782 - loss: 0.1610 - acc_top1: 0.9339 - acc_top5: 0.9980 - 50ms/step\n",
      "step 680/782 - loss: 0.1413 - acc_top1: 0.9337 - acc_top5: 0.9980 - 50ms/step\n",
      "step 690/782 - loss: 0.0764 - acc_top1: 0.9337 - acc_top5: 0.9980 - 50ms/step\n",
      "step 700/782 - loss: 0.0381 - acc_top1: 0.9339 - acc_top5: 0.9980 - 50ms/step\n",
      "step 710/782 - loss: 0.2447 - acc_top1: 0.9337 - acc_top5: 0.9980 - 50ms/step\n",
      "step 720/782 - loss: 0.1918 - acc_top1: 0.9335 - acc_top5: 0.9980 - 50ms/step\n",
      "step 730/782 - loss: 0.0918 - acc_top1: 0.9335 - acc_top5: 0.9981 - 50ms/step\n",
      "step 740/782 - loss: 0.3686 - acc_top1: 0.9332 - acc_top5: 0.9981 - 50ms/step\n",
      "step 750/782 - loss: 0.2845 - acc_top1: 0.9329 - acc_top5: 0.9981 - 50ms/step\n",
      "step 760/782 - loss: 0.3136 - acc_top1: 0.9327 - acc_top5: 0.9981 - 50ms/step\n",
      "step 770/782 - loss: 0.3058 - acc_top1: 0.9323 - acc_top5: 0.9981 - 50ms/step\n",
      "step 780/782 - loss: 0.1141 - acc_top1: 0.9321 - acc_top5: 0.9981 - 50ms/step\n",
      "step 782/782 - loss: 0.1708 - acc_top1: 0.9321 - acc_top5: 0.9981 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\35\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.4002 - acc_top1: 0.6125 - acc_top5: 0.9547 - 19ms/step\n",
      "step  20/157 - loss: 1.9766 - acc_top1: 0.6281 - acc_top5: 0.9617 - 18ms/step\n",
      "step  30/157 - loss: 1.6561 - acc_top1: 0.6240 - acc_top5: 0.9563 - 18ms/step\n",
      "step  40/157 - loss: 2.4687 - acc_top1: 0.6215 - acc_top5: 0.9559 - 18ms/step\n",
      "step  50/157 - loss: 1.1517 - acc_top1: 0.6194 - acc_top5: 0.9550 - 18ms/step\n",
      "step  60/157 - loss: 1.6352 - acc_top1: 0.6185 - acc_top5: 0.9523 - 18ms/step\n",
      "step  70/157 - loss: 1.9400 - acc_top1: 0.6210 - acc_top5: 0.9509 - 18ms/step\n",
      "step  80/157 - loss: 1.6424 - acc_top1: 0.6205 - acc_top5: 0.9506 - 18ms/step\n",
      "step  90/157 - loss: 1.4011 - acc_top1: 0.6215 - acc_top5: 0.9502 - 18ms/step\n",
      "step 100/157 - loss: 1.8390 - acc_top1: 0.6212 - acc_top5: 0.9498 - 18ms/step\n",
      "step 110/157 - loss: 2.4899 - acc_top1: 0.6180 - acc_top5: 0.9479 - 18ms/step\n",
      "step 120/157 - loss: 1.9353 - acc_top1: 0.6173 - acc_top5: 0.9466 - 18ms/step\n",
      "step 130/157 - loss: 2.1013 - acc_top1: 0.6171 - acc_top5: 0.9481 - 18ms/step\n",
      "step 140/157 - loss: 2.1230 - acc_top1: 0.6169 - acc_top5: 0.9480 - 18ms/step\n",
      "step 150/157 - loss: 2.1582 - acc_top1: 0.6173 - acc_top5: 0.9479 - 18ms/step\n",
      "step 157/157 - loss: 0.8466 - acc_top1: 0.6154 - acc_top5: 0.9479 - 18ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.1392 - acc_top1: 0.9547 - acc_top5: 0.9984 - 50ms/step\n",
      "step  20/782 - loss: 0.0740 - acc_top1: 0.9523 - acc_top5: 0.9992 - 50ms/step\n",
      "step  30/782 - loss: 0.1402 - acc_top1: 0.9521 - acc_top5: 0.9995 - 50ms/step\n",
      "step  40/782 - loss: 0.1778 - acc_top1: 0.9484 - acc_top5: 0.9996 - 50ms/step\n",
      "step  50/782 - loss: 0.0879 - acc_top1: 0.9478 - acc_top5: 0.9994 - 50ms/step\n",
      "step  60/782 - loss: 0.1313 - acc_top1: 0.9487 - acc_top5: 0.9995 - 50ms/step\n",
      "step  70/782 - loss: 0.2282 - acc_top1: 0.9484 - acc_top5: 0.9993 - 50ms/step\n",
      "step  80/782 - loss: 0.1097 - acc_top1: 0.9479 - acc_top5: 0.9992 - 50ms/step\n",
      "step  90/782 - loss: 0.1085 - acc_top1: 0.9481 - acc_top5: 0.9993 - 50ms/step\n",
      "step 100/782 - loss: 0.1283 - acc_top1: 0.9486 - acc_top5: 0.9994 - 50ms/step\n",
      "step 110/782 - loss: 0.2271 - acc_top1: 0.9487 - acc_top5: 0.9993 - 50ms/step\n",
      "step 120/782 - loss: 0.0842 - acc_top1: 0.9478 - acc_top5: 0.9992 - 50ms/step\n",
      "step 130/782 - loss: 0.0447 - acc_top1: 0.9486 - acc_top5: 0.9990 - 50ms/step\n",
      "step 140/782 - loss: 0.1514 - acc_top1: 0.9489 - acc_top5: 0.9991 - 50ms/step\n",
      "step 150/782 - loss: 0.2091 - acc_top1: 0.9499 - acc_top5: 0.9992 - 50ms/step\n",
      "step 160/782 - loss: 0.1298 - acc_top1: 0.9496 - acc_top5: 0.9991 - 50ms/step\n",
      "step 170/782 - loss: 0.4240 - acc_top1: 0.9486 - acc_top5: 0.9989 - 50ms/step\n",
      "step 180/782 - loss: 0.0489 - acc_top1: 0.9488 - acc_top5: 0.9988 - 50ms/step\n",
      "step 190/782 - loss: 0.0460 - acc_top1: 0.9487 - acc_top5: 0.9987 - 50ms/step\n",
      "step 200/782 - loss: 0.2403 - acc_top1: 0.9483 - acc_top5: 0.9987 - 50ms/step\n",
      "step 210/782 - loss: 0.1560 - acc_top1: 0.9487 - acc_top5: 0.9987 - 50ms/step\n",
      "step 220/782 - loss: 0.1975 - acc_top1: 0.9489 - acc_top5: 0.9988 - 50ms/step\n",
      "step 230/782 - loss: 0.1267 - acc_top1: 0.9487 - acc_top5: 0.9988 - 50ms/step\n",
      "step 240/782 - loss: 0.1505 - acc_top1: 0.9483 - acc_top5: 0.9988 - 50ms/step\n",
      "step 250/782 - loss: 0.1531 - acc_top1: 0.9484 - acc_top5: 0.9988 - 50ms/step\n",
      "step 260/782 - loss: 0.2801 - acc_top1: 0.9483 - acc_top5: 0.9985 - 50ms/step\n",
      "step 270/782 - loss: 0.1377 - acc_top1: 0.9483 - acc_top5: 0.9983 - 50ms/step\n",
      "step 280/782 - loss: 0.1954 - acc_top1: 0.9487 - acc_top5: 0.9983 - 50ms/step\n",
      "step 290/782 - loss: 0.1058 - acc_top1: 0.9490 - acc_top5: 0.9984 - 50ms/step\n",
      "step 300/782 - loss: 0.2384 - acc_top1: 0.9493 - acc_top5: 0.9984 - 50ms/step\n",
      "step 310/782 - loss: 0.3814 - acc_top1: 0.9497 - acc_top5: 0.9984 - 50ms/step\n",
      "step 320/782 - loss: 0.1254 - acc_top1: 0.9500 - acc_top5: 0.9984 - 50ms/step\n",
      "step 330/782 - loss: 0.1761 - acc_top1: 0.9504 - acc_top5: 0.9984 - 50ms/step\n",
      "step 340/782 - loss: 0.1524 - acc_top1: 0.9504 - acc_top5: 0.9985 - 50ms/step\n",
      "step 350/782 - loss: 0.2125 - acc_top1: 0.9498 - acc_top5: 0.9985 - 50ms/step\n",
      "step 360/782 - loss: 0.1546 - acc_top1: 0.9495 - acc_top5: 0.9985 - 50ms/step\n",
      "step 370/782 - loss: 0.1350 - acc_top1: 0.9493 - acc_top5: 0.9985 - 50ms/step\n",
      "step 380/782 - loss: 0.0770 - acc_top1: 0.9493 - acc_top5: 0.9985 - 50ms/step\n",
      "step 390/782 - loss: 0.0796 - acc_top1: 0.9491 - acc_top5: 0.9986 - 50ms/step\n",
      "step 400/782 - loss: 0.1092 - acc_top1: 0.9488 - acc_top5: 0.9986 - 50ms/step\n",
      "step 410/782 - loss: 0.1231 - acc_top1: 0.9489 - acc_top5: 0.9985 - 50ms/step\n",
      "step 420/782 - loss: 0.0725 - acc_top1: 0.9489 - acc_top5: 0.9985 - 50ms/step\n",
      "step 430/782 - loss: 0.1267 - acc_top1: 0.9488 - acc_top5: 0.9984 - 50ms/step\n",
      "step 440/782 - loss: 0.1447 - acc_top1: 0.9484 - acc_top5: 0.9985 - 50ms/step\n",
      "step 450/782 - loss: 0.1559 - acc_top1: 0.9483 - acc_top5: 0.9985 - 50ms/step\n",
      "step 460/782 - loss: 0.2164 - acc_top1: 0.9482 - acc_top5: 0.9985 - 50ms/step\n",
      "step 470/782 - loss: 0.1255 - acc_top1: 0.9482 - acc_top5: 0.9985 - 50ms/step\n",
      "step 480/782 - loss: 0.1194 - acc_top1: 0.9478 - acc_top5: 0.9985 - 50ms/step\n",
      "step 490/782 - loss: 0.2232 - acc_top1: 0.9476 - acc_top5: 0.9985 - 50ms/step\n",
      "step 500/782 - loss: 0.2450 - acc_top1: 0.9475 - acc_top5: 0.9985 - 50ms/step\n",
      "step 510/782 - loss: 0.0638 - acc_top1: 0.9473 - acc_top5: 0.9985 - 50ms/step\n",
      "step 520/782 - loss: 0.2181 - acc_top1: 0.9472 - acc_top5: 0.9985 - 50ms/step\n",
      "step 530/782 - loss: 0.2662 - acc_top1: 0.9468 - acc_top5: 0.9985 - 50ms/step\n",
      "step 540/782 - loss: 0.0500 - acc_top1: 0.9464 - acc_top5: 0.9985 - 50ms/step\n",
      "step 550/782 - loss: 0.2568 - acc_top1: 0.9462 - acc_top5: 0.9985 - 50ms/step\n",
      "step 560/782 - loss: 0.0461 - acc_top1: 0.9462 - acc_top5: 0.9985 - 50ms/step\n",
      "step 570/782 - loss: 0.1307 - acc_top1: 0.9463 - acc_top5: 0.9984 - 50ms/step\n",
      "step 580/782 - loss: 0.1732 - acc_top1: 0.9459 - acc_top5: 0.9984 - 50ms/step\n",
      "step 590/782 - loss: 0.1646 - acc_top1: 0.9455 - acc_top5: 0.9984 - 50ms/step\n",
      "step 600/782 - loss: 0.1640 - acc_top1: 0.9453 - acc_top5: 0.9984 - 50ms/step\n",
      "step 610/782 - loss: 0.1952 - acc_top1: 0.9450 - acc_top5: 0.9984 - 50ms/step\n",
      "step 620/782 - loss: 0.2945 - acc_top1: 0.9447 - acc_top5: 0.9984 - 50ms/step\n",
      "step 630/782 - loss: 0.0459 - acc_top1: 0.9448 - acc_top5: 0.9984 - 50ms/step\n",
      "step 640/782 - loss: 0.0702 - acc_top1: 0.9445 - acc_top5: 0.9984 - 50ms/step\n",
      "step 650/782 - loss: 0.1116 - acc_top1: 0.9444 - acc_top5: 0.9984 - 50ms/step\n",
      "step 660/782 - loss: 0.1737 - acc_top1: 0.9442 - acc_top5: 0.9984 - 50ms/step\n",
      "step 670/782 - loss: 0.1084 - acc_top1: 0.9441 - acc_top5: 0.9984 - 50ms/step\n",
      "step 680/782 - loss: 0.0330 - acc_top1: 0.9441 - acc_top5: 0.9984 - 50ms/step\n",
      "step 690/782 - loss: 0.2564 - acc_top1: 0.9442 - acc_top5: 0.9984 - 50ms/step\n",
      "step 700/782 - loss: 0.1010 - acc_top1: 0.9442 - acc_top5: 0.9984 - 50ms/step\n",
      "step 710/782 - loss: 0.2097 - acc_top1: 0.9442 - acc_top5: 0.9985 - 50ms/step\n",
      "step 720/782 - loss: 0.1088 - acc_top1: 0.9440 - acc_top5: 0.9984 - 50ms/step\n",
      "step 730/782 - loss: 0.1754 - acc_top1: 0.9437 - acc_top5: 0.9985 - 50ms/step\n",
      "step 740/782 - loss: 0.1215 - acc_top1: 0.9435 - acc_top5: 0.9985 - 50ms/step\n",
      "step 750/782 - loss: 0.2086 - acc_top1: 0.9434 - acc_top5: 0.9984 - 50ms/step\n",
      "step 760/782 - loss: 0.1110 - acc_top1: 0.9434 - acc_top5: 0.9985 - 50ms/step\n",
      "step 770/782 - loss: 0.1125 - acc_top1: 0.9432 - acc_top5: 0.9985 - 50ms/step\n",
      "step 780/782 - loss: 0.1663 - acc_top1: 0.9430 - acc_top5: 0.9985 - 50ms/step\n",
      "step 782/782 - loss: 0.1191 - acc_top1: 0.9430 - acc_top5: 0.9985 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\36\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.3864 - acc_top1: 0.6484 - acc_top5: 0.9656 - 18ms/step\n",
      "step  20/157 - loss: 2.6049 - acc_top1: 0.6328 - acc_top5: 0.9648 - 18ms/step\n",
      "step  30/157 - loss: 1.7122 - acc_top1: 0.6297 - acc_top5: 0.9635 - 18ms/step\n",
      "step  40/157 - loss: 2.5575 - acc_top1: 0.6281 - acc_top5: 0.9609 - 18ms/step\n",
      "step  50/157 - loss: 1.8523 - acc_top1: 0.6253 - acc_top5: 0.9603 - 18ms/step\n",
      "step  60/157 - loss: 1.5743 - acc_top1: 0.6260 - acc_top5: 0.9596 - 18ms/step\n",
      "step  70/157 - loss: 1.9725 - acc_top1: 0.6263 - acc_top5: 0.9583 - 18ms/step\n",
      "step  80/157 - loss: 1.9938 - acc_top1: 0.6248 - acc_top5: 0.9564 - 18ms/step\n",
      "step  90/157 - loss: 1.4121 - acc_top1: 0.6226 - acc_top5: 0.9573 - 18ms/step\n",
      "step 100/157 - loss: 1.7257 - acc_top1: 0.6216 - acc_top5: 0.9570 - 18ms/step\n",
      "step 110/157 - loss: 2.3530 - acc_top1: 0.6185 - acc_top5: 0.9551 - 18ms/step\n",
      "step 120/157 - loss: 1.9712 - acc_top1: 0.6191 - acc_top5: 0.9547 - 18ms/step\n",
      "step 130/157 - loss: 1.6561 - acc_top1: 0.6196 - acc_top5: 0.9549 - 18ms/step\n",
      "step 140/157 - loss: 1.9595 - acc_top1: 0.6193 - acc_top5: 0.9545 - 18ms/step\n",
      "step 150/157 - loss: 1.7225 - acc_top1: 0.6197 - acc_top5: 0.9548 - 18ms/step\n",
      "step 157/157 - loss: 1.4506 - acc_top1: 0.6182 - acc_top5: 0.9542 - 18ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.0408 - acc_top1: 0.9406 - acc_top5: 0.9984 - 50ms/step\n",
      "step  20/782 - loss: 0.1389 - acc_top1: 0.9391 - acc_top5: 0.9992 - 50ms/step\n",
      "step  30/782 - loss: 0.0775 - acc_top1: 0.9391 - acc_top5: 0.9984 - 50ms/step\n",
      "step  40/782 - loss: 0.0881 - acc_top1: 0.9430 - acc_top5: 0.9984 - 50ms/step\n",
      "step  50/782 - loss: 0.1390 - acc_top1: 0.9463 - acc_top5: 0.9981 - 50ms/step\n",
      "step  60/782 - loss: 0.1458 - acc_top1: 0.9492 - acc_top5: 0.9984 - 50ms/step\n",
      "step  70/782 - loss: 0.1617 - acc_top1: 0.9500 - acc_top5: 0.9987 - 50ms/step\n",
      "step  80/782 - loss: 0.2237 - acc_top1: 0.9510 - acc_top5: 0.9988 - 50ms/step\n",
      "step  90/782 - loss: 0.1315 - acc_top1: 0.9502 - acc_top5: 0.9986 - 50ms/step\n",
      "step 100/782 - loss: 0.1117 - acc_top1: 0.9506 - acc_top5: 0.9988 - 50ms/step\n",
      "step 110/782 - loss: 0.0957 - acc_top1: 0.9504 - acc_top5: 0.9989 - 50ms/step\n",
      "step 120/782 - loss: 0.1302 - acc_top1: 0.9505 - acc_top5: 0.9990 - 50ms/step\n",
      "step 130/782 - loss: 0.1400 - acc_top1: 0.9508 - acc_top5: 0.9988 - 50ms/step\n",
      "step 140/782 - loss: 0.1972 - acc_top1: 0.9498 - acc_top5: 0.9989 - 50ms/step\n",
      "step 150/782 - loss: 0.1499 - acc_top1: 0.9490 - acc_top5: 0.9989 - 50ms/step\n",
      "step 160/782 - loss: 0.3828 - acc_top1: 0.9481 - acc_top5: 0.9988 - 50ms/step\n",
      "step 170/782 - loss: 0.1206 - acc_top1: 0.9479 - acc_top5: 0.9988 - 50ms/step\n",
      "step 180/782 - loss: 0.0700 - acc_top1: 0.9478 - acc_top5: 0.9988 - 50ms/step\n",
      "step 190/782 - loss: 0.0943 - acc_top1: 0.9482 - acc_top5: 0.9985 - 50ms/step\n",
      "step 200/782 - loss: 0.0423 - acc_top1: 0.9487 - acc_top5: 0.9984 - 50ms/step\n",
      "step 210/782 - loss: 0.1556 - acc_top1: 0.9487 - acc_top5: 0.9984 - 50ms/step\n",
      "step 220/782 - loss: 0.1307 - acc_top1: 0.9494 - acc_top5: 0.9984 - 50ms/step\n",
      "step 230/782 - loss: 0.1371 - acc_top1: 0.9490 - acc_top5: 0.9985 - 50ms/step\n",
      "step 240/782 - loss: 0.0395 - acc_top1: 0.9500 - acc_top5: 0.9986 - 50ms/step\n",
      "step 250/782 - loss: 0.1964 - acc_top1: 0.9503 - acc_top5: 0.9986 - 50ms/step\n",
      "step 260/782 - loss: 0.0860 - acc_top1: 0.9504 - acc_top5: 0.9987 - 50ms/step\n",
      "step 270/782 - loss: 0.0848 - acc_top1: 0.9508 - acc_top5: 0.9987 - 50ms/step\n",
      "step 280/782 - loss: 0.1704 - acc_top1: 0.9507 - acc_top5: 0.9987 - 50ms/step\n",
      "step 290/782 - loss: 0.1037 - acc_top1: 0.9508 - acc_top5: 0.9987 - 50ms/step\n",
      "step 300/782 - loss: 0.0832 - acc_top1: 0.9506 - acc_top5: 0.9988 - 50ms/step\n",
      "step 310/782 - loss: 0.1542 - acc_top1: 0.9508 - acc_top5: 0.9987 - 50ms/step\n",
      "step 320/782 - loss: 0.0879 - acc_top1: 0.9509 - acc_top5: 0.9987 - 50ms/step\n",
      "step 330/782 - loss: 0.1041 - acc_top1: 0.9507 - acc_top5: 0.9988 - 50ms/step\n",
      "step 340/782 - loss: 0.0718 - acc_top1: 0.9509 - acc_top5: 0.9988 - 50ms/step\n",
      "step 350/782 - loss: 0.1560 - acc_top1: 0.9513 - acc_top5: 0.9988 - 50ms/step\n",
      "step 360/782 - loss: 0.1727 - acc_top1: 0.9508 - acc_top5: 0.9987 - 50ms/step\n",
      "step 370/782 - loss: 0.1117 - acc_top1: 0.9505 - acc_top5: 0.9986 - 50ms/step\n",
      "step 380/782 - loss: 0.1482 - acc_top1: 0.9506 - acc_top5: 0.9986 - 50ms/step\n",
      "step 390/782 - loss: 0.2481 - acc_top1: 0.9503 - acc_top5: 0.9986 - 50ms/step\n",
      "step 400/782 - loss: 0.1952 - acc_top1: 0.9499 - acc_top5: 0.9986 - 50ms/step\n",
      "step 410/782 - loss: 0.1425 - acc_top1: 0.9497 - acc_top5: 0.9986 - 50ms/step\n",
      "step 420/782 - loss: 0.1565 - acc_top1: 0.9497 - acc_top5: 0.9986 - 50ms/step\n",
      "step 430/782 - loss: 0.0517 - acc_top1: 0.9501 - acc_top5: 0.9987 - 50ms/step\n",
      "step 440/782 - loss: 0.2828 - acc_top1: 0.9499 - acc_top5: 0.9987 - 50ms/step\n",
      "step 450/782 - loss: 0.2073 - acc_top1: 0.9498 - acc_top5: 0.9986 - 50ms/step\n",
      "step 460/782 - loss: 0.1868 - acc_top1: 0.9498 - acc_top5: 0.9986 - 50ms/step\n",
      "step 470/782 - loss: 0.1380 - acc_top1: 0.9499 - acc_top5: 0.9986 - 50ms/step\n",
      "step 480/782 - loss: 0.1011 - acc_top1: 0.9501 - acc_top5: 0.9986 - 50ms/step\n",
      "step 490/782 - loss: 0.0730 - acc_top1: 0.9500 - acc_top5: 0.9986 - 50ms/step\n",
      "step 500/782 - loss: 0.0387 - acc_top1: 0.9502 - acc_top5: 0.9986 - 50ms/step\n",
      "step 510/782 - loss: 0.2035 - acc_top1: 0.9500 - acc_top5: 0.9986 - 50ms/step\n",
      "step 520/782 - loss: 0.1086 - acc_top1: 0.9497 - acc_top5: 0.9986 - 50ms/step\n",
      "step 530/782 - loss: 0.1227 - acc_top1: 0.9497 - acc_top5: 0.9986 - 50ms/step\n",
      "step 540/782 - loss: 0.1861 - acc_top1: 0.9495 - acc_top5: 0.9986 - 50ms/step\n",
      "step 550/782 - loss: 0.2699 - acc_top1: 0.9494 - acc_top5: 0.9986 - 50ms/step\n",
      "step 560/782 - loss: 0.0955 - acc_top1: 0.9491 - acc_top5: 0.9986 - 50ms/step\n",
      "step 570/782 - loss: 0.3655 - acc_top1: 0.9488 - acc_top5: 0.9986 - 50ms/step\n",
      "step 580/782 - loss: 0.1365 - acc_top1: 0.9487 - acc_top5: 0.9985 - 50ms/step\n",
      "step 590/782 - loss: 0.2608 - acc_top1: 0.9484 - acc_top5: 0.9986 - 50ms/step\n",
      "step 600/782 - loss: 0.1374 - acc_top1: 0.9485 - acc_top5: 0.9986 - 50ms/step\n",
      "step 610/782 - loss: 0.1317 - acc_top1: 0.9485 - acc_top5: 0.9986 - 50ms/step\n",
      "step 620/782 - loss: 0.2392 - acc_top1: 0.9486 - acc_top5: 0.9986 - 50ms/step\n",
      "step 630/782 - loss: 0.1565 - acc_top1: 0.9485 - acc_top5: 0.9987 - 50ms/step\n",
      "step 640/782 - loss: 0.2014 - acc_top1: 0.9481 - acc_top5: 0.9987 - 50ms/step\n",
      "step 650/782 - loss: 0.1725 - acc_top1: 0.9481 - acc_top5: 0.9987 - 50ms/step\n",
      "step 660/782 - loss: 0.1115 - acc_top1: 0.9479 - acc_top5: 0.9987 - 50ms/step\n",
      "step 670/782 - loss: 0.0587 - acc_top1: 0.9478 - acc_top5: 0.9987 - 50ms/step\n",
      "step 680/782 - loss: 0.1040 - acc_top1: 0.9479 - acc_top5: 0.9987 - 50ms/step\n",
      "step 690/782 - loss: 0.1277 - acc_top1: 0.9479 - acc_top5: 0.9987 - 50ms/step\n",
      "step 700/782 - loss: 0.1429 - acc_top1: 0.9479 - acc_top5: 0.9987 - 50ms/step\n",
      "step 710/782 - loss: 0.1643 - acc_top1: 0.9481 - acc_top5: 0.9987 - 50ms/step\n",
      "step 720/782 - loss: 0.0612 - acc_top1: 0.9481 - acc_top5: 0.9987 - 50ms/step\n",
      "step 730/782 - loss: 0.2162 - acc_top1: 0.9480 - acc_top5: 0.9987 - 50ms/step\n",
      "step 740/782 - loss: 0.2916 - acc_top1: 0.9480 - acc_top5: 0.9986 - 50ms/step\n",
      "step 750/782 - loss: 0.2165 - acc_top1: 0.9479 - acc_top5: 0.9986 - 50ms/step\n",
      "step 760/782 - loss: 0.2045 - acc_top1: 0.9478 - acc_top5: 0.9986 - 50ms/step\n",
      "step 770/782 - loss: 0.1557 - acc_top1: 0.9478 - acc_top5: 0.9987 - 50ms/step\n",
      "step 780/782 - loss: 0.1312 - acc_top1: 0.9477 - acc_top5: 0.9986 - 50ms/step\n",
      "step 782/782 - loss: 0.8813 - acc_top1: 0.9477 - acc_top5: 0.9986 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\37\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.4539 - acc_top1: 0.6297 - acc_top5: 0.9578 - 19ms/step\n",
      "step  20/157 - loss: 2.8703 - acc_top1: 0.6391 - acc_top5: 0.9578 - 18ms/step\n",
      "step  30/157 - loss: 1.9986 - acc_top1: 0.6312 - acc_top5: 0.9536 - 18ms/step\n",
      "step  40/157 - loss: 3.0550 - acc_top1: 0.6285 - acc_top5: 0.9508 - 18ms/step\n",
      "step  50/157 - loss: 1.8623 - acc_top1: 0.6284 - acc_top5: 0.9531 - 18ms/step\n",
      "step  60/157 - loss: 1.5959 - acc_top1: 0.6281 - acc_top5: 0.9508 - 18ms/step\n",
      "step  70/157 - loss: 1.9754 - acc_top1: 0.6353 - acc_top5: 0.9507 - 18ms/step\n",
      "step  80/157 - loss: 1.8053 - acc_top1: 0.6346 - acc_top5: 0.9512 - 18ms/step\n",
      "step  90/157 - loss: 1.3217 - acc_top1: 0.6351 - acc_top5: 0.9517 - 18ms/step\n",
      "step 100/157 - loss: 1.9261 - acc_top1: 0.6331 - acc_top5: 0.9520 - 18ms/step\n",
      "step 110/157 - loss: 2.7867 - acc_top1: 0.6294 - acc_top5: 0.9518 - 18ms/step\n",
      "step 120/157 - loss: 2.4270 - acc_top1: 0.6296 - acc_top5: 0.9516 - 18ms/step\n",
      "step 130/157 - loss: 2.0179 - acc_top1: 0.6302 - acc_top5: 0.9518 - 18ms/step\n",
      "step 140/157 - loss: 2.0974 - acc_top1: 0.6277 - acc_top5: 0.9516 - 18ms/step\n",
      "step 150/157 - loss: 2.2758 - acc_top1: 0.6283 - acc_top5: 0.9523 - 18ms/step\n",
      "step 157/157 - loss: 1.1971 - acc_top1: 0.6273 - acc_top5: 0.9520 - 18ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.1438 - acc_top1: 0.9500 - acc_top5: 0.9984 - 50ms/step\n",
      "step  20/782 - loss: 0.1896 - acc_top1: 0.9484 - acc_top5: 0.9984 - 52ms/step\n",
      "step  30/782 - loss: 0.1166 - acc_top1: 0.9464 - acc_top5: 0.9984 - 51ms/step\n",
      "step  40/782 - loss: 0.1314 - acc_top1: 0.9437 - acc_top5: 0.9980 - 51ms/step\n",
      "step  50/782 - loss: 0.2655 - acc_top1: 0.9431 - acc_top5: 0.9984 - 51ms/step\n",
      "step  60/782 - loss: 0.2088 - acc_top1: 0.9417 - acc_top5: 0.9984 - 50ms/step\n",
      "step  70/782 - loss: 0.0918 - acc_top1: 0.9420 - acc_top5: 0.9984 - 50ms/step\n",
      "step  80/782 - loss: 0.2686 - acc_top1: 0.9432 - acc_top5: 0.9986 - 50ms/step\n",
      "step  90/782 - loss: 0.0762 - acc_top1: 0.9439 - acc_top5: 0.9988 - 50ms/step\n",
      "step 100/782 - loss: 0.1398 - acc_top1: 0.9455 - acc_top5: 0.9984 - 50ms/step\n",
      "step 110/782 - loss: 0.0598 - acc_top1: 0.9450 - acc_top5: 0.9986 - 50ms/step\n",
      "step 120/782 - loss: 0.1567 - acc_top1: 0.9456 - acc_top5: 0.9987 - 50ms/step\n",
      "step 130/782 - loss: 0.1595 - acc_top1: 0.9463 - acc_top5: 0.9988 - 50ms/step\n",
      "step 140/782 - loss: 0.0938 - acc_top1: 0.9477 - acc_top5: 0.9989 - 50ms/step\n",
      "step 150/782 - loss: 0.1449 - acc_top1: 0.9481 - acc_top5: 0.9990 - 50ms/step\n",
      "step 160/782 - loss: 0.1741 - acc_top1: 0.9485 - acc_top5: 0.9988 - 50ms/step\n",
      "step 170/782 - loss: 0.1935 - acc_top1: 0.9495 - acc_top5: 0.9988 - 50ms/step\n",
      "step 180/782 - loss: 0.0952 - acc_top1: 0.9498 - acc_top5: 0.9988 - 50ms/step\n",
      "step 190/782 - loss: 0.0961 - acc_top1: 0.9497 - acc_top5: 0.9988 - 50ms/step\n",
      "step 200/782 - loss: 0.3342 - acc_top1: 0.9497 - acc_top5: 0.9988 - 50ms/step\n",
      "step 210/782 - loss: 0.0730 - acc_top1: 0.9499 - acc_top5: 0.9988 - 50ms/step\n",
      "step 220/782 - loss: 0.2963 - acc_top1: 0.9503 - acc_top5: 0.9989 - 50ms/step\n",
      "step 230/782 - loss: 0.1674 - acc_top1: 0.9503 - acc_top5: 0.9988 - 50ms/step\n",
      "step 240/782 - loss: 0.2562 - acc_top1: 0.9495 - acc_top5: 0.9988 - 50ms/step\n",
      "step 250/782 - loss: 0.0496 - acc_top1: 0.9496 - acc_top5: 0.9989 - 50ms/step\n",
      "step 260/782 - loss: 0.0852 - acc_top1: 0.9491 - acc_top5: 0.9989 - 50ms/step\n",
      "step 270/782 - loss: 0.0792 - acc_top1: 0.9491 - acc_top5: 0.9990 - 50ms/step\n",
      "step 280/782 - loss: 0.1342 - acc_top1: 0.9482 - acc_top5: 0.9989 - 50ms/step\n",
      "step 290/782 - loss: 0.0794 - acc_top1: 0.9484 - acc_top5: 0.9989 - 50ms/step\n",
      "step 300/782 - loss: 0.1146 - acc_top1: 0.9483 - acc_top5: 0.9990 - 50ms/step\n",
      "step 310/782 - loss: 0.2576 - acc_top1: 0.9482 - acc_top5: 0.9990 - 50ms/step\n",
      "step 320/782 - loss: 0.0923 - acc_top1: 0.9481 - acc_top5: 0.9989 - 50ms/step\n",
      "step 330/782 - loss: 0.0706 - acc_top1: 0.9486 - acc_top5: 0.9989 - 50ms/step\n",
      "step 340/782 - loss: 0.1813 - acc_top1: 0.9482 - acc_top5: 0.9989 - 50ms/step\n",
      "step 350/782 - loss: 0.1637 - acc_top1: 0.9485 - acc_top5: 0.9989 - 50ms/step\n",
      "step 360/782 - loss: 0.2053 - acc_top1: 0.9480 - acc_top5: 0.9989 - 50ms/step\n",
      "step 370/782 - loss: 0.1006 - acc_top1: 0.9483 - acc_top5: 0.9989 - 50ms/step\n",
      "step 380/782 - loss: 0.2196 - acc_top1: 0.9482 - acc_top5: 0.9988 - 50ms/step\n",
      "step 390/782 - loss: 0.1669 - acc_top1: 0.9482 - acc_top5: 0.9988 - 50ms/step\n",
      "step 400/782 - loss: 0.1729 - acc_top1: 0.9484 - acc_top5: 0.9988 - 50ms/step\n",
      "step 410/782 - loss: 0.1903 - acc_top1: 0.9484 - acc_top5: 0.9988 - 50ms/step\n",
      "step 420/782 - loss: 0.1207 - acc_top1: 0.9487 - acc_top5: 0.9988 - 50ms/step\n",
      "step 430/782 - loss: 0.1697 - acc_top1: 0.9485 - acc_top5: 0.9988 - 50ms/step\n",
      "step 440/782 - loss: 0.1107 - acc_top1: 0.9486 - acc_top5: 0.9988 - 50ms/step\n",
      "step 450/782 - loss: 0.1972 - acc_top1: 0.9483 - acc_top5: 0.9988 - 50ms/step\n",
      "step 460/782 - loss: 0.1808 - acc_top1: 0.9484 - acc_top5: 0.9987 - 50ms/step\n",
      "step 470/782 - loss: 0.3158 - acc_top1: 0.9481 - acc_top5: 0.9987 - 50ms/step\n",
      "step 480/782 - loss: 0.1806 - acc_top1: 0.9480 - acc_top5: 0.9987 - 50ms/step\n",
      "step 490/782 - loss: 0.0637 - acc_top1: 0.9480 - acc_top5: 0.9987 - 50ms/step\n",
      "step 500/782 - loss: 0.1041 - acc_top1: 0.9479 - acc_top5: 0.9987 - 50ms/step\n",
      "step 510/782 - loss: 0.1103 - acc_top1: 0.9478 - acc_top5: 0.9987 - 50ms/step\n",
      "step 520/782 - loss: 0.0827 - acc_top1: 0.9478 - acc_top5: 0.9987 - 50ms/step\n",
      "step 530/782 - loss: 0.0936 - acc_top1: 0.9479 - acc_top5: 0.9987 - 50ms/step\n",
      "step 540/782 - loss: 0.1149 - acc_top1: 0.9481 - acc_top5: 0.9988 - 50ms/step\n",
      "step 550/782 - loss: 0.2291 - acc_top1: 0.9478 - acc_top5: 0.9987 - 50ms/step\n",
      "step 560/782 - loss: 0.1693 - acc_top1: 0.9478 - acc_top5: 0.9987 - 50ms/step\n",
      "step 570/782 - loss: 0.1084 - acc_top1: 0.9478 - acc_top5: 0.9988 - 50ms/step\n",
      "step 580/782 - loss: 0.0933 - acc_top1: 0.9480 - acc_top5: 0.9988 - 50ms/step\n",
      "step 590/782 - loss: 0.2152 - acc_top1: 0.9482 - acc_top5: 0.9988 - 50ms/step\n",
      "step 600/782 - loss: 0.0276 - acc_top1: 0.9482 - acc_top5: 0.9988 - 50ms/step\n",
      "step 610/782 - loss: 0.1949 - acc_top1: 0.9481 - acc_top5: 0.9987 - 50ms/step\n",
      "step 620/782 - loss: 0.1423 - acc_top1: 0.9481 - acc_top5: 0.9987 - 50ms/step\n",
      "step 630/782 - loss: 0.1380 - acc_top1: 0.9480 - acc_top5: 0.9987 - 50ms/step\n",
      "step 640/782 - loss: 0.1439 - acc_top1: 0.9480 - acc_top5: 0.9987 - 50ms/step\n",
      "step 650/782 - loss: 0.1754 - acc_top1: 0.9480 - acc_top5: 0.9987 - 50ms/step\n",
      "step 660/782 - loss: 0.2188 - acc_top1: 0.9480 - acc_top5: 0.9987 - 50ms/step\n",
      "step 670/782 - loss: 0.1872 - acc_top1: 0.9480 - acc_top5: 0.9987 - 50ms/step\n",
      "step 680/782 - loss: 0.1341 - acc_top1: 0.9480 - acc_top5: 0.9987 - 50ms/step\n",
      "step 690/782 - loss: 0.1782 - acc_top1: 0.9479 - acc_top5: 0.9987 - 50ms/step\n",
      "step 700/782 - loss: 0.0376 - acc_top1: 0.9481 - acc_top5: 0.9988 - 50ms/step\n",
      "step 710/782 - loss: 0.1580 - acc_top1: 0.9481 - acc_top5: 0.9987 - 50ms/step\n",
      "step 720/782 - loss: 0.1363 - acc_top1: 0.9480 - acc_top5: 0.9988 - 50ms/step\n",
      "step 730/782 - loss: 0.2239 - acc_top1: 0.9480 - acc_top5: 0.9988 - 50ms/step\n",
      "step 740/782 - loss: 0.1562 - acc_top1: 0.9479 - acc_top5: 0.9988 - 50ms/step\n",
      "step 750/782 - loss: 0.1134 - acc_top1: 0.9477 - acc_top5: 0.9988 - 50ms/step\n",
      "step 760/782 - loss: 0.2113 - acc_top1: 0.9475 - acc_top5: 0.9988 - 50ms/step\n",
      "step 770/782 - loss: 0.1739 - acc_top1: 0.9474 - acc_top5: 0.9987 - 50ms/step\n",
      "step 780/782 - loss: 0.1297 - acc_top1: 0.9474 - acc_top5: 0.9987 - 50ms/step\n",
      "step 782/782 - loss: 0.5981 - acc_top1: 0.9473 - acc_top5: 0.9987 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\38\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.6289 - acc_top1: 0.6188 - acc_top5: 0.9625 - 20ms/step\n",
      "step  20/157 - loss: 3.0112 - acc_top1: 0.6227 - acc_top5: 0.9641 - 20ms/step\n",
      "step  30/157 - loss: 1.7456 - acc_top1: 0.6281 - acc_top5: 0.9609 - 20ms/step\n",
      "step  40/157 - loss: 2.4564 - acc_top1: 0.6289 - acc_top5: 0.9570 - 20ms/step\n",
      "step  50/157 - loss: 1.5078 - acc_top1: 0.6225 - acc_top5: 0.9569 - 20ms/step\n",
      "step  60/157 - loss: 1.2783 - acc_top1: 0.6206 - acc_top5: 0.9531 - 20ms/step\n",
      "step  70/157 - loss: 2.6931 - acc_top1: 0.6234 - acc_top5: 0.9529 - 20ms/step\n",
      "step  80/157 - loss: 1.8956 - acc_top1: 0.6236 - acc_top5: 0.9525 - 20ms/step\n",
      "step  90/157 - loss: 1.4816 - acc_top1: 0.6243 - acc_top5: 0.9528 - 20ms/step\n",
      "step 100/157 - loss: 2.2336 - acc_top1: 0.6255 - acc_top5: 0.9523 - 20ms/step\n",
      "step 110/157 - loss: 2.9345 - acc_top1: 0.6229 - acc_top5: 0.9509 - 20ms/step\n",
      "step 120/157 - loss: 1.9663 - acc_top1: 0.6217 - acc_top5: 0.9507 - 20ms/step\n",
      "step 130/157 - loss: 2.4185 - acc_top1: 0.6225 - acc_top5: 0.9510 - 20ms/step\n",
      "step 140/157 - loss: 2.6818 - acc_top1: 0.6212 - acc_top5: 0.9513 - 20ms/step\n",
      "step 150/157 - loss: 3.7549 - acc_top1: 0.6217 - acc_top5: 0.9514 - 20ms/step\n",
      "step 157/157 - loss: 0.7645 - acc_top1: 0.6199 - acc_top5: 0.9510 - 20ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.5514 - acc_top1: 0.9313 - acc_top5: 0.9969 - 52ms/step\n",
      "step  20/782 - loss: 0.1690 - acc_top1: 0.9336 - acc_top5: 0.9969 - 53ms/step\n",
      "step  30/782 - loss: 0.1056 - acc_top1: 0.9359 - acc_top5: 0.9974 - 53ms/step\n",
      "step  40/782 - loss: 0.2103 - acc_top1: 0.9379 - acc_top5: 0.9973 - 53ms/step\n",
      "step  50/782 - loss: 0.2463 - acc_top1: 0.9387 - acc_top5: 0.9978 - 52ms/step\n",
      "step  60/782 - loss: 0.2105 - acc_top1: 0.9388 - acc_top5: 0.9977 - 52ms/step\n",
      "step  70/782 - loss: 0.0307 - acc_top1: 0.9424 - acc_top5: 0.9980 - 51ms/step\n",
      "step  80/782 - loss: 0.0889 - acc_top1: 0.9447 - acc_top5: 0.9975 - 51ms/step\n",
      "step  90/782 - loss: 0.1075 - acc_top1: 0.9458 - acc_top5: 0.9972 - 51ms/step\n",
      "step 100/782 - loss: 0.2470 - acc_top1: 0.9466 - acc_top5: 0.9972 - 51ms/step\n",
      "step 110/782 - loss: 0.1341 - acc_top1: 0.9472 - acc_top5: 0.9974 - 51ms/step\n",
      "step 120/782 - loss: 0.1060 - acc_top1: 0.9479 - acc_top5: 0.9975 - 51ms/step\n",
      "step 130/782 - loss: 0.1898 - acc_top1: 0.9493 - acc_top5: 0.9977 - 51ms/step\n",
      "step 140/782 - loss: 0.0890 - acc_top1: 0.9500 - acc_top5: 0.9978 - 51ms/step\n",
      "step 150/782 - loss: 0.1214 - acc_top1: 0.9499 - acc_top5: 0.9978 - 51ms/step\n",
      "step 160/782 - loss: 0.1831 - acc_top1: 0.9492 - acc_top5: 0.9979 - 51ms/step\n",
      "step 170/782 - loss: 0.0930 - acc_top1: 0.9496 - acc_top5: 0.9977 - 51ms/step\n",
      "step 180/782 - loss: 0.0725 - acc_top1: 0.9503 - acc_top5: 0.9977 - 51ms/step\n",
      "step 190/782 - loss: 0.1798 - acc_top1: 0.9503 - acc_top5: 0.9978 - 51ms/step\n",
      "step 200/782 - loss: 0.2603 - acc_top1: 0.9505 - acc_top5: 0.9979 - 50ms/step\n",
      "step 210/782 - loss: 0.0402 - acc_top1: 0.9512 - acc_top5: 0.9979 - 50ms/step\n",
      "step 220/782 - loss: 0.1081 - acc_top1: 0.9513 - acc_top5: 0.9979 - 50ms/step\n",
      "step 230/782 - loss: 0.0874 - acc_top1: 0.9513 - acc_top5: 0.9979 - 50ms/step\n",
      "step 240/782 - loss: 0.1741 - acc_top1: 0.9518 - acc_top5: 0.9979 - 50ms/step\n",
      "step 250/782 - loss: 0.1973 - acc_top1: 0.9519 - acc_top5: 0.9979 - 50ms/step\n",
      "step 260/782 - loss: 0.0850 - acc_top1: 0.9517 - acc_top5: 0.9978 - 50ms/step\n",
      "step 270/782 - loss: 0.1364 - acc_top1: 0.9522 - acc_top5: 0.9979 - 50ms/step\n",
      "step 280/782 - loss: 0.1080 - acc_top1: 0.9521 - acc_top5: 0.9980 - 50ms/step\n",
      "step 290/782 - loss: 0.1517 - acc_top1: 0.9523 - acc_top5: 0.9981 - 50ms/step\n",
      "step 300/782 - loss: 0.0564 - acc_top1: 0.9520 - acc_top5: 0.9981 - 50ms/step\n",
      "step 310/782 - loss: 0.1991 - acc_top1: 0.9521 - acc_top5: 0.9981 - 50ms/step\n",
      "step 320/782 - loss: 0.0816 - acc_top1: 0.9521 - acc_top5: 0.9981 - 50ms/step\n",
      "step 330/782 - loss: 0.2021 - acc_top1: 0.9522 - acc_top5: 0.9982 - 50ms/step\n",
      "step 340/782 - loss: 0.1555 - acc_top1: 0.9519 - acc_top5: 0.9982 - 50ms/step\n",
      "step 350/782 - loss: 0.0727 - acc_top1: 0.9517 - acc_top5: 0.9982 - 50ms/step\n",
      "step 360/782 - loss: 0.0913 - acc_top1: 0.9517 - acc_top5: 0.9982 - 50ms/step\n",
      "step 370/782 - loss: 0.1869 - acc_top1: 0.9517 - acc_top5: 0.9981 - 50ms/step\n",
      "step 380/782 - loss: 0.0487 - acc_top1: 0.9521 - acc_top5: 0.9981 - 50ms/step\n",
      "step 390/782 - loss: 0.0788 - acc_top1: 0.9518 - acc_top5: 0.9981 - 50ms/step\n",
      "step 400/782 - loss: 0.0842 - acc_top1: 0.9513 - acc_top5: 0.9981 - 50ms/step\n",
      "step 410/782 - loss: 0.0994 - acc_top1: 0.9514 - acc_top5: 0.9981 - 50ms/step\n",
      "step 420/782 - loss: 0.2527 - acc_top1: 0.9512 - acc_top5: 0.9980 - 50ms/step\n",
      "step 430/782 - loss: 0.0527 - acc_top1: 0.9509 - acc_top5: 0.9981 - 50ms/step\n",
      "step 440/782 - loss: 0.1112 - acc_top1: 0.9511 - acc_top5: 0.9981 - 50ms/step\n",
      "step 450/782 - loss: 0.0939 - acc_top1: 0.9513 - acc_top5: 0.9981 - 50ms/step\n",
      "step 460/782 - loss: 0.2050 - acc_top1: 0.9512 - acc_top5: 0.9981 - 50ms/step\n",
      "step 470/782 - loss: 0.1643 - acc_top1: 0.9511 - acc_top5: 0.9982 - 50ms/step\n",
      "step 480/782 - loss: 0.1976 - acc_top1: 0.9510 - acc_top5: 0.9981 - 50ms/step\n",
      "step 490/782 - loss: 0.1354 - acc_top1: 0.9509 - acc_top5: 0.9981 - 50ms/step\n",
      "step 500/782 - loss: 0.1521 - acc_top1: 0.9512 - acc_top5: 0.9982 - 50ms/step\n",
      "step 510/782 - loss: 0.1155 - acc_top1: 0.9511 - acc_top5: 0.9982 - 50ms/step\n",
      "step 520/782 - loss: 0.2147 - acc_top1: 0.9508 - acc_top5: 0.9982 - 50ms/step\n",
      "step 530/782 - loss: 0.1610 - acc_top1: 0.9508 - acc_top5: 0.9982 - 50ms/step\n",
      "step 540/782 - loss: 0.1362 - acc_top1: 0.9507 - acc_top5: 0.9982 - 50ms/step\n",
      "step 550/782 - loss: 0.1206 - acc_top1: 0.9507 - acc_top5: 0.9983 - 50ms/step\n",
      "step 560/782 - loss: 0.1313 - acc_top1: 0.9504 - acc_top5: 0.9982 - 50ms/step\n",
      "step 570/782 - loss: 0.1211 - acc_top1: 0.9501 - acc_top5: 0.9982 - 50ms/step\n",
      "step 580/782 - loss: 0.1982 - acc_top1: 0.9502 - acc_top5: 0.9982 - 50ms/step\n",
      "step 590/782 - loss: 0.2670 - acc_top1: 0.9501 - acc_top5: 0.9983 - 50ms/step\n",
      "step 600/782 - loss: 0.0901 - acc_top1: 0.9502 - acc_top5: 0.9983 - 50ms/step\n",
      "step 610/782 - loss: 0.2267 - acc_top1: 0.9501 - acc_top5: 0.9983 - 50ms/step\n",
      "step 620/782 - loss: 0.0939 - acc_top1: 0.9501 - acc_top5: 0.9983 - 50ms/step\n",
      "step 630/782 - loss: 0.1982 - acc_top1: 0.9500 - acc_top5: 0.9983 - 50ms/step\n",
      "step 640/782 - loss: 0.2213 - acc_top1: 0.9499 - acc_top5: 0.9983 - 50ms/step\n",
      "step 650/782 - loss: 0.3565 - acc_top1: 0.9499 - acc_top5: 0.9983 - 50ms/step\n",
      "step 660/782 - loss: 0.1290 - acc_top1: 0.9501 - acc_top5: 0.9983 - 50ms/step\n",
      "step 670/782 - loss: 0.2030 - acc_top1: 0.9502 - acc_top5: 0.9983 - 50ms/step\n",
      "step 680/782 - loss: 0.1312 - acc_top1: 0.9503 - acc_top5: 0.9983 - 50ms/step\n",
      "step 690/782 - loss: 0.1731 - acc_top1: 0.9501 - acc_top5: 0.9983 - 50ms/step\n",
      "step 700/782 - loss: 0.2144 - acc_top1: 0.9501 - acc_top5: 0.9983 - 50ms/step\n",
      "step 710/782 - loss: 0.2084 - acc_top1: 0.9500 - acc_top5: 0.9983 - 50ms/step\n",
      "step 720/782 - loss: 0.1056 - acc_top1: 0.9500 - acc_top5: 0.9983 - 50ms/step\n",
      "step 730/782 - loss: 0.1447 - acc_top1: 0.9499 - acc_top5: 0.9983 - 50ms/step\n",
      "step 740/782 - loss: 0.1318 - acc_top1: 0.9500 - acc_top5: 0.9983 - 50ms/step\n",
      "step 750/782 - loss: 0.1653 - acc_top1: 0.9502 - acc_top5: 0.9983 - 50ms/step\n",
      "step 760/782 - loss: 0.0437 - acc_top1: 0.9502 - acc_top5: 0.9983 - 50ms/step\n",
      "step 770/782 - loss: 0.0872 - acc_top1: 0.9502 - acc_top5: 0.9983 - 50ms/step\n",
      "step 780/782 - loss: 0.0747 - acc_top1: 0.9500 - acc_top5: 0.9984 - 50ms/step\n",
      "step 782/782 - loss: 0.3792 - acc_top1: 0.9500 - acc_top5: 0.9984 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\39\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.4356 - acc_top1: 0.6469 - acc_top5: 0.9672 - 18ms/step\n",
      "step  20/157 - loss: 3.6188 - acc_top1: 0.6336 - acc_top5: 0.9625 - 18ms/step\n",
      "step  30/157 - loss: 2.2794 - acc_top1: 0.6229 - acc_top5: 0.9599 - 18ms/step\n",
      "step  40/157 - loss: 2.3436 - acc_top1: 0.6277 - acc_top5: 0.9582 - 18ms/step\n",
      "step  50/157 - loss: 1.5118 - acc_top1: 0.6309 - acc_top5: 0.9575 - 18ms/step\n",
      "step  60/157 - loss: 1.5052 - acc_top1: 0.6315 - acc_top5: 0.9536 - 18ms/step\n",
      "step  70/157 - loss: 1.5992 - acc_top1: 0.6366 - acc_top5: 0.9529 - 18ms/step\n",
      "step  80/157 - loss: 1.9128 - acc_top1: 0.6324 - acc_top5: 0.9529 - 18ms/step\n",
      "step  90/157 - loss: 1.4056 - acc_top1: 0.6314 - acc_top5: 0.9542 - 18ms/step\n",
      "step 100/157 - loss: 2.0975 - acc_top1: 0.6289 - acc_top5: 0.9545 - 18ms/step\n",
      "step 110/157 - loss: 2.4730 - acc_top1: 0.6251 - acc_top5: 0.9537 - 18ms/step\n",
      "step 120/157 - loss: 1.9163 - acc_top1: 0.6254 - acc_top5: 0.9530 - 18ms/step\n",
      "step 130/157 - loss: 2.4236 - acc_top1: 0.6256 - acc_top5: 0.9535 - 18ms/step\n",
      "step 140/157 - loss: 2.0740 - acc_top1: 0.6263 - acc_top5: 0.9539 - 18ms/step\n",
      "step 150/157 - loss: 1.5332 - acc_top1: 0.6255 - acc_top5: 0.9549 - 18ms/step\n",
      "step 157/157 - loss: 0.8658 - acc_top1: 0.6231 - acc_top5: 0.9542 - 18ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.1460 - acc_top1: 0.9422 - acc_top5: 1.0000 - 50ms/step\n",
      "step  20/782 - loss: 0.1767 - acc_top1: 0.9320 - acc_top5: 0.9992 - 50ms/step\n",
      "step  30/782 - loss: 0.0404 - acc_top1: 0.9406 - acc_top5: 0.9990 - 50ms/step\n",
      "step  40/782 - loss: 0.2743 - acc_top1: 0.9379 - acc_top5: 0.9992 - 50ms/step\n",
      "step  50/782 - loss: 0.0567 - acc_top1: 0.9419 - acc_top5: 0.9991 - 50ms/step\n",
      "step  60/782 - loss: 0.1179 - acc_top1: 0.9419 - acc_top5: 0.9984 - 50ms/step\n",
      "step  70/782 - loss: 0.1429 - acc_top1: 0.9429 - acc_top5: 0.9984 - 50ms/step\n",
      "step  80/782 - loss: 0.2730 - acc_top1: 0.9428 - acc_top5: 0.9984 - 50ms/step\n",
      "step  90/782 - loss: 0.2512 - acc_top1: 0.9437 - acc_top5: 0.9984 - 50ms/step\n",
      "step 100/782 - loss: 0.1198 - acc_top1: 0.9452 - acc_top5: 0.9984 - 50ms/step\n",
      "step 110/782 - loss: 0.0968 - acc_top1: 0.9469 - acc_top5: 0.9986 - 50ms/step\n",
      "step 120/782 - loss: 0.1637 - acc_top1: 0.9471 - acc_top5: 0.9984 - 50ms/step\n",
      "step 130/782 - loss: 0.0651 - acc_top1: 0.9482 - acc_top5: 0.9986 - 50ms/step\n",
      "step 140/782 - loss: 0.2987 - acc_top1: 0.9474 - acc_top5: 0.9987 - 50ms/step\n",
      "step 150/782 - loss: 0.1484 - acc_top1: 0.9484 - acc_top5: 0.9985 - 50ms/step\n",
      "step 160/782 - loss: 0.1493 - acc_top1: 0.9490 - acc_top5: 0.9985 - 50ms/step\n",
      "step 170/782 - loss: 0.1192 - acc_top1: 0.9494 - acc_top5: 0.9986 - 50ms/step\n",
      "step 180/782 - loss: 0.1981 - acc_top1: 0.9494 - acc_top5: 0.9986 - 50ms/step\n",
      "step 190/782 - loss: 0.0339 - acc_top1: 0.9501 - acc_top5: 0.9987 - 50ms/step\n",
      "step 200/782 - loss: 0.1259 - acc_top1: 0.9502 - acc_top5: 0.9985 - 50ms/step\n",
      "step 210/782 - loss: 0.1481 - acc_top1: 0.9501 - acc_top5: 0.9986 - 50ms/step\n",
      "step 220/782 - loss: 0.0561 - acc_top1: 0.9508 - acc_top5: 0.9987 - 50ms/step\n",
      "step 230/782 - loss: 0.0747 - acc_top1: 0.9511 - acc_top5: 0.9986 - 50ms/step\n",
      "step 240/782 - loss: 0.1438 - acc_top1: 0.9511 - acc_top5: 0.9987 - 50ms/step\n",
      "step 250/782 - loss: 0.0923 - acc_top1: 0.9515 - acc_top5: 0.9986 - 50ms/step\n",
      "step 260/782 - loss: 0.1873 - acc_top1: 0.9516 - acc_top5: 0.9987 - 50ms/step\n",
      "step 270/782 - loss: 0.1350 - acc_top1: 0.9519 - acc_top5: 0.9987 - 50ms/step\n",
      "step 280/782 - loss: 0.0832 - acc_top1: 0.9525 - acc_top5: 0.9988 - 50ms/step\n",
      "step 290/782 - loss: 0.1650 - acc_top1: 0.9522 - acc_top5: 0.9988 - 50ms/step\n",
      "step 300/782 - loss: 0.0462 - acc_top1: 0.9523 - acc_top5: 0.9988 - 50ms/step\n",
      "step 310/782 - loss: 0.1330 - acc_top1: 0.9524 - acc_top5: 0.9987 - 50ms/step\n",
      "step 320/782 - loss: 0.0683 - acc_top1: 0.9527 - acc_top5: 0.9987 - 50ms/step\n",
      "step 330/782 - loss: 0.0970 - acc_top1: 0.9530 - acc_top5: 0.9987 - 50ms/step\n",
      "step 340/782 - loss: 0.0340 - acc_top1: 0.9529 - acc_top5: 0.9988 - 50ms/step\n",
      "step 350/782 - loss: 0.1308 - acc_top1: 0.9533 - acc_top5: 0.9988 - 50ms/step\n",
      "step 360/782 - loss: 0.0439 - acc_top1: 0.9533 - acc_top5: 0.9988 - 50ms/step\n",
      "step 370/782 - loss: 0.1325 - acc_top1: 0.9531 - acc_top5: 0.9988 - 50ms/step\n",
      "step 380/782 - loss: 0.2304 - acc_top1: 0.9530 - acc_top5: 0.9988 - 50ms/step\n",
      "step 390/782 - loss: 0.0322 - acc_top1: 0.9530 - acc_top5: 0.9988 - 50ms/step\n",
      "step 400/782 - loss: 0.1188 - acc_top1: 0.9531 - acc_top5: 0.9988 - 50ms/step\n",
      "step 410/782 - loss: 0.0665 - acc_top1: 0.9531 - acc_top5: 0.9989 - 50ms/step\n",
      "step 420/782 - loss: 0.0766 - acc_top1: 0.9529 - acc_top5: 0.9988 - 50ms/step\n",
      "step 430/782 - loss: 0.1250 - acc_top1: 0.9531 - acc_top5: 0.9989 - 50ms/step\n",
      "step 440/782 - loss: 0.2036 - acc_top1: 0.9530 - acc_top5: 0.9989 - 50ms/step\n",
      "step 450/782 - loss: 0.0785 - acc_top1: 0.9533 - acc_top5: 0.9989 - 50ms/step\n",
      "step 460/782 - loss: 0.1548 - acc_top1: 0.9536 - acc_top5: 0.9989 - 50ms/step\n",
      "step 470/782 - loss: 0.1977 - acc_top1: 0.9536 - acc_top5: 0.9989 - 50ms/step\n",
      "step 480/782 - loss: 0.1602 - acc_top1: 0.9535 - acc_top5: 0.9989 - 50ms/step\n",
      "step 490/782 - loss: 0.1501 - acc_top1: 0.9529 - acc_top5: 0.9989 - 50ms/step\n",
      "step 500/782 - loss: 0.1059 - acc_top1: 0.9530 - acc_top5: 0.9990 - 50ms/step\n",
      "step 510/782 - loss: 0.1297 - acc_top1: 0.9528 - acc_top5: 0.9990 - 50ms/step\n",
      "step 520/782 - loss: 0.0256 - acc_top1: 0.9527 - acc_top5: 0.9990 - 50ms/step\n",
      "step 530/782 - loss: 0.0287 - acc_top1: 0.9527 - acc_top5: 0.9990 - 50ms/step\n",
      "step 540/782 - loss: 0.2669 - acc_top1: 0.9522 - acc_top5: 0.9990 - 50ms/step\n",
      "step 550/782 - loss: 0.2050 - acc_top1: 0.9520 - acc_top5: 0.9990 - 50ms/step\n",
      "step 560/782 - loss: 0.1788 - acc_top1: 0.9521 - acc_top5: 0.9990 - 50ms/step\n",
      "step 570/782 - loss: 0.0632 - acc_top1: 0.9518 - acc_top5: 0.9990 - 50ms/step\n",
      "step 580/782 - loss: 0.3145 - acc_top1: 0.9517 - acc_top5: 0.9990 - 50ms/step\n",
      "step 590/782 - loss: 0.0429 - acc_top1: 0.9515 - acc_top5: 0.9990 - 50ms/step\n",
      "step 600/782 - loss: 0.2026 - acc_top1: 0.9514 - acc_top5: 0.9990 - 50ms/step\n",
      "step 610/782 - loss: 0.0958 - acc_top1: 0.9514 - acc_top5: 0.9990 - 50ms/step\n",
      "step 620/782 - loss: 0.0971 - acc_top1: 0.9514 - acc_top5: 0.9990 - 50ms/step\n",
      "step 630/782 - loss: 0.0637 - acc_top1: 0.9515 - acc_top5: 0.9990 - 50ms/step\n",
      "step 640/782 - loss: 0.1175 - acc_top1: 0.9517 - acc_top5: 0.9990 - 50ms/step\n",
      "step 650/782 - loss: 0.0846 - acc_top1: 0.9519 - acc_top5: 0.9990 - 50ms/step\n",
      "step 660/782 - loss: 0.0975 - acc_top1: 0.9517 - acc_top5: 0.9990 - 50ms/step\n",
      "step 670/782 - loss: 0.1492 - acc_top1: 0.9519 - acc_top5: 0.9990 - 50ms/step\n",
      "step 680/782 - loss: 0.0961 - acc_top1: 0.9517 - acc_top5: 0.9990 - 50ms/step\n",
      "step 690/782 - loss: 0.1918 - acc_top1: 0.9514 - acc_top5: 0.9990 - 50ms/step\n",
      "step 700/782 - loss: 0.0597 - acc_top1: 0.9513 - acc_top5: 0.9990 - 50ms/step\n",
      "step 710/782 - loss: 0.3202 - acc_top1: 0.9513 - acc_top5: 0.9990 - 50ms/step\n",
      "step 720/782 - loss: 0.1037 - acc_top1: 0.9514 - acc_top5: 0.9990 - 50ms/step\n",
      "step 730/782 - loss: 0.2068 - acc_top1: 0.9512 - acc_top5: 0.9990 - 50ms/step\n",
      "step 740/782 - loss: 0.0515 - acc_top1: 0.9510 - acc_top5: 0.9990 - 50ms/step\n",
      "step 750/782 - loss: 0.1548 - acc_top1: 0.9510 - acc_top5: 0.9990 - 50ms/step\n",
      "step 760/782 - loss: 0.1604 - acc_top1: 0.9511 - acc_top5: 0.9990 - 50ms/step\n",
      "step 770/782 - loss: 0.1021 - acc_top1: 0.9512 - acc_top5: 0.9990 - 50ms/step\n",
      "step 780/782 - loss: 0.0324 - acc_top1: 0.9514 - acc_top5: 0.9990 - 50ms/step\n",
      "step 782/782 - loss: 0.1693 - acc_top1: 0.9514 - acc_top5: 0.9990 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\40\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.3408 - acc_top1: 0.6422 - acc_top5: 0.9750 - 19ms/step\n",
      "step  20/157 - loss: 2.4455 - acc_top1: 0.6516 - acc_top5: 0.9648 - 19ms/step\n",
      "step  30/157 - loss: 1.6642 - acc_top1: 0.6526 - acc_top5: 0.9604 - 19ms/step\n",
      "step  40/157 - loss: 2.5220 - acc_top1: 0.6453 - acc_top5: 0.9609 - 19ms/step\n",
      "step  50/157 - loss: 1.7239 - acc_top1: 0.6478 - acc_top5: 0.9578 - 18ms/step\n",
      "step  60/157 - loss: 1.5099 - acc_top1: 0.6474 - acc_top5: 0.9549 - 18ms/step\n",
      "step  70/157 - loss: 3.3708 - acc_top1: 0.6467 - acc_top5: 0.9536 - 18ms/step\n",
      "step  80/157 - loss: 1.8982 - acc_top1: 0.6432 - acc_top5: 0.9543 - 18ms/step\n",
      "step  90/157 - loss: 1.5377 - acc_top1: 0.6434 - acc_top5: 0.9535 - 18ms/step\n",
      "step 100/157 - loss: 2.3454 - acc_top1: 0.6414 - acc_top5: 0.9536 - 18ms/step\n",
      "step 110/157 - loss: 2.5703 - acc_top1: 0.6378 - acc_top5: 0.9518 - 18ms/step\n",
      "step 120/157 - loss: 2.3354 - acc_top1: 0.6370 - acc_top5: 0.9508 - 18ms/step\n",
      "step 130/157 - loss: 2.4220 - acc_top1: 0.6380 - acc_top5: 0.9517 - 18ms/step\n",
      "step 140/157 - loss: 2.7435 - acc_top1: 0.6355 - acc_top5: 0.9527 - 18ms/step\n",
      "step 150/157 - loss: 4.4097 - acc_top1: 0.6343 - acc_top5: 0.9524 - 18ms/step\n",
      "step 157/157 - loss: 1.4796 - acc_top1: 0.6320 - acc_top5: 0.9521 - 18ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.1959 - acc_top1: 0.9594 - acc_top5: 0.9984 - 50ms/step\n",
      "step  20/782 - loss: 0.0284 - acc_top1: 0.9641 - acc_top5: 0.9992 - 50ms/step\n",
      "step  30/782 - loss: 0.0464 - acc_top1: 0.9609 - acc_top5: 0.9990 - 49ms/step\n",
      "step  40/782 - loss: 0.0792 - acc_top1: 0.9605 - acc_top5: 0.9992 - 49ms/step\n",
      "step  50/782 - loss: 0.1861 - acc_top1: 0.9606 - acc_top5: 0.9994 - 49ms/step\n",
      "step  60/782 - loss: 0.1823 - acc_top1: 0.9599 - acc_top5: 0.9990 - 49ms/step\n",
      "step  70/782 - loss: 0.0290 - acc_top1: 0.9598 - acc_top5: 0.9989 - 50ms/step\n",
      "step  80/782 - loss: 0.0876 - acc_top1: 0.9594 - acc_top5: 0.9988 - 49ms/step\n",
      "step  90/782 - loss: 0.0983 - acc_top1: 0.9599 - acc_top5: 0.9988 - 49ms/step\n",
      "step 100/782 - loss: 0.0466 - acc_top1: 0.9606 - acc_top5: 0.9988 - 49ms/step\n",
      "step 110/782 - loss: 0.1593 - acc_top1: 0.9595 - acc_top5: 0.9987 - 49ms/step\n",
      "step 120/782 - loss: 0.0511 - acc_top1: 0.9600 - acc_top5: 0.9987 - 49ms/step\n",
      "step 130/782 - loss: 0.1412 - acc_top1: 0.9596 - acc_top5: 0.9988 - 49ms/step\n",
      "step 140/782 - loss: 0.1505 - acc_top1: 0.9596 - acc_top5: 0.9988 - 49ms/step\n",
      "step 150/782 - loss: 0.0576 - acc_top1: 0.9602 - acc_top5: 0.9989 - 50ms/step\n",
      "step 160/782 - loss: 0.0866 - acc_top1: 0.9597 - acc_top5: 0.9989 - 50ms/step\n",
      "step 170/782 - loss: 0.0692 - acc_top1: 0.9599 - acc_top5: 0.9990 - 50ms/step\n",
      "step 180/782 - loss: 0.0641 - acc_top1: 0.9605 - acc_top5: 0.9990 - 50ms/step\n",
      "step 190/782 - loss: 0.0946 - acc_top1: 0.9608 - acc_top5: 0.9990 - 50ms/step\n",
      "step 200/782 - loss: 0.0900 - acc_top1: 0.9609 - acc_top5: 0.9990 - 50ms/step\n",
      "step 210/782 - loss: 0.1068 - acc_top1: 0.9616 - acc_top5: 0.9990 - 50ms/step\n",
      "step 220/782 - loss: 0.1058 - acc_top1: 0.9616 - acc_top5: 0.9990 - 50ms/step\n",
      "step 230/782 - loss: 0.0587 - acc_top1: 0.9615 - acc_top5: 0.9990 - 50ms/step\n",
      "step 240/782 - loss: 0.1246 - acc_top1: 0.9619 - acc_top5: 0.9990 - 50ms/step\n",
      "step 250/782 - loss: 0.1067 - acc_top1: 0.9618 - acc_top5: 0.9989 - 50ms/step\n",
      "step 260/782 - loss: 0.0428 - acc_top1: 0.9621 - acc_top5: 0.9989 - 50ms/step\n",
      "step 270/782 - loss: 0.0326 - acc_top1: 0.9620 - acc_top5: 0.9988 - 50ms/step\n",
      "step 280/782 - loss: 0.1240 - acc_top1: 0.9623 - acc_top5: 0.9989 - 50ms/step\n",
      "step 290/782 - loss: 0.0730 - acc_top1: 0.9623 - acc_top5: 0.9988 - 50ms/step\n",
      "step 300/782 - loss: 0.0373 - acc_top1: 0.9623 - acc_top5: 0.9988 - 50ms/step\n",
      "step 310/782 - loss: 0.0361 - acc_top1: 0.9625 - acc_top5: 0.9987 - 50ms/step\n",
      "step 320/782 - loss: 0.0590 - acc_top1: 0.9624 - acc_top5: 0.9987 - 50ms/step\n",
      "step 330/782 - loss: 0.1596 - acc_top1: 0.9621 - acc_top5: 0.9987 - 50ms/step\n",
      "step 340/782 - loss: 0.1691 - acc_top1: 0.9622 - acc_top5: 0.9986 - 50ms/step\n",
      "step 350/782 - loss: 0.1519 - acc_top1: 0.9619 - acc_top5: 0.9986 - 50ms/step\n",
      "step 360/782 - loss: 0.1987 - acc_top1: 0.9618 - acc_top5: 0.9986 - 50ms/step\n",
      "step 370/782 - loss: 0.0780 - acc_top1: 0.9621 - acc_top5: 0.9986 - 50ms/step\n",
      "step 380/782 - loss: 0.1511 - acc_top1: 0.9620 - acc_top5: 0.9985 - 50ms/step\n",
      "step 390/782 - loss: 0.0706 - acc_top1: 0.9617 - acc_top5: 0.9984 - 50ms/step\n",
      "step 400/782 - loss: 0.1438 - acc_top1: 0.9619 - acc_top5: 0.9984 - 50ms/step\n",
      "step 410/782 - loss: 0.1054 - acc_top1: 0.9619 - acc_top5: 0.9984 - 50ms/step\n",
      "step 420/782 - loss: 0.0907 - acc_top1: 0.9619 - acc_top5: 0.9984 - 50ms/step\n",
      "step 430/782 - loss: 0.1317 - acc_top1: 0.9617 - acc_top5: 0.9984 - 50ms/step\n",
      "step 440/782 - loss: 0.1008 - acc_top1: 0.9617 - acc_top5: 0.9984 - 50ms/step\n",
      "step 450/782 - loss: 0.1050 - acc_top1: 0.9616 - acc_top5: 0.9984 - 50ms/step\n",
      "step 460/782 - loss: 0.1004 - acc_top1: 0.9614 - acc_top5: 0.9984 - 50ms/step\n",
      "step 470/782 - loss: 0.0654 - acc_top1: 0.9614 - acc_top5: 0.9984 - 50ms/step\n",
      "step 480/782 - loss: 0.1398 - acc_top1: 0.9613 - acc_top5: 0.9984 - 50ms/step\n",
      "step 490/782 - loss: 0.1049 - acc_top1: 0.9616 - acc_top5: 0.9984 - 50ms/step\n",
      "step 500/782 - loss: 0.0957 - acc_top1: 0.9616 - acc_top5: 0.9984 - 50ms/step\n",
      "step 510/782 - loss: 0.1187 - acc_top1: 0.9617 - acc_top5: 0.9984 - 50ms/step\n",
      "step 520/782 - loss: 0.1172 - acc_top1: 0.9615 - acc_top5: 0.9984 - 50ms/step\n",
      "step 530/782 - loss: 0.1624 - acc_top1: 0.9614 - acc_top5: 0.9984 - 50ms/step\n",
      "step 540/782 - loss: 0.1035 - acc_top1: 0.9612 - acc_top5: 0.9985 - 50ms/step\n",
      "step 550/782 - loss: 0.0873 - acc_top1: 0.9609 - acc_top5: 0.9985 - 50ms/step\n",
      "step 560/782 - loss: 0.1951 - acc_top1: 0.9607 - acc_top5: 0.9985 - 50ms/step\n",
      "step 570/782 - loss: 0.1249 - acc_top1: 0.9603 - acc_top5: 0.9985 - 50ms/step\n",
      "step 580/782 - loss: 0.1400 - acc_top1: 0.9602 - acc_top5: 0.9985 - 50ms/step\n",
      "step 590/782 - loss: 0.2769 - acc_top1: 0.9597 - acc_top5: 0.9985 - 50ms/step\n",
      "step 600/782 - loss: 0.1627 - acc_top1: 0.9596 - acc_top5: 0.9985 - 50ms/step\n",
      "step 610/782 - loss: 0.1346 - acc_top1: 0.9593 - acc_top5: 0.9985 - 50ms/step\n",
      "step 620/782 - loss: 0.1006 - acc_top1: 0.9591 - acc_top5: 0.9985 - 50ms/step\n",
      "step 630/782 - loss: 0.2229 - acc_top1: 0.9589 - acc_top5: 0.9985 - 50ms/step\n",
      "step 640/782 - loss: 0.1485 - acc_top1: 0.9586 - acc_top5: 0.9985 - 50ms/step\n",
      "step 650/782 - loss: 0.3150 - acc_top1: 0.9582 - acc_top5: 0.9985 - 50ms/step\n",
      "step 660/782 - loss: 0.0573 - acc_top1: 0.9583 - acc_top5: 0.9986 - 50ms/step\n",
      "step 670/782 - loss: 0.0977 - acc_top1: 0.9580 - acc_top5: 0.9985 - 50ms/step\n",
      "step 680/782 - loss: 0.1424 - acc_top1: 0.9579 - acc_top5: 0.9985 - 50ms/step\n",
      "step 690/782 - loss: 0.1159 - acc_top1: 0.9577 - acc_top5: 0.9986 - 50ms/step\n",
      "step 700/782 - loss: 0.0940 - acc_top1: 0.9577 - acc_top5: 0.9986 - 50ms/step\n",
      "step 710/782 - loss: 0.2392 - acc_top1: 0.9577 - acc_top5: 0.9986 - 50ms/step\n",
      "step 720/782 - loss: 0.1405 - acc_top1: 0.9575 - acc_top5: 0.9986 - 50ms/step\n",
      "step 730/782 - loss: 0.0915 - acc_top1: 0.9574 - acc_top5: 0.9985 - 50ms/step\n",
      "step 740/782 - loss: 0.1250 - acc_top1: 0.9573 - acc_top5: 0.9985 - 50ms/step\n",
      "step 750/782 - loss: 0.1328 - acc_top1: 0.9571 - acc_top5: 0.9985 - 50ms/step\n",
      "step 760/782 - loss: 0.0782 - acc_top1: 0.9569 - acc_top5: 0.9985 - 50ms/step\n",
      "step 770/782 - loss: 0.1514 - acc_top1: 0.9569 - acc_top5: 0.9985 - 50ms/step\n",
      "step 780/782 - loss: 0.1331 - acc_top1: 0.9567 - acc_top5: 0.9985 - 50ms/step\n",
      "step 782/782 - loss: 0.8805 - acc_top1: 0.9567 - acc_top5: 0.9985 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\41\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.2911 - acc_top1: 0.6234 - acc_top5: 0.9734 - 19ms/step\n",
      "step  20/157 - loss: 3.1412 - acc_top1: 0.6367 - acc_top5: 0.9703 - 18ms/step\n",
      "step  30/157 - loss: 1.8827 - acc_top1: 0.6385 - acc_top5: 0.9651 - 18ms/step\n",
      "step  40/157 - loss: 2.9790 - acc_top1: 0.6375 - acc_top5: 0.9613 - 18ms/step\n",
      "step  50/157 - loss: 1.8461 - acc_top1: 0.6391 - acc_top5: 0.9606 - 18ms/step\n",
      "step  60/157 - loss: 1.7610 - acc_top1: 0.6383 - acc_top5: 0.9568 - 18ms/step\n",
      "step  70/157 - loss: 4.5864 - acc_top1: 0.6371 - acc_top5: 0.9556 - 18ms/step\n",
      "step  80/157 - loss: 2.8307 - acc_top1: 0.6330 - acc_top5: 0.9551 - 18ms/step\n",
      "step  90/157 - loss: 1.3276 - acc_top1: 0.6323 - acc_top5: 0.9556 - 18ms/step\n",
      "step 100/157 - loss: 2.3793 - acc_top1: 0.6298 - acc_top5: 0.9552 - 18ms/step\n",
      "step 110/157 - loss: 3.2274 - acc_top1: 0.6251 - acc_top5: 0.9541 - 18ms/step\n",
      "step 120/157 - loss: 2.1433 - acc_top1: 0.6247 - acc_top5: 0.9539 - 18ms/step\n",
      "step 130/157 - loss: 2.2674 - acc_top1: 0.6244 - acc_top5: 0.9550 - 18ms/step\n",
      "step 140/157 - loss: 4.3600 - acc_top1: 0.6240 - acc_top5: 0.9552 - 18ms/step\n",
      "step 150/157 - loss: 11.2811 - acc_top1: 0.6230 - acc_top5: 0.9552 - 18ms/step\n",
      "step 157/157 - loss: 2.0013 - acc_top1: 0.6217 - acc_top5: 0.9553 - 18ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.2265 - acc_top1: 0.9359 - acc_top5: 1.0000 - 50ms/step\n",
      "step  20/782 - loss: 0.2154 - acc_top1: 0.9242 - acc_top5: 0.9984 - 50ms/step\n",
      "step  30/782 - loss: 0.2039 - acc_top1: 0.9187 - acc_top5: 0.9979 - 50ms/step\n",
      "step  40/782 - loss: 0.1030 - acc_top1: 0.9262 - acc_top5: 0.9980 - 50ms/step\n",
      "step  50/782 - loss: 0.0737 - acc_top1: 0.9297 - acc_top5: 0.9981 - 50ms/step\n",
      "step  60/782 - loss: 0.1246 - acc_top1: 0.9299 - acc_top5: 0.9984 - 50ms/step\n",
      "step  70/782 - loss: 0.1262 - acc_top1: 0.9310 - acc_top5: 0.9984 - 50ms/step\n",
      "step  80/782 - loss: 0.1518 - acc_top1: 0.9340 - acc_top5: 0.9980 - 50ms/step\n",
      "step  90/782 - loss: 0.0966 - acc_top1: 0.9361 - acc_top5: 0.9983 - 50ms/step\n",
      "step 100/782 - loss: 0.2808 - acc_top1: 0.9378 - acc_top5: 0.9984 - 50ms/step\n",
      "step 110/782 - loss: 0.1929 - acc_top1: 0.9391 - acc_top5: 0.9986 - 50ms/step\n",
      "step 120/782 - loss: 0.0464 - acc_top1: 0.9402 - acc_top5: 0.9984 - 50ms/step\n",
      "step 130/782 - loss: 0.1292 - acc_top1: 0.9412 - acc_top5: 0.9983 - 50ms/step\n",
      "step 140/782 - loss: 0.0740 - acc_top1: 0.9426 - acc_top5: 0.9983 - 50ms/step\n",
      "step 150/782 - loss: 0.0749 - acc_top1: 0.9433 - acc_top5: 0.9982 - 50ms/step\n",
      "step 160/782 - loss: 0.1054 - acc_top1: 0.9441 - acc_top5: 0.9981 - 50ms/step\n",
      "step 170/782 - loss: 0.1176 - acc_top1: 0.9450 - acc_top5: 0.9980 - 50ms/step\n",
      "step 180/782 - loss: 0.1624 - acc_top1: 0.9456 - acc_top5: 0.9980 - 50ms/step\n",
      "step 190/782 - loss: 0.0360 - acc_top1: 0.9470 - acc_top5: 0.9980 - 50ms/step\n",
      "step 200/782 - loss: 0.1853 - acc_top1: 0.9473 - acc_top5: 0.9980 - 50ms/step\n",
      "step 210/782 - loss: 0.1576 - acc_top1: 0.9485 - acc_top5: 0.9981 - 50ms/step\n",
      "step 220/782 - loss: 0.2202 - acc_top1: 0.9486 - acc_top5: 0.9980 - 50ms/step\n",
      "step 230/782 - loss: 0.1505 - acc_top1: 0.9492 - acc_top5: 0.9981 - 50ms/step\n",
      "step 240/782 - loss: 0.2869 - acc_top1: 0.9490 - acc_top5: 0.9980 - 50ms/step\n",
      "step 250/782 - loss: 0.0988 - acc_top1: 0.9495 - acc_top5: 0.9979 - 50ms/step\n",
      "step 260/782 - loss: 0.1226 - acc_top1: 0.9500 - acc_top5: 0.9980 - 50ms/step\n",
      "step 270/782 - loss: 0.1376 - acc_top1: 0.9500 - acc_top5: 0.9979 - 50ms/step\n",
      "step 280/782 - loss: 0.3289 - acc_top1: 0.9496 - acc_top5: 0.9979 - 50ms/step\n",
      "step 290/782 - loss: 0.1383 - acc_top1: 0.9501 - acc_top5: 0.9980 - 50ms/step\n",
      "step 300/782 - loss: 0.0505 - acc_top1: 0.9509 - acc_top5: 0.9980 - 50ms/step\n",
      "step 310/782 - loss: 0.2971 - acc_top1: 0.9512 - acc_top5: 0.9980 - 50ms/step\n",
      "step 320/782 - loss: 0.1135 - acc_top1: 0.9516 - acc_top5: 0.9981 - 50ms/step\n",
      "step 330/782 - loss: 0.0848 - acc_top1: 0.9520 - acc_top5: 0.9982 - 50ms/step\n",
      "step 340/782 - loss: 0.1817 - acc_top1: 0.9523 - acc_top5: 0.9982 - 50ms/step\n",
      "step 350/782 - loss: 0.1665 - acc_top1: 0.9524 - acc_top5: 0.9982 - 50ms/step\n",
      "step 360/782 - loss: 0.0743 - acc_top1: 0.9528 - acc_top5: 0.9982 - 50ms/step\n",
      "step 370/782 - loss: 0.0950 - acc_top1: 0.9527 - acc_top5: 0.9982 - 50ms/step\n",
      "step 380/782 - loss: 0.1484 - acc_top1: 0.9528 - acc_top5: 0.9982 - 50ms/step\n",
      "step 390/782 - loss: 0.1690 - acc_top1: 0.9528 - acc_top5: 0.9982 - 50ms/step\n",
      "step 400/782 - loss: 0.3066 - acc_top1: 0.9526 - acc_top5: 0.9982 - 50ms/step\n",
      "step 410/782 - loss: 0.0804 - acc_top1: 0.9530 - acc_top5: 0.9982 - 50ms/step\n",
      "step 420/782 - loss: 0.1025 - acc_top1: 0.9530 - acc_top5: 0.9983 - 50ms/step\n",
      "step 430/782 - loss: 0.1110 - acc_top1: 0.9530 - acc_top5: 0.9983 - 50ms/step\n",
      "step 440/782 - loss: 0.1111 - acc_top1: 0.9531 - acc_top5: 0.9983 - 50ms/step\n",
      "step 450/782 - loss: 0.1498 - acc_top1: 0.9531 - acc_top5: 0.9983 - 50ms/step\n",
      "step 460/782 - loss: 0.2045 - acc_top1: 0.9532 - acc_top5: 0.9983 - 50ms/step\n",
      "step 470/782 - loss: 0.0956 - acc_top1: 0.9536 - acc_top5: 0.9983 - 50ms/step\n",
      "step 480/782 - loss: 0.1311 - acc_top1: 0.9537 - acc_top5: 0.9984 - 50ms/step\n",
      "step 490/782 - loss: 0.1917 - acc_top1: 0.9537 - acc_top5: 0.9984 - 50ms/step\n",
      "step 500/782 - loss: 0.0944 - acc_top1: 0.9535 - acc_top5: 0.9984 - 50ms/step\n",
      "step 510/782 - loss: 0.1863 - acc_top1: 0.9532 - acc_top5: 0.9984 - 50ms/step\n",
      "step 520/782 - loss: 0.0896 - acc_top1: 0.9533 - acc_top5: 0.9984 - 50ms/step\n",
      "step 530/782 - loss: 0.1000 - acc_top1: 0.9534 - acc_top5: 0.9984 - 50ms/step\n",
      "step 540/782 - loss: 0.1089 - acc_top1: 0.9534 - acc_top5: 0.9984 - 50ms/step\n",
      "step 550/782 - loss: 0.2700 - acc_top1: 0.9536 - acc_top5: 0.9984 - 50ms/step\n",
      "step 560/782 - loss: 0.0408 - acc_top1: 0.9538 - acc_top5: 0.9984 - 50ms/step\n",
      "step 570/782 - loss: 0.1912 - acc_top1: 0.9537 - acc_top5: 0.9984 - 50ms/step\n",
      "step 580/782 - loss: 0.0259 - acc_top1: 0.9540 - acc_top5: 0.9984 - 50ms/step\n",
      "step 590/782 - loss: 0.1781 - acc_top1: 0.9538 - acc_top5: 0.9984 - 50ms/step\n",
      "step 600/782 - loss: 0.0330 - acc_top1: 0.9541 - acc_top5: 0.9984 - 50ms/step\n",
      "step 610/782 - loss: 0.3269 - acc_top1: 0.9540 - acc_top5: 0.9984 - 50ms/step\n",
      "step 620/782 - loss: 0.0253 - acc_top1: 0.9542 - acc_top5: 0.9984 - 50ms/step\n",
      "step 630/782 - loss: 0.0927 - acc_top1: 0.9543 - acc_top5: 0.9985 - 50ms/step\n",
      "step 640/782 - loss: 0.0837 - acc_top1: 0.9542 - acc_top5: 0.9985 - 50ms/step\n",
      "step 650/782 - loss: 0.1762 - acc_top1: 0.9543 - acc_top5: 0.9985 - 50ms/step\n",
      "step 660/782 - loss: 0.1000 - acc_top1: 0.9542 - acc_top5: 0.9985 - 50ms/step\n",
      "step 670/782 - loss: 0.0844 - acc_top1: 0.9542 - acc_top5: 0.9985 - 50ms/step\n",
      "step 680/782 - loss: 0.0984 - acc_top1: 0.9541 - acc_top5: 0.9985 - 50ms/step\n",
      "step 690/782 - loss: 0.1011 - acc_top1: 0.9541 - acc_top5: 0.9985 - 50ms/step\n",
      "step 700/782 - loss: 0.0840 - acc_top1: 0.9540 - acc_top5: 0.9985 - 50ms/step\n",
      "step 710/782 - loss: 0.1429 - acc_top1: 0.9540 - acc_top5: 0.9985 - 50ms/step\n",
      "step 720/782 - loss: 0.1308 - acc_top1: 0.9540 - acc_top5: 0.9985 - 50ms/step\n",
      "step 730/782 - loss: 0.0570 - acc_top1: 0.9540 - acc_top5: 0.9985 - 50ms/step\n",
      "step 740/782 - loss: 0.1323 - acc_top1: 0.9541 - acc_top5: 0.9985 - 50ms/step\n",
      "step 750/782 - loss: 0.2284 - acc_top1: 0.9541 - acc_top5: 0.9985 - 50ms/step\n",
      "step 760/782 - loss: 0.1147 - acc_top1: 0.9539 - acc_top5: 0.9985 - 50ms/step\n",
      "step 770/782 - loss: 0.1013 - acc_top1: 0.9538 - acc_top5: 0.9985 - 50ms/step\n",
      "step 780/782 - loss: 0.1446 - acc_top1: 0.9538 - acc_top5: 0.9985 - 50ms/step\n",
      "step 782/782 - loss: 0.2153 - acc_top1: 0.9538 - acc_top5: 0.9985 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\42\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.4658 - acc_top1: 0.6141 - acc_top5: 0.9641 - 19ms/step\n",
      "step  20/157 - loss: 3.5415 - acc_top1: 0.6203 - acc_top5: 0.9633 - 18ms/step\n",
      "step  30/157 - loss: 2.2428 - acc_top1: 0.6224 - acc_top5: 0.9609 - 18ms/step\n",
      "step  40/157 - loss: 2.9918 - acc_top1: 0.6242 - acc_top5: 0.9578 - 18ms/step\n",
      "step  50/157 - loss: 1.9735 - acc_top1: 0.6253 - acc_top5: 0.9563 - 18ms/step\n",
      "step  60/157 - loss: 1.7048 - acc_top1: 0.6232 - acc_top5: 0.9563 - 18ms/step\n",
      "step  70/157 - loss: 2.7621 - acc_top1: 0.6228 - acc_top5: 0.9558 - 18ms/step\n",
      "step  80/157 - loss: 2.3459 - acc_top1: 0.6238 - acc_top5: 0.9555 - 18ms/step\n",
      "step  90/157 - loss: 1.4962 - acc_top1: 0.6234 - acc_top5: 0.9550 - 18ms/step\n",
      "step 100/157 - loss: 2.4208 - acc_top1: 0.6239 - acc_top5: 0.9553 - 18ms/step\n",
      "step 110/157 - loss: 3.2450 - acc_top1: 0.6199 - acc_top5: 0.9536 - 18ms/step\n",
      "step 120/157 - loss: 2.8953 - acc_top1: 0.6195 - acc_top5: 0.9520 - 18ms/step\n",
      "step 130/157 - loss: 2.3833 - acc_top1: 0.6197 - acc_top5: 0.9528 - 18ms/step\n",
      "step 140/157 - loss: 2.0051 - acc_top1: 0.6185 - acc_top5: 0.9523 - 18ms/step\n",
      "step 150/157 - loss: 3.2981 - acc_top1: 0.6185 - acc_top5: 0.9523 - 18ms/step\n",
      "step 157/157 - loss: 1.5006 - acc_top1: 0.6169 - acc_top5: 0.9523 - 18ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.1540 - acc_top1: 0.9609 - acc_top5: 0.9984 - 49ms/step\n",
      "step  20/782 - loss: 0.1851 - acc_top1: 0.9555 - acc_top5: 0.9977 - 51ms/step\n",
      "step  30/782 - loss: 0.0994 - acc_top1: 0.9557 - acc_top5: 0.9979 - 51ms/step\n",
      "step  40/782 - loss: 0.0791 - acc_top1: 0.9547 - acc_top5: 0.9977 - 50ms/step\n",
      "step  50/782 - loss: 0.1844 - acc_top1: 0.9553 - acc_top5: 0.9978 - 50ms/step\n",
      "step  60/782 - loss: 0.2404 - acc_top1: 0.9544 - acc_top5: 0.9977 - 50ms/step\n",
      "step  70/782 - loss: 0.1037 - acc_top1: 0.9542 - acc_top5: 0.9978 - 50ms/step\n",
      "step  80/782 - loss: 0.1041 - acc_top1: 0.9564 - acc_top5: 0.9980 - 50ms/step\n",
      "step  90/782 - loss: 0.0785 - acc_top1: 0.9575 - acc_top5: 0.9983 - 50ms/step\n",
      "step 100/782 - loss: 0.0682 - acc_top1: 0.9580 - acc_top5: 0.9984 - 50ms/step\n",
      "step 110/782 - loss: 0.1149 - acc_top1: 0.9587 - acc_top5: 0.9986 - 50ms/step\n",
      "step 120/782 - loss: 0.0599 - acc_top1: 0.9592 - acc_top5: 0.9986 - 50ms/step\n",
      "step 130/782 - loss: 0.1314 - acc_top1: 0.9599 - acc_top5: 0.9987 - 50ms/step\n",
      "step 140/782 - loss: 0.0901 - acc_top1: 0.9587 - acc_top5: 0.9987 - 50ms/step\n",
      "step 150/782 - loss: 0.0373 - acc_top1: 0.9580 - acc_top5: 0.9986 - 50ms/step\n",
      "step 160/782 - loss: 0.0865 - acc_top1: 0.9586 - acc_top5: 0.9987 - 50ms/step\n",
      "step 170/782 - loss: 0.0899 - acc_top1: 0.9585 - acc_top5: 0.9988 - 50ms/step\n",
      "step 180/782 - loss: 0.1011 - acc_top1: 0.9584 - acc_top5: 0.9989 - 50ms/step\n",
      "step 190/782 - loss: 0.1385 - acc_top1: 0.9582 - acc_top5: 0.9988 - 50ms/step\n",
      "step 200/782 - loss: 0.0589 - acc_top1: 0.9578 - acc_top5: 0.9989 - 50ms/step\n",
      "step 210/782 - loss: 0.2216 - acc_top1: 0.9574 - acc_top5: 0.9990 - 50ms/step\n",
      "step 220/782 - loss: 0.1040 - acc_top1: 0.9575 - acc_top5: 0.9990 - 50ms/step\n",
      "step 230/782 - loss: 0.0359 - acc_top1: 0.9577 - acc_top5: 0.9990 - 50ms/step\n",
      "step 240/782 - loss: 0.0446 - acc_top1: 0.9579 - acc_top5: 0.9991 - 50ms/step\n",
      "step 250/782 - loss: 0.0817 - acc_top1: 0.9584 - acc_top5: 0.9991 - 50ms/step\n",
      "step 260/782 - loss: 0.2510 - acc_top1: 0.9586 - acc_top5: 0.9992 - 50ms/step\n",
      "step 270/782 - loss: 0.1132 - acc_top1: 0.9586 - acc_top5: 0.9992 - 50ms/step\n",
      "step 280/782 - loss: 0.1014 - acc_top1: 0.9585 - acc_top5: 0.9992 - 50ms/step\n",
      "step 290/782 - loss: 0.1264 - acc_top1: 0.9583 - acc_top5: 0.9992 - 50ms/step\n",
      "step 300/782 - loss: 0.1015 - acc_top1: 0.9585 - acc_top5: 0.9992 - 50ms/step\n",
      "step 310/782 - loss: 0.1081 - acc_top1: 0.9583 - acc_top5: 0.9992 - 50ms/step\n",
      "step 320/782 - loss: 0.0605 - acc_top1: 0.9583 - acc_top5: 0.9992 - 50ms/step\n",
      "step 330/782 - loss: 0.1635 - acc_top1: 0.9582 - acc_top5: 0.9992 - 50ms/step\n",
      "step 340/782 - loss: 0.1903 - acc_top1: 0.9581 - acc_top5: 0.9992 - 50ms/step\n",
      "step 350/782 - loss: 0.1805 - acc_top1: 0.9584 - acc_top5: 0.9992 - 50ms/step\n",
      "step 360/782 - loss: 0.1339 - acc_top1: 0.9585 - acc_top5: 0.9991 - 50ms/step\n",
      "step 370/782 - loss: 0.1890 - acc_top1: 0.9584 - acc_top5: 0.9991 - 50ms/step\n",
      "step 380/782 - loss: 0.1835 - acc_top1: 0.9586 - acc_top5: 0.9991 - 50ms/step\n",
      "step 390/782 - loss: 0.1110 - acc_top1: 0.9590 - acc_top5: 0.9991 - 50ms/step\n",
      "step 400/782 - loss: 0.0693 - acc_top1: 0.9593 - acc_top5: 0.9991 - 50ms/step\n",
      "step 410/782 - loss: 0.0592 - acc_top1: 0.9594 - acc_top5: 0.9991 - 50ms/step\n",
      "step 420/782 - loss: 0.1274 - acc_top1: 0.9595 - acc_top5: 0.9991 - 50ms/step\n",
      "step 430/782 - loss: 0.0767 - acc_top1: 0.9598 - acc_top5: 0.9991 - 50ms/step\n",
      "step 440/782 - loss: 0.1518 - acc_top1: 0.9598 - acc_top5: 0.9991 - 50ms/step\n",
      "step 450/782 - loss: 0.1420 - acc_top1: 0.9598 - acc_top5: 0.9991 - 50ms/step\n",
      "step 460/782 - loss: 0.0409 - acc_top1: 0.9594 - acc_top5: 0.9990 - 50ms/step\n",
      "step 470/782 - loss: 0.0764 - acc_top1: 0.9594 - acc_top5: 0.9991 - 50ms/step\n",
      "step 480/782 - loss: 0.0837 - acc_top1: 0.9594 - acc_top5: 0.9991 - 50ms/step\n",
      "step 490/782 - loss: 0.1023 - acc_top1: 0.9595 - acc_top5: 0.9991 - 50ms/step\n",
      "step 500/782 - loss: 0.1183 - acc_top1: 0.9595 - acc_top5: 0.9990 - 50ms/step\n",
      "step 510/782 - loss: 0.1083 - acc_top1: 0.9594 - acc_top5: 0.9990 - 50ms/step\n",
      "step 520/782 - loss: 0.1950 - acc_top1: 0.9592 - acc_top5: 0.9989 - 50ms/step\n",
      "step 530/782 - loss: 0.0967 - acc_top1: 0.9593 - acc_top5: 0.9989 - 50ms/step\n",
      "step 540/782 - loss: 0.1267 - acc_top1: 0.9593 - acc_top5: 0.9988 - 50ms/step\n",
      "step 550/782 - loss: 0.0689 - acc_top1: 0.9593 - acc_top5: 0.9987 - 50ms/step\n",
      "step 560/782 - loss: 0.0847 - acc_top1: 0.9595 - acc_top5: 0.9987 - 50ms/step\n",
      "step 570/782 - loss: 0.1538 - acc_top1: 0.9597 - acc_top5: 0.9988 - 50ms/step\n",
      "step 580/782 - loss: 0.2353 - acc_top1: 0.9596 - acc_top5: 0.9988 - 50ms/step\n",
      "step 590/782 - loss: 0.1424 - acc_top1: 0.9594 - acc_top5: 0.9988 - 50ms/step\n",
      "step 600/782 - loss: 0.2670 - acc_top1: 0.9595 - acc_top5: 0.9988 - 50ms/step\n",
      "step 610/782 - loss: 0.1914 - acc_top1: 0.9597 - acc_top5: 0.9988 - 50ms/step\n",
      "step 620/782 - loss: 0.0387 - acc_top1: 0.9596 - acc_top5: 0.9988 - 50ms/step\n",
      "step 630/782 - loss: 0.1274 - acc_top1: 0.9596 - acc_top5: 0.9988 - 50ms/step\n",
      "step 640/782 - loss: 0.0358 - acc_top1: 0.9598 - acc_top5: 0.9988 - 50ms/step\n",
      "step 650/782 - loss: 0.0452 - acc_top1: 0.9597 - acc_top5: 0.9988 - 50ms/step\n",
      "step 660/782 - loss: 0.0789 - acc_top1: 0.9599 - acc_top5: 0.9987 - 50ms/step\n",
      "step 670/782 - loss: 0.1454 - acc_top1: 0.9597 - acc_top5: 0.9988 - 50ms/step\n",
      "step 680/782 - loss: 0.1658 - acc_top1: 0.9597 - acc_top5: 0.9988 - 50ms/step\n",
      "step 690/782 - loss: 0.0980 - acc_top1: 0.9597 - acc_top5: 0.9988 - 50ms/step\n",
      "step 700/782 - loss: 0.1023 - acc_top1: 0.9596 - acc_top5: 0.9988 - 50ms/step\n",
      "step 710/782 - loss: 0.2692 - acc_top1: 0.9596 - acc_top5: 0.9988 - 50ms/step\n",
      "step 720/782 - loss: 0.1621 - acc_top1: 0.9594 - acc_top5: 0.9988 - 50ms/step\n",
      "step 730/782 - loss: 0.0712 - acc_top1: 0.9595 - acc_top5: 0.9988 - 50ms/step\n",
      "step 740/782 - loss: 0.1250 - acc_top1: 0.9594 - acc_top5: 0.9988 - 50ms/step\n",
      "step 750/782 - loss: 0.1537 - acc_top1: 0.9591 - acc_top5: 0.9987 - 50ms/step\n",
      "step 760/782 - loss: 0.1374 - acc_top1: 0.9592 - acc_top5: 0.9987 - 50ms/step\n",
      "step 770/782 - loss: 0.1327 - acc_top1: 0.9590 - acc_top5: 0.9987 - 50ms/step\n",
      "step 780/782 - loss: 0.1573 - acc_top1: 0.9589 - acc_top5: 0.9987 - 50ms/step\n",
      "step 782/782 - loss: 0.9458 - acc_top1: 0.9588 - acc_top5: 0.9987 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\43\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.3442 - acc_top1: 0.6328 - acc_top5: 0.9641 - 19ms/step\n",
      "step  20/157 - loss: 2.9209 - acc_top1: 0.6367 - acc_top5: 0.9641 - 19ms/step\n",
      "step  30/157 - loss: 2.4782 - acc_top1: 0.6411 - acc_top5: 0.9531 - 19ms/step\n",
      "step  40/157 - loss: 2.8976 - acc_top1: 0.6375 - acc_top5: 0.9527 - 19ms/step\n",
      "step  50/157 - loss: 1.4125 - acc_top1: 0.6362 - acc_top5: 0.9534 - 18ms/step\n",
      "step  60/157 - loss: 1.9609 - acc_top1: 0.6344 - acc_top5: 0.9500 - 18ms/step\n",
      "step  70/157 - loss: 3.0133 - acc_top1: 0.6337 - acc_top5: 0.9507 - 18ms/step\n",
      "step  80/157 - loss: 1.6929 - acc_top1: 0.6330 - acc_top5: 0.9504 - 18ms/step\n",
      "step  90/157 - loss: 1.4896 - acc_top1: 0.6314 - acc_top5: 0.9505 - 18ms/step\n",
      "step 100/157 - loss: 2.5822 - acc_top1: 0.6291 - acc_top5: 0.9513 - 18ms/step\n",
      "step 110/157 - loss: 2.9133 - acc_top1: 0.6261 - acc_top5: 0.9499 - 18ms/step\n",
      "step 120/157 - loss: 2.7490 - acc_top1: 0.6251 - acc_top5: 0.9488 - 18ms/step\n",
      "step 130/157 - loss: 2.3623 - acc_top1: 0.6255 - acc_top5: 0.9500 - 18ms/step\n",
      "step 140/157 - loss: 2.3975 - acc_top1: 0.6249 - acc_top5: 0.9498 - 18ms/step\n",
      "step 150/157 - loss: 3.7075 - acc_top1: 0.6268 - acc_top5: 0.9499 - 18ms/step\n",
      "step 157/157 - loss: 1.2802 - acc_top1: 0.6247 - acc_top5: 0.9498 - 18ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.2985 - acc_top1: 0.9656 - acc_top5: 0.9984 - 49ms/step\n",
      "step  20/782 - loss: 0.1724 - acc_top1: 0.9492 - acc_top5: 0.9992 - 50ms/step\n",
      "step  30/782 - loss: 0.2149 - acc_top1: 0.9458 - acc_top5: 0.9990 - 50ms/step\n",
      "step  40/782 - loss: 0.2997 - acc_top1: 0.9449 - acc_top5: 0.9992 - 50ms/step\n",
      "step  50/782 - loss: 0.0641 - acc_top1: 0.9487 - acc_top5: 0.9991 - 51ms/step\n",
      "step  60/782 - loss: 0.2436 - acc_top1: 0.9497 - acc_top5: 0.9992 - 51ms/step\n",
      "step  70/782 - loss: 0.1147 - acc_top1: 0.9502 - acc_top5: 0.9993 - 51ms/step\n",
      "step  80/782 - loss: 0.1841 - acc_top1: 0.9525 - acc_top5: 0.9994 - 51ms/step\n",
      "step  90/782 - loss: 0.2217 - acc_top1: 0.9523 - acc_top5: 0.9995 - 51ms/step\n",
      "step 100/782 - loss: 0.0742 - acc_top1: 0.9530 - acc_top5: 0.9995 - 50ms/step\n",
      "step 110/782 - loss: 0.0313 - acc_top1: 0.9528 - acc_top5: 0.9994 - 50ms/step\n",
      "step 120/782 - loss: 0.0954 - acc_top1: 0.9540 - acc_top5: 0.9995 - 50ms/step\n",
      "step 130/782 - loss: 0.0794 - acc_top1: 0.9544 - acc_top5: 0.9994 - 50ms/step\n",
      "step 140/782 - loss: 0.1685 - acc_top1: 0.9549 - acc_top5: 0.9993 - 50ms/step\n",
      "step 150/782 - loss: 0.0483 - acc_top1: 0.9556 - acc_top5: 0.9993 - 50ms/step\n",
      "step 160/782 - loss: 0.0828 - acc_top1: 0.9563 - acc_top5: 0.9992 - 50ms/step\n",
      "step 170/782 - loss: 0.2162 - acc_top1: 0.9558 - acc_top5: 0.9991 - 50ms/step\n",
      "step 180/782 - loss: 0.0903 - acc_top1: 0.9560 - acc_top5: 0.9990 - 50ms/step\n",
      "step 190/782 - loss: 0.0682 - acc_top1: 0.9562 - acc_top5: 0.9989 - 50ms/step\n",
      "step 200/782 - loss: 0.0410 - acc_top1: 0.9559 - acc_top5: 0.9988 - 50ms/step\n",
      "step 210/782 - loss: 0.0901 - acc_top1: 0.9563 - acc_top5: 0.9988 - 50ms/step\n",
      "step 220/782 - loss: 0.0603 - acc_top1: 0.9565 - acc_top5: 0.9988 - 50ms/step\n",
      "step 230/782 - loss: 0.1674 - acc_top1: 0.9567 - acc_top5: 0.9988 - 50ms/step\n",
      "step 240/782 - loss: 0.0692 - acc_top1: 0.9568 - acc_top5: 0.9988 - 50ms/step\n",
      "step 250/782 - loss: 0.1220 - acc_top1: 0.9573 - acc_top5: 0.9988 - 50ms/step\n",
      "step 260/782 - loss: 0.1464 - acc_top1: 0.9575 - acc_top5: 0.9989 - 50ms/step\n",
      "step 270/782 - loss: 0.0147 - acc_top1: 0.9583 - acc_top5: 0.9989 - 50ms/step\n",
      "step 280/782 - loss: 0.0875 - acc_top1: 0.9589 - acc_top5: 0.9989 - 50ms/step\n",
      "step 290/782 - loss: 0.1903 - acc_top1: 0.9592 - acc_top5: 0.9988 - 50ms/step\n",
      "step 300/782 - loss: 0.1102 - acc_top1: 0.9594 - acc_top5: 0.9989 - 50ms/step\n",
      "step 310/782 - loss: 0.0889 - acc_top1: 0.9598 - acc_top5: 0.9989 - 50ms/step\n",
      "step 320/782 - loss: 0.1622 - acc_top1: 0.9595 - acc_top5: 0.9989 - 50ms/step\n",
      "step 330/782 - loss: 0.1574 - acc_top1: 0.9596 - acc_top5: 0.9989 - 50ms/step\n",
      "step 340/782 - loss: 0.1140 - acc_top1: 0.9597 - acc_top5: 0.9989 - 50ms/step\n",
      "step 350/782 - loss: 0.0827 - acc_top1: 0.9598 - acc_top5: 0.9989 - 50ms/step\n",
      "step 360/782 - loss: 0.0708 - acc_top1: 0.9598 - acc_top5: 0.9989 - 50ms/step\n",
      "step 370/782 - loss: 0.1883 - acc_top1: 0.9603 - acc_top5: 0.9989 - 50ms/step\n",
      "step 380/782 - loss: 0.1112 - acc_top1: 0.9603 - acc_top5: 0.9988 - 50ms/step\n",
      "step 390/782 - loss: 0.0698 - acc_top1: 0.9603 - acc_top5: 0.9988 - 50ms/step\n",
      "step 400/782 - loss: 0.0584 - acc_top1: 0.9604 - acc_top5: 0.9988 - 50ms/step\n",
      "step 410/782 - loss: 0.0770 - acc_top1: 0.9606 - acc_top5: 0.9988 - 50ms/step\n",
      "step 420/782 - loss: 0.1245 - acc_top1: 0.9607 - acc_top5: 0.9988 - 50ms/step\n",
      "step 430/782 - loss: 0.1026 - acc_top1: 0.9606 - acc_top5: 0.9988 - 50ms/step\n",
      "step 440/782 - loss: 0.0763 - acc_top1: 0.9610 - acc_top5: 0.9988 - 50ms/step\n",
      "step 450/782 - loss: 0.0947 - acc_top1: 0.9607 - acc_top5: 0.9988 - 50ms/step\n",
      "step 460/782 - loss: 0.1025 - acc_top1: 0.9605 - acc_top5: 0.9988 - 50ms/step\n",
      "step 470/782 - loss: 0.0988 - acc_top1: 0.9601 - acc_top5: 0.9988 - 50ms/step\n",
      "step 480/782 - loss: 0.2308 - acc_top1: 0.9603 - acc_top5: 0.9988 - 50ms/step\n",
      "step 490/782 - loss: 0.0451 - acc_top1: 0.9601 - acc_top5: 0.9988 - 50ms/step\n",
      "step 500/782 - loss: 0.0978 - acc_top1: 0.9601 - acc_top5: 0.9987 - 50ms/step\n",
      "step 510/782 - loss: 0.0783 - acc_top1: 0.9599 - acc_top5: 0.9987 - 50ms/step\n",
      "step 520/782 - loss: 0.1096 - acc_top1: 0.9596 - acc_top5: 0.9987 - 50ms/step\n",
      "step 530/782 - loss: 0.0607 - acc_top1: 0.9598 - acc_top5: 0.9987 - 50ms/step\n",
      "step 540/782 - loss: 0.0624 - acc_top1: 0.9597 - acc_top5: 0.9987 - 50ms/step\n",
      "step 550/782 - loss: 0.0934 - acc_top1: 0.9597 - acc_top5: 0.9988 - 50ms/step\n",
      "step 560/782 - loss: 0.0766 - acc_top1: 0.9598 - acc_top5: 0.9988 - 50ms/step\n",
      "step 570/782 - loss: 0.0334 - acc_top1: 0.9599 - acc_top5: 0.9987 - 50ms/step\n",
      "step 580/782 - loss: 0.0469 - acc_top1: 0.9598 - acc_top5: 0.9988 - 50ms/step\n",
      "step 590/782 - loss: 0.0272 - acc_top1: 0.9597 - acc_top5: 0.9988 - 50ms/step\n",
      "step 600/782 - loss: 0.1749 - acc_top1: 0.9599 - acc_top5: 0.9988 - 50ms/step\n",
      "step 610/782 - loss: 0.1680 - acc_top1: 0.9598 - acc_top5: 0.9988 - 50ms/step\n",
      "step 620/782 - loss: 0.0931 - acc_top1: 0.9598 - acc_top5: 0.9988 - 50ms/step\n",
      "step 630/782 - loss: 0.0505 - acc_top1: 0.9600 - acc_top5: 0.9988 - 50ms/step\n",
      "step 640/782 - loss: 0.1729 - acc_top1: 0.9600 - acc_top5: 0.9988 - 50ms/step\n",
      "step 650/782 - loss: 0.1530 - acc_top1: 0.9601 - acc_top5: 0.9988 - 50ms/step\n",
      "step 660/782 - loss: 0.0862 - acc_top1: 0.9601 - acc_top5: 0.9988 - 50ms/step\n",
      "step 670/782 - loss: 0.0990 - acc_top1: 0.9599 - acc_top5: 0.9987 - 50ms/step\n",
      "step 680/782 - loss: 0.0984 - acc_top1: 0.9599 - acc_top5: 0.9987 - 50ms/step\n",
      "step 690/782 - loss: 0.1615 - acc_top1: 0.9598 - acc_top5: 0.9987 - 50ms/step\n",
      "step 700/782 - loss: 0.1733 - acc_top1: 0.9595 - acc_top5: 0.9987 - 50ms/step\n",
      "step 710/782 - loss: 0.1004 - acc_top1: 0.9594 - acc_top5: 0.9987 - 50ms/step\n",
      "step 720/782 - loss: 0.0675 - acc_top1: 0.9593 - acc_top5: 0.9987 - 50ms/step\n",
      "step 730/782 - loss: 0.0899 - acc_top1: 0.9592 - acc_top5: 0.9987 - 50ms/step\n",
      "step 740/782 - loss: 0.1007 - acc_top1: 0.9591 - acc_top5: 0.9987 - 50ms/step\n",
      "step 750/782 - loss: 0.0640 - acc_top1: 0.9591 - acc_top5: 0.9987 - 50ms/step\n",
      "step 760/782 - loss: 0.0558 - acc_top1: 0.9591 - acc_top5: 0.9987 - 50ms/step\n",
      "step 770/782 - loss: 0.0753 - acc_top1: 0.9591 - acc_top5: 0.9987 - 50ms/step\n",
      "step 780/782 - loss: 0.0466 - acc_top1: 0.9592 - acc_top5: 0.9987 - 50ms/step\n",
      "step 782/782 - loss: 0.2534 - acc_top1: 0.9592 - acc_top5: 0.9987 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\44\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.5904 - acc_top1: 0.6344 - acc_top5: 0.9703 - 19ms/step\n",
      "step  20/157 - loss: 3.2929 - acc_top1: 0.6383 - acc_top5: 0.9664 - 18ms/step\n",
      "step  30/157 - loss: 2.5587 - acc_top1: 0.6302 - acc_top5: 0.9589 - 18ms/step\n",
      "step  40/157 - loss: 2.5015 - acc_top1: 0.6309 - acc_top5: 0.9570 - 18ms/step\n",
      "step  50/157 - loss: 2.2262 - acc_top1: 0.6328 - acc_top5: 0.9547 - 18ms/step\n",
      "step  60/157 - loss: 3.2080 - acc_top1: 0.6302 - acc_top5: 0.9523 - 18ms/step\n",
      "step  70/157 - loss: 7.1316 - acc_top1: 0.6292 - acc_top5: 0.9516 - 18ms/step\n",
      "step  80/157 - loss: 1.9152 - acc_top1: 0.6260 - acc_top5: 0.9510 - 18ms/step\n",
      "step  90/157 - loss: 1.5794 - acc_top1: 0.6245 - acc_top5: 0.9510 - 19ms/step\n",
      "step 100/157 - loss: 2.1213 - acc_top1: 0.6261 - acc_top5: 0.9502 - 18ms/step\n",
      "step 110/157 - loss: 4.1589 - acc_top1: 0.6233 - acc_top5: 0.9482 - 18ms/step\n",
      "step 120/157 - loss: 2.9754 - acc_top1: 0.6227 - acc_top5: 0.9474 - 18ms/step\n",
      "step 130/157 - loss: 2.2381 - acc_top1: 0.6237 - acc_top5: 0.9475 - 18ms/step\n",
      "step 140/157 - loss: 5.4243 - acc_top1: 0.6234 - acc_top5: 0.9487 - 18ms/step\n",
      "step 150/157 - loss: 11.6310 - acc_top1: 0.6248 - acc_top5: 0.9491 - 18ms/step\n",
      "step 157/157 - loss: 0.6400 - acc_top1: 0.6229 - acc_top5: 0.9491 - 18ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.1563 - acc_top1: 0.9625 - acc_top5: 1.0000 - 52ms/step\n",
      "step  20/782 - loss: 0.0962 - acc_top1: 0.9586 - acc_top5: 1.0000 - 51ms/step\n",
      "step  30/782 - loss: 0.1254 - acc_top1: 0.9594 - acc_top5: 0.9990 - 50ms/step\n",
      "step  40/782 - loss: 0.1049 - acc_top1: 0.9590 - acc_top5: 0.9992 - 50ms/step\n",
      "step  50/782 - loss: 0.1322 - acc_top1: 0.9597 - acc_top5: 0.9988 - 50ms/step\n",
      "step  60/782 - loss: 0.1113 - acc_top1: 0.9591 - acc_top5: 0.9990 - 50ms/step\n",
      "step  70/782 - loss: 0.0995 - acc_top1: 0.9598 - acc_top5: 0.9989 - 50ms/step\n",
      "step  80/782 - loss: 0.0520 - acc_top1: 0.9605 - acc_top5: 0.9990 - 50ms/step\n",
      "step  90/782 - loss: 0.1013 - acc_top1: 0.9613 - acc_top5: 0.9988 - 50ms/step\n",
      "step 100/782 - loss: 0.0591 - acc_top1: 0.9623 - acc_top5: 0.9988 - 51ms/step\n",
      "step 110/782 - loss: 0.0389 - acc_top1: 0.9631 - acc_top5: 0.9987 - 51ms/step\n",
      "step 120/782 - loss: 0.0467 - acc_top1: 0.9642 - acc_top5: 0.9988 - 51ms/step\n",
      "step 130/782 - loss: 0.0847 - acc_top1: 0.9644 - acc_top5: 0.9988 - 51ms/step\n",
      "step 140/782 - loss: 0.0683 - acc_top1: 0.9654 - acc_top5: 0.9989 - 51ms/step\n",
      "step 150/782 - loss: 0.1025 - acc_top1: 0.9665 - acc_top5: 0.9989 - 51ms/step\n",
      "step 160/782 - loss: 0.1614 - acc_top1: 0.9668 - acc_top5: 0.9989 - 51ms/step\n",
      "step 170/782 - loss: 0.0840 - acc_top1: 0.9675 - acc_top5: 0.9989 - 51ms/step\n",
      "step 180/782 - loss: 0.1094 - acc_top1: 0.9679 - acc_top5: 0.9990 - 51ms/step\n",
      "step 190/782 - loss: 0.0544 - acc_top1: 0.9680 - acc_top5: 0.9990 - 51ms/step\n",
      "step 200/782 - loss: 0.1597 - acc_top1: 0.9679 - acc_top5: 0.9989 - 51ms/step\n",
      "step 210/782 - loss: 0.0880 - acc_top1: 0.9684 - acc_top5: 0.9990 - 51ms/step\n",
      "step 220/782 - loss: 0.0575 - acc_top1: 0.9688 - acc_top5: 0.9989 - 51ms/step\n",
      "step 230/782 - loss: 0.0696 - acc_top1: 0.9687 - acc_top5: 0.9989 - 51ms/step\n",
      "step 240/782 - loss: 0.0413 - acc_top1: 0.9687 - acc_top5: 0.9989 - 51ms/step\n",
      "step 250/782 - loss: 0.2900 - acc_top1: 0.9685 - acc_top5: 0.9988 - 51ms/step\n",
      "step 260/782 - loss: 0.0593 - acc_top1: 0.9684 - acc_top5: 0.9988 - 51ms/step\n",
      "step 270/782 - loss: 0.0341 - acc_top1: 0.9689 - acc_top5: 0.9988 - 50ms/step\n",
      "step 280/782 - loss: 0.0753 - acc_top1: 0.9687 - acc_top5: 0.9988 - 50ms/step\n",
      "step 290/782 - loss: 0.1783 - acc_top1: 0.9686 - acc_top5: 0.9989 - 50ms/step\n",
      "step 300/782 - loss: 0.0870 - acc_top1: 0.9689 - acc_top5: 0.9988 - 50ms/step\n",
      "step 310/782 - loss: 0.1146 - acc_top1: 0.9690 - acc_top5: 0.9988 - 50ms/step\n",
      "step 320/782 - loss: 0.1138 - acc_top1: 0.9690 - acc_top5: 0.9988 - 50ms/step\n",
      "step 330/782 - loss: 0.1241 - acc_top1: 0.9692 - acc_top5: 0.9988 - 50ms/step\n",
      "step 340/782 - loss: 0.0131 - acc_top1: 0.9692 - acc_top5: 0.9988 - 50ms/step\n",
      "step 350/782 - loss: 0.0317 - acc_top1: 0.9692 - acc_top5: 0.9988 - 50ms/step\n",
      "step 360/782 - loss: 0.1240 - acc_top1: 0.9688 - acc_top5: 0.9987 - 50ms/step\n",
      "step 370/782 - loss: 0.0703 - acc_top1: 0.9689 - acc_top5: 0.9988 - 50ms/step\n",
      "step 380/782 - loss: 0.2409 - acc_top1: 0.9685 - acc_top5: 0.9988 - 50ms/step\n",
      "step 390/782 - loss: 0.0525 - acc_top1: 0.9683 - acc_top5: 0.9988 - 50ms/step\n",
      "step 400/782 - loss: 0.3418 - acc_top1: 0.9682 - acc_top5: 0.9988 - 50ms/step\n",
      "step 410/782 - loss: 0.0564 - acc_top1: 0.9684 - acc_top5: 0.9988 - 50ms/step\n",
      "step 420/782 - loss: 0.1304 - acc_top1: 0.9685 - acc_top5: 0.9988 - 50ms/step\n",
      "step 430/782 - loss: 0.0305 - acc_top1: 0.9686 - acc_top5: 0.9988 - 50ms/step\n",
      "step 440/782 - loss: 0.2269 - acc_top1: 0.9682 - acc_top5: 0.9988 - 50ms/step\n",
      "step 450/782 - loss: 0.1663 - acc_top1: 0.9682 - acc_top5: 0.9987 - 50ms/step\n",
      "step 460/782 - loss: 0.0586 - acc_top1: 0.9681 - acc_top5: 0.9987 - 50ms/step\n",
      "step 470/782 - loss: 0.1865 - acc_top1: 0.9681 - acc_top5: 0.9987 - 50ms/step\n",
      "step 480/782 - loss: 0.1613 - acc_top1: 0.9678 - acc_top5: 0.9986 - 50ms/step\n",
      "step 490/782 - loss: 0.1445 - acc_top1: 0.9680 - acc_top5: 0.9986 - 50ms/step\n",
      "step 500/782 - loss: 0.0263 - acc_top1: 0.9680 - acc_top5: 0.9987 - 50ms/step\n",
      "step 510/782 - loss: 0.0702 - acc_top1: 0.9681 - acc_top5: 0.9987 - 50ms/step\n",
      "step 520/782 - loss: 0.1555 - acc_top1: 0.9681 - acc_top5: 0.9986 - 50ms/step\n",
      "step 530/782 - loss: 0.1003 - acc_top1: 0.9678 - acc_top5: 0.9986 - 50ms/step\n",
      "step 540/782 - loss: 0.0972 - acc_top1: 0.9677 - acc_top5: 0.9986 - 50ms/step\n",
      "step 550/782 - loss: 0.1205 - acc_top1: 0.9677 - acc_top5: 0.9987 - 50ms/step\n",
      "step 560/782 - loss: 0.0895 - acc_top1: 0.9677 - acc_top5: 0.9987 - 50ms/step\n",
      "step 570/782 - loss: 0.0616 - acc_top1: 0.9678 - acc_top5: 0.9987 - 50ms/step\n",
      "step 580/782 - loss: 0.0774 - acc_top1: 0.9677 - acc_top5: 0.9986 - 50ms/step\n",
      "step 590/782 - loss: 0.1265 - acc_top1: 0.9677 - acc_top5: 0.9986 - 50ms/step\n",
      "step 600/782 - loss: 0.0951 - acc_top1: 0.9674 - acc_top5: 0.9986 - 50ms/step\n",
      "step 610/782 - loss: 0.1010 - acc_top1: 0.9673 - acc_top5: 0.9986 - 50ms/step\n",
      "step 620/782 - loss: 0.1919 - acc_top1: 0.9674 - acc_top5: 0.9986 - 50ms/step\n",
      "step 630/782 - loss: 0.0624 - acc_top1: 0.9674 - acc_top5: 0.9986 - 50ms/step\n",
      "step 640/782 - loss: 0.1016 - acc_top1: 0.9672 - acc_top5: 0.9987 - 50ms/step\n",
      "step 650/782 - loss: 0.0340 - acc_top1: 0.9672 - acc_top5: 0.9987 - 50ms/step\n",
      "step 660/782 - loss: 0.1002 - acc_top1: 0.9670 - acc_top5: 0.9987 - 50ms/step\n",
      "step 670/782 - loss: 0.0993 - acc_top1: 0.9669 - acc_top5: 0.9987 - 50ms/step\n",
      "step 680/782 - loss: 0.1923 - acc_top1: 0.9665 - acc_top5: 0.9987 - 50ms/step\n",
      "step 690/782 - loss: 0.0740 - acc_top1: 0.9663 - acc_top5: 0.9987 - 50ms/step\n",
      "step 700/782 - loss: 0.1078 - acc_top1: 0.9663 - acc_top5: 0.9987 - 50ms/step\n",
      "step 710/782 - loss: 0.1974 - acc_top1: 0.9661 - acc_top5: 0.9987 - 50ms/step\n",
      "step 720/782 - loss: 0.0480 - acc_top1: 0.9661 - acc_top5: 0.9987 - 50ms/step\n",
      "step 730/782 - loss: 0.0789 - acc_top1: 0.9658 - acc_top5: 0.9987 - 50ms/step\n",
      "step 740/782 - loss: 0.0543 - acc_top1: 0.9657 - acc_top5: 0.9987 - 50ms/step\n",
      "step 750/782 - loss: 0.0994 - acc_top1: 0.9656 - acc_top5: 0.9987 - 50ms/step\n",
      "step 760/782 - loss: 0.4475 - acc_top1: 0.9654 - acc_top5: 0.9987 - 50ms/step\n",
      "step 770/782 - loss: 0.0327 - acc_top1: 0.9653 - acc_top5: 0.9987 - 50ms/step\n",
      "step 780/782 - loss: 0.0883 - acc_top1: 0.9651 - acc_top5: 0.9987 - 50ms/step\n",
      "step 782/782 - loss: 0.4282 - acc_top1: 0.9650 - acc_top5: 0.9987 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\45\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.2948 - acc_top1: 0.6562 - acc_top5: 0.9719 - 18ms/step\n",
      "step  20/157 - loss: 2.7858 - acc_top1: 0.6500 - acc_top5: 0.9609 - 18ms/step\n",
      "step  30/157 - loss: 2.0591 - acc_top1: 0.6370 - acc_top5: 0.9589 - 18ms/step\n",
      "step  40/157 - loss: 2.8398 - acc_top1: 0.6379 - acc_top5: 0.9578 - 18ms/step\n",
      "step  50/157 - loss: 1.7553 - acc_top1: 0.6350 - acc_top5: 0.9541 - 18ms/step\n",
      "step  60/157 - loss: 1.8639 - acc_top1: 0.6328 - acc_top5: 0.9510 - 18ms/step\n",
      "step  70/157 - loss: 2.3446 - acc_top1: 0.6317 - acc_top5: 0.9502 - 18ms/step\n",
      "step  80/157 - loss: 2.2091 - acc_top1: 0.6289 - acc_top5: 0.9494 - 18ms/step\n",
      "step  90/157 - loss: 1.4782 - acc_top1: 0.6307 - acc_top5: 0.9502 - 18ms/step\n",
      "step 100/157 - loss: 1.7789 - acc_top1: 0.6320 - acc_top5: 0.9494 - 18ms/step\n",
      "step 110/157 - loss: 3.0110 - acc_top1: 0.6298 - acc_top5: 0.9483 - 18ms/step\n",
      "step 120/157 - loss: 1.8719 - acc_top1: 0.6294 - acc_top5: 0.9486 - 18ms/step\n",
      "step 130/157 - loss: 1.8998 - acc_top1: 0.6300 - acc_top5: 0.9493 - 18ms/step\n",
      "step 140/157 - loss: 1.5942 - acc_top1: 0.6283 - acc_top5: 0.9507 - 18ms/step\n",
      "step 150/157 - loss: 1.8898 - acc_top1: 0.6275 - acc_top5: 0.9514 - 18ms/step\n",
      "step 157/157 - loss: 0.8119 - acc_top1: 0.6262 - acc_top5: 0.9503 - 18ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.1679 - acc_top1: 0.9641 - acc_top5: 0.9984 - 51ms/step\n",
      "step  20/782 - loss: 0.1385 - acc_top1: 0.9586 - acc_top5: 0.9984 - 51ms/step\n",
      "step  30/782 - loss: 0.0657 - acc_top1: 0.9630 - acc_top5: 0.9984 - 51ms/step\n",
      "step  40/782 - loss: 0.1955 - acc_top1: 0.9594 - acc_top5: 0.9980 - 51ms/step\n",
      "step  50/782 - loss: 0.2457 - acc_top1: 0.9587 - acc_top5: 0.9981 - 51ms/step\n",
      "step  60/782 - loss: 0.1097 - acc_top1: 0.9589 - acc_top5: 0.9984 - 50ms/step\n",
      "step  70/782 - loss: 0.1097 - acc_top1: 0.9600 - acc_top5: 0.9987 - 50ms/step\n",
      "step  80/782 - loss: 0.0449 - acc_top1: 0.9604 - acc_top5: 0.9988 - 50ms/step\n",
      "step  90/782 - loss: 0.0893 - acc_top1: 0.9618 - acc_top5: 0.9990 - 50ms/step\n",
      "step 100/782 - loss: 0.0505 - acc_top1: 0.9630 - acc_top5: 0.9991 - 50ms/step\n",
      "step 110/782 - loss: 0.0715 - acc_top1: 0.9624 - acc_top5: 0.9990 - 50ms/step\n",
      "step 120/782 - loss: 0.0480 - acc_top1: 0.9639 - acc_top5: 0.9990 - 50ms/step\n",
      "step 130/782 - loss: 0.0340 - acc_top1: 0.9649 - acc_top5: 0.9989 - 50ms/step\n",
      "step 140/782 - loss: 0.0280 - acc_top1: 0.9647 - acc_top5: 0.9989 - 50ms/step\n",
      "step 150/782 - loss: 0.0466 - acc_top1: 0.9651 - acc_top5: 0.9989 - 50ms/step\n",
      "step 160/782 - loss: 0.1568 - acc_top1: 0.9646 - acc_top5: 0.9988 - 51ms/step\n",
      "step 170/782 - loss: 0.0333 - acc_top1: 0.9646 - acc_top5: 0.9989 - 51ms/step\n",
      "step 180/782 - loss: 0.0835 - acc_top1: 0.9647 - acc_top5: 0.9989 - 51ms/step\n",
      "step 190/782 - loss: 0.1092 - acc_top1: 0.9643 - acc_top5: 0.9989 - 51ms/step\n",
      "step 200/782 - loss: 0.0471 - acc_top1: 0.9644 - acc_top5: 0.9990 - 51ms/step\n",
      "step 210/782 - loss: 0.0358 - acc_top1: 0.9647 - acc_top5: 0.9990 - 51ms/step\n",
      "step 220/782 - loss: 0.0713 - acc_top1: 0.9645 - acc_top5: 0.9988 - 51ms/step\n",
      "step 230/782 - loss: 0.1265 - acc_top1: 0.9649 - acc_top5: 0.9988 - 51ms/step\n",
      "step 240/782 - loss: 0.2391 - acc_top1: 0.9645 - acc_top5: 0.9988 - 51ms/step\n",
      "step 250/782 - loss: 0.0811 - acc_top1: 0.9647 - acc_top5: 0.9988 - 51ms/step\n",
      "step 260/782 - loss: 0.1170 - acc_top1: 0.9650 - acc_top5: 0.9988 - 50ms/step\n",
      "step 270/782 - loss: 0.0441 - acc_top1: 0.9652 - acc_top5: 0.9988 - 50ms/step\n",
      "step 280/782 - loss: 0.1505 - acc_top1: 0.9654 - acc_top5: 0.9988 - 50ms/step\n",
      "step 290/782 - loss: 0.2145 - acc_top1: 0.9654 - acc_top5: 0.9987 - 50ms/step\n",
      "step 300/782 - loss: 0.0173 - acc_top1: 0.9654 - acc_top5: 0.9986 - 50ms/step\n",
      "step 310/782 - loss: 0.0505 - acc_top1: 0.9654 - acc_top5: 0.9987 - 50ms/step\n",
      "step 320/782 - loss: 0.1813 - acc_top1: 0.9652 - acc_top5: 0.9987 - 50ms/step\n",
      "step 330/782 - loss: 0.1111 - acc_top1: 0.9653 - acc_top5: 0.9988 - 50ms/step\n",
      "step 340/782 - loss: 0.0286 - acc_top1: 0.9654 - acc_top5: 0.9988 - 50ms/step\n",
      "step 350/782 - loss: 0.1099 - acc_top1: 0.9654 - acc_top5: 0.9988 - 50ms/step\n",
      "step 360/782 - loss: 0.0534 - acc_top1: 0.9658 - acc_top5: 0.9987 - 50ms/step\n",
      "step 370/782 - loss: 0.1564 - acc_top1: 0.9658 - acc_top5: 0.9988 - 50ms/step\n",
      "step 380/782 - loss: 0.0898 - acc_top1: 0.9656 - acc_top5: 0.9988 - 50ms/step\n",
      "step 390/782 - loss: 0.1170 - acc_top1: 0.9651 - acc_top5: 0.9988 - 50ms/step\n",
      "step 400/782 - loss: 0.0548 - acc_top1: 0.9648 - acc_top5: 0.9987 - 50ms/step\n",
      "step 410/782 - loss: 0.2264 - acc_top1: 0.9646 - acc_top5: 0.9987 - 50ms/step\n",
      "step 420/782 - loss: 0.0943 - acc_top1: 0.9647 - acc_top5: 0.9987 - 50ms/step\n",
      "step 430/782 - loss: 0.0960 - acc_top1: 0.9645 - acc_top5: 0.9988 - 50ms/step\n",
      "step 440/782 - loss: 0.0237 - acc_top1: 0.9645 - acc_top5: 0.9988 - 50ms/step\n",
      "step 450/782 - loss: 0.0573 - acc_top1: 0.9645 - acc_top5: 0.9988 - 50ms/step\n",
      "step 460/782 - loss: 0.0627 - acc_top1: 0.9644 - acc_top5: 0.9987 - 50ms/step\n",
      "step 470/782 - loss: 0.1094 - acc_top1: 0.9645 - acc_top5: 0.9987 - 50ms/step\n",
      "step 480/782 - loss: 0.0947 - acc_top1: 0.9645 - acc_top5: 0.9988 - 50ms/step\n",
      "step 490/782 - loss: 0.0709 - acc_top1: 0.9642 - acc_top5: 0.9987 - 50ms/step\n",
      "step 500/782 - loss: 0.0346 - acc_top1: 0.9643 - acc_top5: 0.9986 - 50ms/step\n",
      "step 510/782 - loss: 0.0450 - acc_top1: 0.9644 - acc_top5: 0.9986 - 50ms/step\n",
      "step 520/782 - loss: 0.0480 - acc_top1: 0.9642 - acc_top5: 0.9986 - 50ms/step\n",
      "step 530/782 - loss: 0.0770 - acc_top1: 0.9641 - acc_top5: 0.9987 - 50ms/step\n",
      "step 540/782 - loss: 0.1193 - acc_top1: 0.9641 - acc_top5: 0.9987 - 50ms/step\n",
      "step 550/782 - loss: 0.0533 - acc_top1: 0.9639 - acc_top5: 0.9987 - 50ms/step\n",
      "step 560/782 - loss: 0.1825 - acc_top1: 0.9638 - acc_top5: 0.9987 - 50ms/step\n",
      "step 570/782 - loss: 0.0939 - acc_top1: 0.9637 - acc_top5: 0.9987 - 50ms/step\n",
      "step 580/782 - loss: 0.0852 - acc_top1: 0.9637 - acc_top5: 0.9987 - 50ms/step\n",
      "step 590/782 - loss: 0.2172 - acc_top1: 0.9636 - acc_top5: 0.9987 - 50ms/step\n",
      "step 600/782 - loss: 0.0278 - acc_top1: 0.9636 - acc_top5: 0.9988 - 50ms/step\n",
      "step 610/782 - loss: 0.0642 - acc_top1: 0.9637 - acc_top5: 0.9987 - 50ms/step\n",
      "step 620/782 - loss: 0.1779 - acc_top1: 0.9639 - acc_top5: 0.9987 - 50ms/step\n",
      "step 630/782 - loss: 0.0980 - acc_top1: 0.9640 - acc_top5: 0.9988 - 50ms/step\n",
      "step 640/782 - loss: 0.0868 - acc_top1: 0.9640 - acc_top5: 0.9987 - 50ms/step\n",
      "step 650/782 - loss: 0.1063 - acc_top1: 0.9639 - acc_top5: 0.9987 - 50ms/step\n",
      "step 660/782 - loss: 0.1587 - acc_top1: 0.9640 - acc_top5: 0.9987 - 50ms/step\n",
      "step 670/782 - loss: 0.0564 - acc_top1: 0.9640 - acc_top5: 0.9987 - 50ms/step\n",
      "step 680/782 - loss: 0.0914 - acc_top1: 0.9640 - acc_top5: 0.9987 - 50ms/step\n",
      "step 690/782 - loss: 0.0642 - acc_top1: 0.9640 - acc_top5: 0.9987 - 50ms/step\n",
      "step 700/782 - loss: 0.1610 - acc_top1: 0.9639 - acc_top5: 0.9987 - 50ms/step\n",
      "step 710/782 - loss: 0.1721 - acc_top1: 0.9637 - acc_top5: 0.9987 - 50ms/step\n",
      "step 720/782 - loss: 0.0961 - acc_top1: 0.9636 - acc_top5: 0.9988 - 50ms/step\n",
      "step 730/782 - loss: 0.1543 - acc_top1: 0.9635 - acc_top5: 0.9988 - 50ms/step\n",
      "step 740/782 - loss: 0.0965 - acc_top1: 0.9636 - acc_top5: 0.9988 - 50ms/step\n",
      "step 750/782 - loss: 0.0747 - acc_top1: 0.9636 - acc_top5: 0.9988 - 50ms/step\n",
      "step 760/782 - loss: 0.0881 - acc_top1: 0.9637 - acc_top5: 0.9988 - 50ms/step\n",
      "step 770/782 - loss: 0.1437 - acc_top1: 0.9636 - acc_top5: 0.9988 - 50ms/step\n",
      "step 780/782 - loss: 0.2215 - acc_top1: 0.9636 - acc_top5: 0.9989 - 50ms/step\n",
      "step 782/782 - loss: 0.6220 - acc_top1: 0.9635 - acc_top5: 0.9989 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\46\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.3416 - acc_top1: 0.6359 - acc_top5: 0.9547 - 19ms/step\n",
      "step  20/157 - loss: 3.2512 - acc_top1: 0.6484 - acc_top5: 0.9516 - 19ms/step\n",
      "step  30/157 - loss: 2.1542 - acc_top1: 0.6521 - acc_top5: 0.9484 - 19ms/step\n",
      "step  40/157 - loss: 2.5948 - acc_top1: 0.6492 - acc_top5: 0.9484 - 19ms/step\n",
      "step  50/157 - loss: 1.7495 - acc_top1: 0.6438 - acc_top5: 0.9484 - 19ms/step\n",
      "step  60/157 - loss: 2.1329 - acc_top1: 0.6404 - acc_top5: 0.9482 - 19ms/step\n",
      "step  70/157 - loss: 4.8388 - acc_top1: 0.6353 - acc_top5: 0.9489 - 19ms/step\n",
      "step  80/157 - loss: 2.2732 - acc_top1: 0.6352 - acc_top5: 0.9496 - 19ms/step\n",
      "step  90/157 - loss: 1.8449 - acc_top1: 0.6351 - acc_top5: 0.9497 - 19ms/step\n",
      "step 100/157 - loss: 1.9279 - acc_top1: 0.6352 - acc_top5: 0.9503 - 19ms/step\n",
      "step 110/157 - loss: 2.9279 - acc_top1: 0.6310 - acc_top5: 0.9491 - 19ms/step\n",
      "step 120/157 - loss: 2.9215 - acc_top1: 0.6298 - acc_top5: 0.9490 - 19ms/step\n",
      "step 130/157 - loss: 2.2355 - acc_top1: 0.6290 - acc_top5: 0.9498 - 19ms/step\n",
      "step 140/157 - loss: 2.9889 - acc_top1: 0.6279 - acc_top5: 0.9500 - 19ms/step\n",
      "step 150/157 - loss: 5.7312 - acc_top1: 0.6281 - acc_top5: 0.9502 - 19ms/step\n",
      "step 157/157 - loss: 1.1412 - acc_top1: 0.6253 - acc_top5: 0.9499 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.1635 - acc_top1: 0.9594 - acc_top5: 0.9969 - 51ms/step\n",
      "step  20/782 - loss: 0.2202 - acc_top1: 0.9602 - acc_top5: 0.9977 - 50ms/step\n",
      "step  30/782 - loss: 0.1541 - acc_top1: 0.9536 - acc_top5: 0.9969 - 50ms/step\n",
      "step  40/782 - loss: 0.2274 - acc_top1: 0.9520 - acc_top5: 0.9973 - 50ms/step\n",
      "step  50/782 - loss: 0.1201 - acc_top1: 0.9522 - acc_top5: 0.9978 - 50ms/step\n",
      "step  60/782 - loss: 0.1605 - acc_top1: 0.9516 - acc_top5: 0.9977 - 50ms/step\n",
      "step  70/782 - loss: 0.0705 - acc_top1: 0.9533 - acc_top5: 0.9978 - 50ms/step\n",
      "step  80/782 - loss: 0.1304 - acc_top1: 0.9539 - acc_top5: 0.9977 - 50ms/step\n",
      "step  90/782 - loss: 0.1501 - acc_top1: 0.9556 - acc_top5: 0.9979 - 50ms/step\n",
      "step 100/782 - loss: 0.1016 - acc_top1: 0.9564 - acc_top5: 0.9981 - 50ms/step\n",
      "step 110/782 - loss: 0.0670 - acc_top1: 0.9578 - acc_top5: 0.9983 - 50ms/step\n",
      "step 120/782 - loss: 0.0335 - acc_top1: 0.9582 - acc_top5: 0.9983 - 50ms/step\n",
      "step 130/782 - loss: 0.0690 - acc_top1: 0.9593 - acc_top5: 0.9984 - 50ms/step\n",
      "step 140/782 - loss: 0.0744 - acc_top1: 0.9595 - acc_top5: 0.9984 - 50ms/step\n",
      "step 150/782 - loss: 0.1216 - acc_top1: 0.9600 - acc_top5: 0.9985 - 50ms/step\n",
      "step 160/782 - loss: 0.1460 - acc_top1: 0.9604 - acc_top5: 0.9985 - 50ms/step\n",
      "step 170/782 - loss: 0.0307 - acc_top1: 0.9607 - acc_top5: 0.9986 - 50ms/step\n",
      "step 180/782 - loss: 0.0773 - acc_top1: 0.9612 - acc_top5: 0.9985 - 50ms/step\n",
      "step 190/782 - loss: 0.0725 - acc_top1: 0.9619 - acc_top5: 0.9986 - 50ms/step\n",
      "step 200/782 - loss: 0.0399 - acc_top1: 0.9624 - acc_top5: 0.9986 - 50ms/step\n",
      "step 210/782 - loss: 0.0993 - acc_top1: 0.9619 - acc_top5: 0.9985 - 50ms/step\n",
      "step 220/782 - loss: 0.1330 - acc_top1: 0.9619 - acc_top5: 0.9985 - 50ms/step\n",
      "step 230/782 - loss: 0.0546 - acc_top1: 0.9617 - acc_top5: 0.9986 - 50ms/step\n",
      "step 240/782 - loss: 0.2090 - acc_top1: 0.9617 - acc_top5: 0.9986 - 50ms/step\n",
      "step 250/782 - loss: 0.0167 - acc_top1: 0.9621 - acc_top5: 0.9986 - 50ms/step\n",
      "step 260/782 - loss: 0.2273 - acc_top1: 0.9615 - acc_top5: 0.9984 - 50ms/step\n",
      "step 270/782 - loss: 0.0530 - acc_top1: 0.9616 - acc_top5: 0.9985 - 50ms/step\n",
      "step 280/782 - loss: 0.0490 - acc_top1: 0.9616 - acc_top5: 0.9985 - 50ms/step\n",
      "step 290/782 - loss: 0.1394 - acc_top1: 0.9619 - acc_top5: 0.9986 - 50ms/step\n",
      "step 300/782 - loss: 0.0804 - acc_top1: 0.9620 - acc_top5: 0.9986 - 50ms/step\n",
      "step 310/782 - loss: 0.1346 - acc_top1: 0.9626 - acc_top5: 0.9987 - 50ms/step\n",
      "step 320/782 - loss: 0.0823 - acc_top1: 0.9627 - acc_top5: 0.9987 - 50ms/step\n",
      "step 330/782 - loss: 0.0606 - acc_top1: 0.9632 - acc_top5: 0.9987 - 50ms/step\n",
      "step 340/782 - loss: 0.0434 - acc_top1: 0.9636 - acc_top5: 0.9987 - 50ms/step\n",
      "step 350/782 - loss: 0.1477 - acc_top1: 0.9637 - acc_top5: 0.9988 - 50ms/step\n",
      "step 360/782 - loss: 0.2022 - acc_top1: 0.9637 - acc_top5: 0.9987 - 50ms/step\n",
      "step 370/782 - loss: 0.0374 - acc_top1: 0.9639 - acc_top5: 0.9987 - 50ms/step\n",
      "step 380/782 - loss: 0.0440 - acc_top1: 0.9637 - acc_top5: 0.9987 - 50ms/step\n",
      "step 390/782 - loss: 0.1143 - acc_top1: 0.9635 - acc_top5: 0.9987 - 50ms/step\n",
      "step 400/782 - loss: 0.0750 - acc_top1: 0.9637 - acc_top5: 0.9987 - 50ms/step\n",
      "step 410/782 - loss: 0.1388 - acc_top1: 0.9639 - acc_top5: 0.9987 - 50ms/step\n",
      "step 420/782 - loss: 0.0877 - acc_top1: 0.9641 - acc_top5: 0.9987 - 50ms/step\n",
      "step 430/782 - loss: 0.0555 - acc_top1: 0.9640 - acc_top5: 0.9987 - 50ms/step\n",
      "step 440/782 - loss: 0.0650 - acc_top1: 0.9641 - acc_top5: 0.9988 - 50ms/step\n",
      "step 450/782 - loss: 0.2125 - acc_top1: 0.9642 - acc_top5: 0.9988 - 50ms/step\n",
      "step 460/782 - loss: 0.1199 - acc_top1: 0.9640 - acc_top5: 0.9987 - 50ms/step\n",
      "step 470/782 - loss: 0.0780 - acc_top1: 0.9641 - acc_top5: 0.9988 - 50ms/step\n",
      "step 480/782 - loss: 0.0967 - acc_top1: 0.9640 - acc_top5: 0.9988 - 50ms/step\n",
      "step 490/782 - loss: 0.1067 - acc_top1: 0.9638 - acc_top5: 0.9988 - 50ms/step\n",
      "step 500/782 - loss: 0.0646 - acc_top1: 0.9637 - acc_top5: 0.9988 - 50ms/step\n",
      "step 510/782 - loss: 0.0628 - acc_top1: 0.9637 - acc_top5: 0.9988 - 50ms/step\n",
      "step 520/782 - loss: 0.0227 - acc_top1: 0.9640 - acc_top5: 0.9988 - 50ms/step\n",
      "step 530/782 - loss: 0.1272 - acc_top1: 0.9638 - acc_top5: 0.9988 - 50ms/step\n",
      "step 540/782 - loss: 0.1460 - acc_top1: 0.9639 - acc_top5: 0.9988 - 50ms/step\n",
      "step 550/782 - loss: 0.1620 - acc_top1: 0.9638 - acc_top5: 0.9988 - 50ms/step\n",
      "step 560/782 - loss: 0.0256 - acc_top1: 0.9640 - acc_top5: 0.9988 - 50ms/step\n",
      "step 570/782 - loss: 0.0968 - acc_top1: 0.9642 - acc_top5: 0.9988 - 50ms/step\n",
      "step 580/782 - loss: 0.0301 - acc_top1: 0.9642 - acc_top5: 0.9988 - 50ms/step\n",
      "step 590/782 - loss: 0.0442 - acc_top1: 0.9643 - acc_top5: 0.9988 - 50ms/step\n",
      "step 600/782 - loss: 0.0681 - acc_top1: 0.9644 - acc_top5: 0.9989 - 50ms/step\n",
      "step 610/782 - loss: 0.0619 - acc_top1: 0.9644 - acc_top5: 0.9988 - 50ms/step\n",
      "step 620/782 - loss: 0.0996 - acc_top1: 0.9642 - acc_top5: 0.9988 - 50ms/step\n",
      "step 630/782 - loss: 0.0744 - acc_top1: 0.9642 - acc_top5: 0.9988 - 50ms/step\n",
      "step 640/782 - loss: 0.1116 - acc_top1: 0.9642 - acc_top5: 0.9988 - 50ms/step\n",
      "step 650/782 - loss: 0.1492 - acc_top1: 0.9641 - acc_top5: 0.9988 - 50ms/step\n",
      "step 660/782 - loss: 0.2188 - acc_top1: 0.9640 - acc_top5: 0.9988 - 50ms/step\n",
      "step 670/782 - loss: 0.1291 - acc_top1: 0.9641 - acc_top5: 0.9988 - 50ms/step\n",
      "step 680/782 - loss: 0.1415 - acc_top1: 0.9643 - acc_top5: 0.9989 - 50ms/step\n",
      "step 690/782 - loss: 0.0929 - acc_top1: 0.9643 - acc_top5: 0.9988 - 50ms/step\n",
      "step 700/782 - loss: 0.0260 - acc_top1: 0.9642 - acc_top5: 0.9988 - 50ms/step\n",
      "step 710/782 - loss: 0.1131 - acc_top1: 0.9643 - acc_top5: 0.9988 - 50ms/step\n",
      "step 720/782 - loss: 0.0776 - acc_top1: 0.9644 - acc_top5: 0.9988 - 50ms/step\n",
      "step 730/782 - loss: 0.1555 - acc_top1: 0.9642 - acc_top5: 0.9988 - 50ms/step\n",
      "step 740/782 - loss: 0.0466 - acc_top1: 0.9641 - acc_top5: 0.9988 - 50ms/step\n",
      "step 750/782 - loss: 0.0330 - acc_top1: 0.9643 - acc_top5: 0.9988 - 50ms/step\n",
      "step 760/782 - loss: 0.0612 - acc_top1: 0.9643 - acc_top5: 0.9988 - 50ms/step\n",
      "step 770/782 - loss: 0.1467 - acc_top1: 0.9642 - acc_top5: 0.9988 - 50ms/step\n",
      "step 780/782 - loss: 0.1030 - acc_top1: 0.9641 - acc_top5: 0.9988 - 50ms/step\n",
      "step 782/782 - loss: 1.1435 - acc_top1: 0.9641 - acc_top5: 0.9988 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\47\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.6885 - acc_top1: 0.6422 - acc_top5: 0.9563 - 19ms/step\n",
      "step  20/157 - loss: 3.0383 - acc_top1: 0.6383 - acc_top5: 0.9555 - 19ms/step\n",
      "step  30/157 - loss: 2.1849 - acc_top1: 0.6359 - acc_top5: 0.9516 - 19ms/step\n",
      "step  40/157 - loss: 2.7145 - acc_top1: 0.6359 - acc_top5: 0.9543 - 19ms/step\n",
      "step  50/157 - loss: 1.7771 - acc_top1: 0.6388 - acc_top5: 0.9556 - 19ms/step\n",
      "step  60/157 - loss: 2.0775 - acc_top1: 0.6388 - acc_top5: 0.9534 - 19ms/step\n",
      "step  70/157 - loss: 8.2568 - acc_top1: 0.6364 - acc_top5: 0.9520 - 19ms/step\n",
      "step  80/157 - loss: 2.3318 - acc_top1: 0.6334 - acc_top5: 0.9512 - 19ms/step\n",
      "step  90/157 - loss: 1.3844 - acc_top1: 0.6337 - acc_top5: 0.9509 - 19ms/step\n",
      "step 100/157 - loss: 1.9572 - acc_top1: 0.6319 - acc_top5: 0.9505 - 19ms/step\n",
      "step 110/157 - loss: 2.9139 - acc_top1: 0.6300 - acc_top5: 0.9503 - 19ms/step\n",
      "step 120/157 - loss: 2.2610 - acc_top1: 0.6285 - acc_top5: 0.9496 - 19ms/step\n",
      "step 130/157 - loss: 2.2209 - acc_top1: 0.6294 - acc_top5: 0.9499 - 19ms/step\n",
      "step 140/157 - loss: 5.6848 - acc_top1: 0.6287 - acc_top5: 0.9501 - 19ms/step\n",
      "step 150/157 - loss: 12.5883 - acc_top1: 0.6298 - acc_top5: 0.9508 - 19ms/step\n",
      "step 157/157 - loss: 0.9156 - acc_top1: 0.6281 - acc_top5: 0.9509 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.1214 - acc_top1: 0.9547 - acc_top5: 0.9984 - 52ms/step\n",
      "step  20/782 - loss: 0.2399 - acc_top1: 0.9555 - acc_top5: 0.9977 - 51ms/step\n",
      "step  30/782 - loss: 0.0913 - acc_top1: 0.9568 - acc_top5: 0.9984 - 51ms/step\n",
      "step  40/782 - loss: 0.1101 - acc_top1: 0.9578 - acc_top5: 0.9988 - 51ms/step\n",
      "step  50/782 - loss: 0.1304 - acc_top1: 0.9603 - acc_top5: 0.9988 - 50ms/step\n",
      "step  60/782 - loss: 0.1786 - acc_top1: 0.9591 - acc_top5: 0.9990 - 50ms/step\n",
      "step  70/782 - loss: 0.2315 - acc_top1: 0.9585 - acc_top5: 0.9987 - 51ms/step\n",
      "step  80/782 - loss: 0.0289 - acc_top1: 0.9592 - acc_top5: 0.9986 - 51ms/step\n",
      "step  90/782 - loss: 0.0439 - acc_top1: 0.9613 - acc_top5: 0.9988 - 51ms/step\n",
      "step 100/782 - loss: 0.1453 - acc_top1: 0.9608 - acc_top5: 0.9986 - 51ms/step\n",
      "step 110/782 - loss: 0.2494 - acc_top1: 0.9612 - acc_top5: 0.9984 - 51ms/step\n",
      "step 120/782 - loss: 0.1395 - acc_top1: 0.9615 - acc_top5: 0.9986 - 51ms/step\n",
      "step 130/782 - loss: 0.1980 - acc_top1: 0.9625 - acc_top5: 0.9986 - 51ms/step\n",
      "step 140/782 - loss: 0.0123 - acc_top1: 0.9637 - acc_top5: 0.9985 - 51ms/step\n",
      "step 150/782 - loss: 0.1365 - acc_top1: 0.9644 - acc_top5: 0.9986 - 50ms/step\n",
      "step 160/782 - loss: 0.1244 - acc_top1: 0.9646 - acc_top5: 0.9985 - 50ms/step\n",
      "step 170/782 - loss: 0.0773 - acc_top1: 0.9650 - acc_top5: 0.9985 - 50ms/step\n",
      "step 180/782 - loss: 0.1153 - acc_top1: 0.9655 - acc_top5: 0.9986 - 50ms/step\n",
      "step 190/782 - loss: 0.0521 - acc_top1: 0.9659 - acc_top5: 0.9987 - 50ms/step\n",
      "step 200/782 - loss: 0.1427 - acc_top1: 0.9657 - acc_top5: 0.9985 - 50ms/step\n",
      "step 210/782 - loss: 0.0508 - acc_top1: 0.9665 - acc_top5: 0.9986 - 50ms/step\n",
      "step 220/782 - loss: 0.0902 - acc_top1: 0.9668 - acc_top5: 0.9987 - 50ms/step\n",
      "step 230/782 - loss: 0.2566 - acc_top1: 0.9664 - acc_top5: 0.9986 - 50ms/step\n",
      "step 240/782 - loss: 0.2000 - acc_top1: 0.9664 - acc_top5: 0.9986 - 50ms/step\n",
      "step 250/782 - loss: 0.0838 - acc_top1: 0.9667 - acc_top5: 0.9986 - 50ms/step\n",
      "step 260/782 - loss: 0.0192 - acc_top1: 0.9671 - acc_top5: 0.9986 - 51ms/step\n",
      "step 270/782 - loss: 0.0598 - acc_top1: 0.9672 - acc_top5: 0.9986 - 51ms/step\n",
      "step 280/782 - loss: 0.1280 - acc_top1: 0.9672 - acc_top5: 0.9985 - 51ms/step\n",
      "step 290/782 - loss: 0.1358 - acc_top1: 0.9673 - acc_top5: 0.9986 - 51ms/step\n",
      "step 300/782 - loss: 0.0301 - acc_top1: 0.9673 - acc_top5: 0.9986 - 51ms/step\n",
      "step 310/782 - loss: 0.1324 - acc_top1: 0.9671 - acc_top5: 0.9986 - 51ms/step\n",
      "step 320/782 - loss: 0.0442 - acc_top1: 0.9670 - acc_top5: 0.9986 - 51ms/step\n",
      "step 330/782 - loss: 0.0668 - acc_top1: 0.9670 - acc_top5: 0.9985 - 51ms/step\n",
      "step 340/782 - loss: 0.1943 - acc_top1: 0.9671 - acc_top5: 0.9986 - 51ms/step\n",
      "step 350/782 - loss: 0.0886 - acc_top1: 0.9669 - acc_top5: 0.9986 - 51ms/step\n",
      "step 360/782 - loss: 0.0593 - acc_top1: 0.9671 - acc_top5: 0.9986 - 51ms/step\n",
      "step 370/782 - loss: 0.0727 - acc_top1: 0.9670 - acc_top5: 0.9986 - 51ms/step\n",
      "step 380/782 - loss: 0.0800 - acc_top1: 0.9669 - acc_top5: 0.9985 - 51ms/step\n",
      "step 390/782 - loss: 0.0818 - acc_top1: 0.9669 - acc_top5: 0.9986 - 51ms/step\n",
      "step 400/782 - loss: 0.1076 - acc_top1: 0.9673 - acc_top5: 0.9986 - 51ms/step\n",
      "step 410/782 - loss: 0.0846 - acc_top1: 0.9672 - acc_top5: 0.9986 - 51ms/step\n",
      "step 420/782 - loss: 0.1731 - acc_top1: 0.9674 - acc_top5: 0.9986 - 51ms/step\n",
      "step 430/782 - loss: 0.0811 - acc_top1: 0.9674 - acc_top5: 0.9987 - 51ms/step\n",
      "step 440/782 - loss: 0.0282 - acc_top1: 0.9675 - acc_top5: 0.9987 - 51ms/step\n",
      "step 450/782 - loss: 0.1013 - acc_top1: 0.9676 - acc_top5: 0.9987 - 51ms/step\n",
      "step 460/782 - loss: 0.2745 - acc_top1: 0.9674 - acc_top5: 0.9987 - 51ms/step\n",
      "step 470/782 - loss: 0.2147 - acc_top1: 0.9673 - acc_top5: 0.9987 - 50ms/step\n",
      "step 480/782 - loss: 0.1176 - acc_top1: 0.9672 - acc_top5: 0.9987 - 50ms/step\n",
      "step 490/782 - loss: 0.0372 - acc_top1: 0.9672 - acc_top5: 0.9987 - 51ms/step\n",
      "step 500/782 - loss: 0.0746 - acc_top1: 0.9673 - acc_top5: 0.9987 - 51ms/step\n",
      "step 510/782 - loss: 0.0534 - acc_top1: 0.9675 - acc_top5: 0.9987 - 51ms/step\n",
      "step 520/782 - loss: 0.1575 - acc_top1: 0.9674 - acc_top5: 0.9987 - 50ms/step\n",
      "step 530/782 - loss: 0.0524 - acc_top1: 0.9673 - acc_top5: 0.9987 - 51ms/step\n",
      "step 540/782 - loss: 0.0321 - acc_top1: 0.9674 - acc_top5: 0.9987 - 51ms/step\n",
      "step 550/782 - loss: 0.0640 - acc_top1: 0.9674 - acc_top5: 0.9988 - 50ms/step\n",
      "step 560/782 - loss: 0.1675 - acc_top1: 0.9674 - acc_top5: 0.9987 - 50ms/step\n",
      "step 570/782 - loss: 0.1175 - acc_top1: 0.9673 - acc_top5: 0.9987 - 50ms/step\n",
      "step 580/782 - loss: 0.0357 - acc_top1: 0.9674 - acc_top5: 0.9988 - 50ms/step\n",
      "step 590/782 - loss: 0.1135 - acc_top1: 0.9673 - acc_top5: 0.9988 - 50ms/step\n",
      "step 600/782 - loss: 0.2112 - acc_top1: 0.9671 - acc_top5: 0.9988 - 50ms/step\n",
      "step 610/782 - loss: 0.0362 - acc_top1: 0.9670 - acc_top5: 0.9988 - 50ms/step\n",
      "step 620/782 - loss: 0.0290 - acc_top1: 0.9670 - acc_top5: 0.9988 - 50ms/step\n",
      "step 630/782 - loss: 0.1376 - acc_top1: 0.9668 - acc_top5: 0.9988 - 50ms/step\n",
      "step 640/782 - loss: 0.0420 - acc_top1: 0.9669 - acc_top5: 0.9988 - 50ms/step\n",
      "step 650/782 - loss: 0.1744 - acc_top1: 0.9670 - acc_top5: 0.9988 - 50ms/step\n",
      "step 660/782 - loss: 0.0387 - acc_top1: 0.9670 - acc_top5: 0.9988 - 50ms/step\n",
      "step 670/782 - loss: 0.0869 - acc_top1: 0.9670 - acc_top5: 0.9988 - 50ms/step\n",
      "step 680/782 - loss: 0.1338 - acc_top1: 0.9670 - acc_top5: 0.9988 - 50ms/step\n",
      "step 690/782 - loss: 0.0248 - acc_top1: 0.9669 - acc_top5: 0.9988 - 50ms/step\n",
      "step 700/782 - loss: 0.1078 - acc_top1: 0.9669 - acc_top5: 0.9988 - 50ms/step\n",
      "step 710/782 - loss: 0.0861 - acc_top1: 0.9669 - acc_top5: 0.9989 - 50ms/step\n",
      "step 720/782 - loss: 0.0708 - acc_top1: 0.9671 - acc_top5: 0.9988 - 50ms/step\n",
      "step 730/782 - loss: 0.0746 - acc_top1: 0.9670 - acc_top5: 0.9989 - 50ms/step\n",
      "step 740/782 - loss: 0.0717 - acc_top1: 0.9670 - acc_top5: 0.9988 - 50ms/step\n",
      "step 750/782 - loss: 0.1035 - acc_top1: 0.9671 - acc_top5: 0.9988 - 50ms/step\n",
      "step 760/782 - loss: 0.0855 - acc_top1: 0.9670 - acc_top5: 0.9988 - 50ms/step\n",
      "step 770/782 - loss: 0.0397 - acc_top1: 0.9670 - acc_top5: 0.9988 - 50ms/step\n",
      "step 780/782 - loss: 0.0473 - acc_top1: 0.9668 - acc_top5: 0.9989 - 50ms/step\n",
      "step 782/782 - loss: 0.5985 - acc_top1: 0.9668 - acc_top5: 0.9989 - 50ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\48\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.5793 - acc_top1: 0.6234 - acc_top5: 0.9688 - 19ms/step\n",
      "step  20/157 - loss: 2.9414 - acc_top1: 0.6250 - acc_top5: 0.9672 - 19ms/step\n",
      "step  30/157 - loss: 2.4029 - acc_top1: 0.6271 - acc_top5: 0.9625 - 19ms/step\n",
      "step  40/157 - loss: 2.7376 - acc_top1: 0.6281 - acc_top5: 0.9598 - 19ms/step\n",
      "step  50/157 - loss: 1.6065 - acc_top1: 0.6297 - acc_top5: 0.9587 - 19ms/step\n",
      "step  60/157 - loss: 1.8648 - acc_top1: 0.6315 - acc_top5: 0.9570 - 19ms/step\n",
      "step  70/157 - loss: 3.2546 - acc_top1: 0.6321 - acc_top5: 0.9571 - 19ms/step\n",
      "step  80/157 - loss: 2.6893 - acc_top1: 0.6311 - acc_top5: 0.9570 - 19ms/step\n",
      "step  90/157 - loss: 1.7754 - acc_top1: 0.6333 - acc_top5: 0.9580 - 19ms/step\n",
      "step 100/157 - loss: 1.9446 - acc_top1: 0.6314 - acc_top5: 0.9573 - 19ms/step\n",
      "step 110/157 - loss: 2.6682 - acc_top1: 0.6300 - acc_top5: 0.9568 - 19ms/step\n",
      "step 120/157 - loss: 2.6361 - acc_top1: 0.6290 - acc_top5: 0.9560 - 19ms/step\n",
      "step 130/157 - loss: 2.0716 - acc_top1: 0.6297 - acc_top5: 0.9558 - 19ms/step\n",
      "step 140/157 - loss: 2.1046 - acc_top1: 0.6290 - acc_top5: 0.9560 - 19ms/step\n",
      "step 150/157 - loss: 3.8680 - acc_top1: 0.6292 - acc_top5: 0.9559 - 19ms/step\n",
      "step 157/157 - loss: 1.3818 - acc_top1: 0.6278 - acc_top5: 0.9554 - 19ms/step\n",
      "Eval samples: 10000\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\conda\\envs\\pytorch\\lib\\site-packages\\paddle\\nn\\layer\\norm.py:824: UserWarning: When training, we now always track global mean and variance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  10/782 - loss: 0.2251 - acc_top1: 0.9500 - acc_top5: 1.0000 - 53ms/step\n",
      "step  20/782 - loss: 0.3331 - acc_top1: 0.9430 - acc_top5: 0.9984 - 53ms/step\n",
      "step  30/782 - loss: 0.1279 - acc_top1: 0.9464 - acc_top5: 0.9979 - 53ms/step\n",
      "step  40/782 - loss: 0.2004 - acc_top1: 0.9492 - acc_top5: 0.9977 - 52ms/step\n",
      "step  50/782 - loss: 0.0921 - acc_top1: 0.9494 - acc_top5: 0.9981 - 52ms/step\n",
      "step  60/782 - loss: 0.0790 - acc_top1: 0.9513 - acc_top5: 0.9984 - 52ms/step\n",
      "step  70/782 - loss: 0.0608 - acc_top1: 0.9542 - acc_top5: 0.9987 - 52ms/step\n",
      "step  80/782 - loss: 0.2009 - acc_top1: 0.9563 - acc_top5: 0.9986 - 51ms/step\n",
      "step  90/782 - loss: 0.1554 - acc_top1: 0.9568 - acc_top5: 0.9986 - 51ms/step\n",
      "step 100/782 - loss: 0.0359 - acc_top1: 0.9578 - acc_top5: 0.9988 - 51ms/step\n",
      "step 110/782 - loss: 0.0964 - acc_top1: 0.9592 - acc_top5: 0.9989 - 51ms/step\n",
      "step 120/782 - loss: 0.0892 - acc_top1: 0.9605 - acc_top5: 0.9988 - 52ms/step\n",
      "step 130/782 - loss: 0.2581 - acc_top1: 0.9609 - acc_top5: 0.9988 - 52ms/step\n",
      "step 140/782 - loss: 0.0918 - acc_top1: 0.9610 - acc_top5: 0.9989 - 52ms/step\n",
      "step 150/782 - loss: 0.0922 - acc_top1: 0.9608 - acc_top5: 0.9990 - 52ms/step\n",
      "step 160/782 - loss: 0.1581 - acc_top1: 0.9607 - acc_top5: 0.9989 - 52ms/step\n",
      "step 170/782 - loss: 0.2454 - acc_top1: 0.9608 - acc_top5: 0.9989 - 52ms/step\n",
      "step 180/782 - loss: 0.0859 - acc_top1: 0.9609 - acc_top5: 0.9989 - 53ms/step\n",
      "step 190/782 - loss: 0.1857 - acc_top1: 0.9607 - acc_top5: 0.9988 - 53ms/step\n",
      "step 200/782 - loss: 0.1981 - acc_top1: 0.9602 - acc_top5: 0.9988 - 52ms/step\n",
      "step 210/782 - loss: 0.1782 - acc_top1: 0.9602 - acc_top5: 0.9988 - 52ms/step\n",
      "step 220/782 - loss: 0.1764 - acc_top1: 0.9605 - acc_top5: 0.9989 - 52ms/step\n",
      "step 230/782 - loss: 0.2477 - acc_top1: 0.9604 - acc_top5: 0.9988 - 52ms/step\n",
      "step 240/782 - loss: 0.0349 - acc_top1: 0.9605 - acc_top5: 0.9989 - 52ms/step\n",
      "step 250/782 - loss: 0.1649 - acc_top1: 0.9607 - acc_top5: 0.9989 - 52ms/step\n",
      "step 260/782 - loss: 0.0447 - acc_top1: 0.9611 - acc_top5: 0.9989 - 52ms/step\n",
      "step 270/782 - loss: 0.1108 - acc_top1: 0.9616 - acc_top5: 0.9988 - 52ms/step\n",
      "step 280/782 - loss: 0.0953 - acc_top1: 0.9619 - acc_top5: 0.9988 - 52ms/step\n",
      "step 290/782 - loss: 0.0301 - acc_top1: 0.9623 - acc_top5: 0.9989 - 52ms/step\n",
      "step 300/782 - loss: 0.0400 - acc_top1: 0.9622 - acc_top5: 0.9989 - 52ms/step\n",
      "step 310/782 - loss: 0.2127 - acc_top1: 0.9620 - acc_top5: 0.9987 - 52ms/step\n",
      "step 320/782 - loss: 0.0864 - acc_top1: 0.9621 - acc_top5: 0.9987 - 52ms/step\n",
      "step 330/782 - loss: 0.0829 - acc_top1: 0.9625 - acc_top5: 0.9988 - 52ms/step\n",
      "step 340/782 - loss: 0.0388 - acc_top1: 0.9628 - acc_top5: 0.9988 - 52ms/step\n",
      "step 350/782 - loss: 0.1604 - acc_top1: 0.9630 - acc_top5: 0.9988 - 52ms/step\n",
      "step 360/782 - loss: 0.0772 - acc_top1: 0.9631 - acc_top5: 0.9988 - 52ms/step\n",
      "step 370/782 - loss: 0.0295 - acc_top1: 0.9633 - acc_top5: 0.9989 - 52ms/step\n",
      "step 380/782 - loss: 0.1693 - acc_top1: 0.9634 - acc_top5: 0.9989 - 52ms/step\n",
      "step 390/782 - loss: 0.1301 - acc_top1: 0.9636 - acc_top5: 0.9988 - 51ms/step\n",
      "step 400/782 - loss: 0.0245 - acc_top1: 0.9641 - acc_top5: 0.9988 - 51ms/step\n",
      "step 410/782 - loss: 0.0840 - acc_top1: 0.9641 - acc_top5: 0.9989 - 51ms/step\n",
      "step 420/782 - loss: 0.1090 - acc_top1: 0.9641 - acc_top5: 0.9988 - 51ms/step\n",
      "step 430/782 - loss: 0.1445 - acc_top1: 0.9644 - acc_top5: 0.9988 - 51ms/step\n",
      "step 440/782 - loss: 0.0358 - acc_top1: 0.9646 - acc_top5: 0.9988 - 51ms/step\n",
      "step 450/782 - loss: 0.0578 - acc_top1: 0.9650 - acc_top5: 0.9988 - 51ms/step\n",
      "step 460/782 - loss: 0.1401 - acc_top1: 0.9652 - acc_top5: 0.9988 - 51ms/step\n",
      "step 470/782 - loss: 0.0846 - acc_top1: 0.9653 - acc_top5: 0.9989 - 51ms/step\n",
      "step 480/782 - loss: 0.0899 - acc_top1: 0.9655 - acc_top5: 0.9988 - 51ms/step\n",
      "step 490/782 - loss: 0.0983 - acc_top1: 0.9656 - acc_top5: 0.9989 - 51ms/step\n",
      "step 500/782 - loss: 0.0917 - acc_top1: 0.9659 - acc_top5: 0.9989 - 51ms/step\n",
      "step 510/782 - loss: 0.0569 - acc_top1: 0.9660 - acc_top5: 0.9989 - 51ms/step\n",
      "step 520/782 - loss: 0.0787 - acc_top1: 0.9660 - acc_top5: 0.9989 - 51ms/step\n",
      "step 530/782 - loss: 0.1205 - acc_top1: 0.9660 - acc_top5: 0.9989 - 51ms/step\n",
      "step 540/782 - loss: 0.0893 - acc_top1: 0.9661 - acc_top5: 0.9989 - 51ms/step\n",
      "step 550/782 - loss: 0.0474 - acc_top1: 0.9663 - acc_top5: 0.9989 - 51ms/step\n",
      "step 560/782 - loss: 0.0543 - acc_top1: 0.9664 - acc_top5: 0.9990 - 51ms/step\n",
      "step 570/782 - loss: 0.0812 - acc_top1: 0.9663 - acc_top5: 0.9990 - 51ms/step\n",
      "step 580/782 - loss: 0.0511 - acc_top1: 0.9663 - acc_top5: 0.9989 - 51ms/step\n",
      "step 590/782 - loss: 0.0826 - acc_top1: 0.9662 - acc_top5: 0.9989 - 51ms/step\n",
      "step 600/782 - loss: 0.1368 - acc_top1: 0.9662 - acc_top5: 0.9990 - 51ms/step\n",
      "step 610/782 - loss: 0.0326 - acc_top1: 0.9662 - acc_top5: 0.9990 - 51ms/step\n",
      "step 620/782 - loss: 0.0558 - acc_top1: 0.9661 - acc_top5: 0.9990 - 51ms/step\n",
      "step 630/782 - loss: 0.0757 - acc_top1: 0.9661 - acc_top5: 0.9990 - 51ms/step\n",
      "step 640/782 - loss: 0.2224 - acc_top1: 0.9660 - acc_top5: 0.9990 - 51ms/step\n",
      "step 650/782 - loss: 0.1093 - acc_top1: 0.9658 - acc_top5: 0.9990 - 51ms/step\n",
      "step 660/782 - loss: 0.1182 - acc_top1: 0.9656 - acc_top5: 0.9990 - 51ms/step\n",
      "step 670/782 - loss: 0.1757 - acc_top1: 0.9656 - acc_top5: 0.9990 - 51ms/step\n",
      "step 680/782 - loss: 0.1897 - acc_top1: 0.9656 - acc_top5: 0.9990 - 51ms/step\n",
      "step 690/782 - loss: 0.0528 - acc_top1: 0.9657 - acc_top5: 0.9990 - 51ms/step\n",
      "step 700/782 - loss: 0.0906 - acc_top1: 0.9658 - acc_top5: 0.9990 - 51ms/step\n",
      "step 710/782 - loss: 0.1028 - acc_top1: 0.9658 - acc_top5: 0.9990 - 51ms/step\n",
      "step 720/782 - loss: 0.0521 - acc_top1: 0.9659 - acc_top5: 0.9990 - 51ms/step\n",
      "step 730/782 - loss: 0.0726 - acc_top1: 0.9659 - acc_top5: 0.9990 - 51ms/step\n",
      "step 740/782 - loss: 0.0553 - acc_top1: 0.9659 - acc_top5: 0.9990 - 51ms/step\n",
      "step 750/782 - loss: 0.1522 - acc_top1: 0.9659 - acc_top5: 0.9990 - 51ms/step\n",
      "step 760/782 - loss: 0.0805 - acc_top1: 0.9660 - acc_top5: 0.9990 - 51ms/step\n",
      "step 770/782 - loss: 0.1630 - acc_top1: 0.9659 - acc_top5: 0.9990 - 51ms/step\n",
      "step 780/782 - loss: 0.0163 - acc_top1: 0.9659 - acc_top5: 0.9990 - 51ms/step\n",
      "step 782/782 - loss: 0.2871 - acc_top1: 0.9659 - acc_top5: 0.9989 - 51ms/step\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\49\n",
      "Eval begin...\n",
      "step  10/157 - loss: 1.5545 - acc_top1: 0.6531 - acc_top5: 0.9625 - 22ms/step\n",
      "step  20/157 - loss: 3.4662 - acc_top1: 0.6445 - acc_top5: 0.9633 - 21ms/step\n",
      "step  30/157 - loss: 2.2461 - acc_top1: 0.6432 - acc_top5: 0.9578 - 20ms/step\n",
      "step  40/157 - loss: 3.2439 - acc_top1: 0.6453 - acc_top5: 0.9539 - 20ms/step\n",
      "step  50/157 - loss: 1.7416 - acc_top1: 0.6444 - acc_top5: 0.9556 - 20ms/step\n",
      "step  60/157 - loss: 1.9213 - acc_top1: 0.6424 - acc_top5: 0.9518 - 20ms/step\n",
      "step  70/157 - loss: 2.7318 - acc_top1: 0.6429 - acc_top5: 0.9493 - 20ms/step\n",
      "step  80/157 - loss: 2.1368 - acc_top1: 0.6420 - acc_top5: 0.9494 - 20ms/step\n",
      "step  90/157 - loss: 1.6500 - acc_top1: 0.6410 - acc_top5: 0.9488 - 19ms/step\n",
      "step 100/157 - loss: 2.2626 - acc_top1: 0.6392 - acc_top5: 0.9478 - 19ms/step\n",
      "step 110/157 - loss: 3.3904 - acc_top1: 0.6362 - acc_top5: 0.9480 - 19ms/step\n",
      "step 120/157 - loss: 2.7653 - acc_top1: 0.6353 - acc_top5: 0.9469 - 19ms/step\n",
      "step 130/157 - loss: 2.3440 - acc_top1: 0.6331 - acc_top5: 0.9482 - 19ms/step\n",
      "step 140/157 - loss: 2.2548 - acc_top1: 0.6329 - acc_top5: 0.9473 - 19ms/step\n",
      "step 150/157 - loss: 2.3534 - acc_top1: 0.6331 - acc_top5: 0.9476 - 19ms/step\n",
      "step 157/157 - loss: 2.2324 - acc_top1: 0.6307 - acc_top5: 0.9473 - 19ms/step\n",
      "Eval samples: 10000\n",
      "save checkpoint at C:\\Users\\33079\\PycharmProjects\\pys\\resnet\\output\\final\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
